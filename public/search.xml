<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Tornado5.0.2翻译文档 - Tornado]]></title>
    <url>%2F2018%2F06%2F03%2Ftornado502_start%2F</url>
    <content type="text"><![CDATA[Tornado 是一个 Python 的 Web 框架和异步网络库，最初由 FriendFeed 开发。通过使用非阻塞网络I/O，Tornado 可以支持数以万计的连接，非常适合长轮询、WebSockets 和其他需要与每个用户建立长连接的应用程序。 快速链接 当前版本：5.0.2(download from PyPi release notes) 源代码 邮件列表：discussion 和 announcements Stack Overflow Wiki Hello, World以下是利用 Tornado 实现的一个简单的 “Hello, World” 应用程序： 12345678910111213141516import tornado.ioloopimport tornado.webclass MainHandler(tornado.web.RequestHandler): def get(self): self.write(&quot;Hello, world&quot;)def make_app(): return tornado.web.Application([ (r&quot;/&quot;, MainHandler), ])if __name__ == &quot;__main__&quot;: app = make_app() app.listen(8888) tornado.ioloop.IOLoop.current().start() 以上例子没有使用 Tornado 的任何异步特性，异步特性可以移步 simple char room。 线程和 WSGITornado 与其他大多 Python web 框架不同，它不基于 WSGI 且每个进程只能运行一个 Tornado 线程，移步 用户手册 了解更多 Tornado 的异步编程方式。 然而 WSGI 支持的一些方法可以在 tornado.wsgi 模块中找到，但这并不是 Tornado 后续发展的重点，大多数 Tornado 应用应该直接使用其自带的接口（例如 tornado.web）而不是 WSGI。 一般来说，Tornado 的应用代码并不是线程安全的，Tornado 中唯一可以安全地从其他线程调用的方法是 IOLoop.add_callback 。你也可以使用 IOLoop.run_in_executor 在另一个线程上异步运行阻塞函数，但请注意，传给 run_in_executor 的函数不能引用任何 Tornado 对象。与阻塞函数进行交互时，推荐使用 run_in_executor 。 安装1pip install tornado Tornado 在 PyPi 可获取安装列表中，所以可以直接使用 pip 方式进行安装。请注意，源代码发行版包含演示应用程序，当以这种方式安装 Tornado 时，这些演示应用程序将不会存在，因此您可能希望下载源代码 tar 包或克隆 git repository。 安装条件：Tornado 运行在 Python 2.7 和 Python 3.4+上。 Python 2.7.9 中需要对 ssl 模块进行更新（在某些发行版中，这些更新可能在较早的 Python 版本中可用）。除了 pip 或 setup.py 自动安装的依赖外，以下可选软件包可能会有用： pycurl 在 tornado.curl_httpclient 中需要使用，需要 Libcurl 7.22或更高版本； Twisted 在 tornado.platform.twisted 的诸多类中会使用到； pycares 是一个可选的非阻塞 DNS 解析器，可以在线程不适用时使用； monotonic 或 Monotime 添加对单调时钟的支持，从而提高时钟频繁调整场景下的可靠性。 在Python 3中不再需要。 平台：尽管为了获得最佳性能和可扩展性，Tornado 应该可以在任何类Unix平台上运行，但对于生产部署，建议只使用Linux（with epoll）和 BSD（with kqueue）（尽管 Mac OS X 源自 BSD 并支持 kqueue，但其网络 性能一般很差，所以建议仅用于开发使用）。Tornado 也可以在 Windows 上运行，这种配置没有官方的支持，只推荐用于开发。 如果不修改 Tornado IOLoop 接口，就不可能添加本地 Tornado Windows IOLoop 实现，且不能利用像 AsyncIO 或 Twisted 等框架的 Windows 的 IOCP 支持。 文档 用户手册 简介 异步和非阻塞 I/O 协程 Queue 示例-一个并发的爬虫程序 Tornado 的 web 应用程序结构 模板和UI 认证和安全 运行和部署 Web 框架 tornado.web - RequestHandler Application 类 tornado.template - 灵活的输出生成 tornado.routing - 基本的路由实现 tornado.escape - 转义和字符串操作 tornado.locale - 国际化支持 tornado.websocket - 与浏览器的双向通信 HTTP 服务器和客户端 tornado.httpserver - 非阻塞 HTTP 服务器 tornado.httpclient - 异步 HTTP 客户端 tornado.httputil - 操作HTTP标头和URL tornado.httpconnection - HTTP/1.x客户端/服务器实现 异步网络 tornado.ioloop - 主时间循环 tornado.iostream - 对非阻塞 socket 的简易包装 tronado.netutil - 网络实用程序 tornado.tcpclient - IOStream 连接工厂 tornado.tcpserver - 基于 TCP 服务器的 IOStream 协程和并发 tornado.gen - 基于生成器的协程 tornado.locks - 同步原语 tornado.queues - 协程队列 tornado.process - 多进程的实用程序 服务集成 tornado.auth - 集成 OpenID 和 OAuth 的第三方登录服务 tornado.wsgi - 与其他 Python 框架和服务器的互操作性 tornado.platform.caresresolver - 使用 C-Ares 的异步DNS解析器 tornado.platform.twisted - 兼容 Tornado 和 Twisted tornado.asyncio - 兼容 Tornado 和 asyncio 其他 tornado.authoreload - 开发环境中自动检测代码变化 tornado.concrrent - 与 Future 对象交互 tornado.log - 日志支持 tornado.options - 命令行解析 tornado.stack_context - 异步回调异常处理 tornado.testing - 异步代码的单元测试 tornado.util - 通用程序 讨论和支持你可以在 the Tornado developer mailing list 参与 Tornado 的讨论，在 GitHub issue tracker 上提交你的bug，更多资源可以在 Tornado wiki 找到，新版本发布在 announcements mailing list。 Read More: Tornado]]></content>
      <categories>
        <category>Tornado</category>
      </categories>
      <tags>
        <tag>Tornado</tag>
        <tag>翻译文档</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Datatables使用系列-基础配置]]></title>
    <url>%2F2018%2F05%2F27%2Fdatatables1%2F</url>
    <content type="text"><![CDATA[Datatables 为是 JQuery 的一款表格插件库，是一个高度灵活的工具，可以将任何 HTML 表格添加高级的交互功能。特别是后台开发者，对前端相关框架或知识并不是很熟悉，在做一些内部使用的工具时，总会涉及到页面展示的功能，Buttons 同样是一个高度可定制的、免费并且开源的按钮 CSS 样式库。类似插件对于后台开发者来说，简直是福音。本文主要介绍 Datatables 的基本配置和使用。 下载从 Download 页面可下载需要使用的 JS 文件和其他的插件，可以根据需求选择不同的样式、包文件和扩展文件，如： 123456789101112Step 1. Choose a styling framework Bootstrap 4Step 2. Select packages jQuery 3 Bootstrap 4 DataTablesExtensions 可选Step 3. Pick a download method Minify Concatenate 下载方式常用是CDN和本地文件方式（推荐） 然后我们下载打包好的文件到本地就可以了。 基本使用将文件打包下载后，因为一般页面都是在 web 应用中使用，所以笔者创建了一个简单的 Tornado 应用，将所有的静态文件放到了 /static 目录下： 注意： jquery.js 文件是必需的，目前使用 3.0 版本。 然后我们可以创建一个简单的 HTML 静态页面，引入基本的 js 文件： 123456789101112131415161718&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Datatables&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;static/js/Bootstrap-4-4.0.0/css/bootstrap.css&quot;/&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;static/js/DataTables-1.10.16/css/dataTables.bootstrap4.css&quot;/&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;static/css/datatables.min.css&quot;/&gt; &lt;script src=&quot;static/js/jquery.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;static/js/Bootstrap-4-4.0.0/js/bootstrap.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;static/js/DataTables-1.10.16/js/jquery.dataTables.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;static/js/DataTables-1.10.16/js/dataTables.bootstrap4.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 注意： 如果在调试过程中，出现表格显示错误，请检查 css 文件和 js 文件的引用顺序。 接下来创建一个 div 作为表格区域： 12345678910111213&lt;div&gt; &lt;table id=&quot;test_dt&quot; class=&quot;table table-striped table-bordered&quot; cellspacing=&quot;0&quot; style=&quot;width: 100%&quot;&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;id&lt;/th&gt; &lt;th&gt;username&lt;/th&gt; &lt;th&gt;sex&lt;/th&gt; &lt;th&gt;address&lt;/th&gt; &lt;th&gt;age&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;/table&gt;&lt;/div&gt; table 中的 class 属性都是在 css 文件中定义好的，直接引用即可。 然后就是通过 js 为 test_dt 表格填充数据，数据源一般 Data sources 这些，我们在 web 应用中使用最多的就是 Ajax 方式了，可以局部刷新页面。 本文数据源方式使用 Ajax data source (objects) 这种方式，js 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142function init_table()&#123; $(&apos;#test_dt&apos;).DataTable(&#123; order: [[ 0, &quot;desc&quot;]], searching: true, stateSave: true, paging: true, pagingType: &apos;full_numbers&apos;, autoWidth: true, responsive: false, pageLength: 10, destroy: true, ajax: &#123; &apos;url&apos;: &apos;/index&apos;, &apos;dataSrc&apos;: handle_data_src, &apos;type&apos;: &apos;POST&apos;, &apos;async&apos;: false, &apos;data&apos;: &#123;&#125; &#125;, columns:[ &#123; &quot;data&quot;:&quot;id&quot;, &quot;width&quot;: &quot;10%&quot; &#125;, &#123; &quot;data&quot;:&quot;username&quot;, &quot;width&quot;: &quot;10%&quot; &#125;, &#123; &quot;data&quot;:&quot;sex&quot;, &quot;width&quot;: &quot;10%&quot; &#125;, &#123; &quot;data&quot;:&quot;address&quot;, &quot;width&quot;: &quot;10%&quot; &#125;, &#123; &quot;data&quot;:&quot;age&quot;, &quot;width&quot;: &quot;10%&quot; &#125;, ], &#125;);&#125; 通过 POST 方式获取表格数据，方便表格的刷新操作。其中 dataSrc 参数可以对请求的数据在显示到表格前做其他处理。 效果图基本如下： 配置参数Datatables 支持参数配置方式以提供各种场景下对表格的需求，目前笔者主要使用的参数如下： 参数 含义 可选值 order 表格在初始化的时候的排序 order searching 允许表格搜索 true false stateSave 允许浏览器缓存 Datatables，以便下次恢复之前的状态 true false paging 允许表格分页 true false pagingType 分页按钮显示选项 pagingType autoWidth 定义是否由控件自动控制列宽 true false pageLength 单页显示的数据条数 integer destroy 销毁已经存在的 Datatables 实例并替换新的选项 true false ajax 异步获取数据填充到表格显示 ajax columns 设定列的所有初始属性 columns info 控制总数信息（标准界面右下角显示总数和过滤条数的控件）的显隐 true false 其他参数设置可参考 Options 。 Tips[持续更新]ajax.reload() 场景：间隔一定时间进行局部刷新。 该场景下如果每次都初始化 datatables ，则会使表格在刷新时失去鼠标控制，用户体验较差。使用 ajax.reload(callback, resetPaging) 方式可以在重复初始化 datatables 下重新加载数据，如： 1$(&apos;#test_dt&apos;).DataTable().ajax.reload(null, false); 当使用该方式时，如果 ajax 提交的请求 data 字段带的参数值需要动态更新的话，可以使用： 123&apos;data&apos;: function(d)&#123; d.test_name = test_name;&#125; 通过 js 函数动态更新参数值，这样在 reload 表格时，提交的参数就是动态获取的了。 render 场景：获取数据后，需要根据某列值对数据进行预处理。 我们可以在 dataSrc 中对数据进行处理，但是 Datatables 提供了另外一种更加方便的处理方式，即 columns.render 方法，如： 123456789101112&#123; &quot;data&quot;:&quot;address&quot;, render: function (data, type, row, meta) &#123; var node = &quot;&lt;span &quot;; if (data &lt;= bw_warn) &#123; node += &quot;style=&apos;color:red;font-weight: bold;&apos;&quot; &#125; node += &quot;&gt;&quot; + data + &quot;&lt;/span&gt;&quot; return node; &#125;, &quot;width&quot;: &quot;10%&quot;&#125;, 可以根据列值，对该列的显示添加其他的 css 属性。 自定义排序 Datatables 可以根据指定的列进行排序，但是实际场景中某列值可能较为复杂，而我们只需使用其中的一部分进行排序，例如：去掉前缀。Datatables 的扩展插件提供了很多较为强大的自定义排序功能。 例如：去掉前缀字符 prefix ，将剩下字符作为整型排序，可以参考 Anti-the。 首先新建一个名为 jquery.datatable.sort.plugin.js 文件，将以下 copy 到文件中。 1234567891011121314jQuery.extend( jQuery.fn.dataTableExt.oSort, &#123; &quot;anti-prefix-pre&quot;: function ( a ) &#123; var x = a.replace(/^prefix /i, &quot;&quot;); return parseInt(x); &#125;, &quot;anti-prefix-asc&quot;: function ( a, b ) &#123; return ((a &lt; b) ? -1 : ((a &gt; b) ? 1 : 0)); &#125;, &quot;anti-prefix-desc&quot;: function ( a, b ) &#123; return ((a &lt; b) ? 1 : ((a &gt; b) ? -1 : 0)); &#125;&#125; ); 然后在页面中按照如下方式引用 1&lt;script type=&quot;text/javascript&quot; src=&quot;/path/jquery.datatable.sort.plugin.js&quot;&gt;&lt;/script&gt; 最后在 Datatables 中加入 123columnDefs: [ &#123; type: &apos;anti-prefix&apos;, targets: 0 &#125;] 其中：targets 表示列编号，从0开始。 Example： 12345$(&apos;#example&apos;).dataTable( &#123; columnDefs: [ &#123; type: &apos;anti-the&apos;, targets: 0 &#125; ]&#125; ); Read More: Datatables]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>datatables</tag>
        <tag>jquery</tag>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQLAlchemy bug系列(一)]]></title>
    <url>%2F2018%2F05%2F13%2Fsqlalchemybug1%2F</url>
    <content type="text"><![CDATA[项目框架采用 Tornado ，SQLAlchemy 作为数据库ORM，简陋的代码如下： 12345678910111213141516def query(self, filters, orders=&apos;&apos;): try: if not isinstance(filters, str) or not isinstance(orders, str): return False if not len(orders): return self.s.query(Node).filter(filters).first() else: r = self.s.query(Node).filter(filters).order_by(orders).all() if len(r): return r[0] return None except Exception as e: self.logger.error(traceback.format_exc()) return None finally: Session.remove() 但是在跑了一段时间后出现了如下问题： (sqlalchemy.exc.InvalidRequestError) Can’t reconnect until invalid transaction is rolled back 错误中很明显是数据库的连接由于事务某些错误出现了问题，SQLAlchemy 在尝试重新连接时失败了。 几经翻找资料后，问题产生原因如下： 从数据库连接池（pool）中获取的 connection 没有以 session.commit() 或 session.rollback() 或 session.close() 的某一种放回 pool 中。这时 connection 的事务（transaction ）没有完结，而在后续与数据库交互中，由于某些原因（如死锁、超时）数据库连接池中的 connection 又死掉了，当获取到这个连接时，SQLAlchemy 尝试重新连接。但由于 transaction 还没完结，无法重连。就抛出了上述错误。 解决办法： 显示调用事务结束 使用 try...except... 代码块，except 中捕获到异常时，调用 session.rollback() 回滚事务。 打开 autocommit 使用 SQLAlchemy 的初始化方式为： 123456789101112131415Base = declarative_base()engine = create_engine( &quot;mysql+pymysql://&#123;&#125;:&#123;&#125;@&#123;&#125;:&#123;&#125;/&#123;&#125;&quot;.format( mysql_config[&apos;default&apos;][&apos;user&apos;], mysql_config[&apos;default&apos;][&apos;password&apos;], mysql_config[&apos;default&apos;][&apos;host&apos;], mysql_config[&apos;default&apos;][&apos;port&apos;], mysql_config[&apos;default&apos;][&apos;name&apos;], ), encoding=&quot;utf-8&quot;, echo=False, pool_recycle=mysql_config[&apos;connect_pool&apos;][&apos;pool_recycle&apos;], pool_size=mysql_config[&apos;connect_pool&apos;][&apos;pool_size&apos;])Session = scoped_session(sessionmaker(bind=engine)) 默认是使用事务操作，我们可以在初始化语句中加上 autocommit=true 关闭事务： 12345678910111213engine = create_engine( &quot;mysql+pymysql://&#123;&#125;:&#123;&#125;@&#123;&#125;:&#123;&#125;/&#123;&#125;?autocommit=true&quot;.format( mysql_config[&apos;default&apos;][&apos;user&apos;], mysql_config[&apos;default&apos;][&apos;password&apos;], mysql_config[&apos;default&apos;][&apos;host&apos;], mysql_config[&apos;default&apos;][&apos;port&apos;], mysql_config[&apos;default&apos;][&apos;name&apos;], ), encoding=&quot;utf-8&quot;, echo=False, pool_recycle=mysql_config[&apos;connect_pool&apos;][&apos;pool_recycle&apos;], pool_size=mysql_config[&apos;connect_pool&apos;][&apos;pool_size&apos;]) 这样生成的查询语句就会立即执行。注意：个人感觉这并不是一个好方法，还是老老实实捕获异常吧。]]></content>
      <categories>
        <category>SQLAlchemy</category>
      </categories>
      <tags>
        <tag>SQLAlchemy</tag>
        <tag>bug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP POST提交数据分常见方式]]></title>
    <url>%2F2018%2F05%2F13%2Fhttppostmethod%2F</url>
    <content type="text"><![CDATA[HTTP/1.1协议 规定的 HTTP 请求方法有 OPTIONS、GET、HEAD、POST、PUT、DELETE、TRACE、CONNECT这几种。其中 POST 一般用来向服务端提交数据，我们在写类似接口文档这种被业务方调用的服务时，涉及到 POST 的默认提交方式基本都是：application/x-www-form-urlencoded 。本 文主要讨论 POST 提交数据的几种方式。 我们知道，HTTP 协议是以 ASCII 码传输，建立在 TCP/IP 协议之上的应用层规范。规范把 HTTP 请求分为三个部分：状态行、请求头、消息主体。类似于下面这样： 协议规定 POST 提交的数据必须放在消息主体（entity-body）中，但协议并没有规定数据必须使用什么编码方式。实际上，开发者完全可以自己决定消息主体的格式，只要最后发送的 HTTP 请求满足上面的格式就可以。 但是，数据发送出去，还要服务端解析成功才有意义。一般服务端语言如 java php python 等，以及它们的 framework，都内置了自动解析常见数据格式的功能。服务端通常是根据请求头（headers）中的 Content-Type字段来获知请求中的消息主体是用何种方式编码，再对主体进行解析。所以说到 POST 提交数据方案，包含了 Content-Type 和消息主体编码方式两部分。下面进入正题。 application/x-www-form-urlencoded这种方式是最常见的 POST 提交数据的方式。浏览器的原生 &lt;form&gt; 表单，如果不设置 enctype 属性，那么最终就会以 application/x-www-form-urlencoded 方式提交数据。请求类似于下面这样（无关的请求头在本文中都省略掉了）： 123POST http://www.example.com HTTP/1.1Content-Type: application/x-www-form-urlencoded;charset=utf-8title=test&amp;sub%5B%5D=1&amp;sub%5B%5D=2&amp;sub%5B%5D=3 首先，Content-Type 被指定为 application/x-www-form-urlencoded ；其次，提交的数据按照 key1=val1&amp;key2=val2 的方式进行编码，key 和 val 都进行了 URL 转码。大部分服务端语言都对这种方式有很好的支持。 通常，我们用 Ajax 提交数据时，也是使用这种方式。例如 JQuery 和 QWrap 的 Ajax，Content-Type 默认值都是 application/x-www-form-urlencoded;charset=utf-8 。 multipart/form-data在使用表单上传文件时，必须让 &lt;form&gt; 表单的 enctype 等于 multipart/form-data。直接来看一个请求示例： 12345678910111213POST http://www.example.com HTTP/1.1Content-Type:multipart/form-data; boundary=----WebKitFormBoundaryrGKCBY7qhFd3TrwA------WebKitFormBoundaryrGKCBY7qhFd3TrwAContent-Disposition: form-data; name=&quot;text&quot;title------WebKitFormBoundaryrGKCBY7qhFd3TrwAContent-Disposition: form-data; name=&quot;file&quot;; filename=&quot;chrome.png&quot;Content-Type: image/pngPNG ... content of chrome.png ...------WebKitFormBoundaryrGKCBY7qhFd3TrwA-- 这个例子稍微复杂点。首先生成了一个 boundary 用于分割不同的字段，为了避免与正文内容重复，boundary 很长很复杂。然后 Content-Type 里指明了数据是以 multipart/form-data 来编码，本次请求的 boundary 是什么内容。消息主体里按照字段个数又分为多个结构类似的部分，每部分都是以 --boundary 开始，紧接着是内容描述信息，然后是回车，最后是字段具体内容（文本或二进制）。如果传输的是文件，还要包含文件名和文件类型信息。消息主体最后以 --boundary-- 标示结束。关于 multipart/form-data 的详细定义，请前往 rfc1867 查看。这种方式一般用来上传文件，各大服务端语言对它也有着良好的支持。 以上提到的两种 POST 数据的方式，都是浏览器原生支持的，而且现阶段标准中原生 \ 表单也 只支持这两种方式（通过 &lt;form&gt; 元素的 enctype 属性指定，默认为 application/x-www-form-urlencoded。其实 enctype 还支持 text/plain，不过用得非常少）。 随着越来越多的 Web 站点，尤其是 WebApp，全部使用 Ajax 进行数据交互之后，我们完全可以定义新的数据提交方式，给开发带来更多便利。 application/jsonapplication/json 这个 Content-Type 作为响应头大家肯定不陌生。实际上，现在越来越多的人把它作为请求头，用来告诉服务端消息主体是序列化后的 JSON 字符串。由于 JSON 规范的流行，除了低版本 IE 之外的各大浏览器都原生支持 JSON.stringify，服务端语言也都有处理 JSON 的函数，使用 JSON 不会遇上什么麻烦。JSON 格式支持比键值对复杂得多的结构化数据，这一点也很有用。 Google 的 AngularJS 中的 Ajax 功能，默认就是提交 JSON 字符串。例如下面这段代码： 1234var data = &#123;&apos;title&apos;:&apos;test&apos;, &apos;sub&apos; : [1,2,3]&#125;;$http.post(url, data).success(function(result) &#123; ...&#125;); 最终发送的请求是： 1234POST http://www.example.com HTTP/1.1 Content-Type: application/json;charset=utf-8&#123;&quot;title&quot;:&quot;test&quot;,&quot;sub&quot;:[1,2,3]&#125; 这种方案，可以方便的提交复杂的结构化数据，特别适合 RESTful 的接口。各大抓包工具如 Chrome 自带的开发者工具、Firebug、Fiddler，都会以树形结构展示 JSON 数据，非常友好。但也有些服务端语言还没有支持这种方式，例如 php 就无法通过 $_POST 对象从上面的请求中获得内容。这时候，需要自己动手处理下：在请求头中 Content-Type 为 application/json 时，从 php://input 里获得原始输入流，再 json_decode 成对象。一些 php 框架已经开始这么做了。 当然 AngularJS 也可以配置为使用 x-www-form-urlencoded 方式提交数据。如有需要，可以参考 这里。 text/xml这种方式目前几乎很少在用，因为 JSON 方式更加灵活方便，而 xml 方式显得比较臃肿和繁琐。XML-RPC（XML Remote Procedure Call） 协议是一套允许运行在不同操作系统、不同环境的程序实现基于 Internet 过程调用的规范和一系列的实现。这种远程过程调用使用 HTTP 作为传输协议，XML 作为传送信息的编码格式。 典型的 XML-RPC 请求是这样的： 123456789101112POST http://www.example.com HTTP/1.1 Content-Type: text/xml&lt;?xml version=&quot;1.0&quot;?&gt;&lt;methodCall&gt; &lt;methodName&gt;examples.getStateName&lt;/methodName&gt; &lt;params&gt; &lt;param&gt; &lt;value&gt;&lt;i4&gt;41&lt;/i4&gt;&lt;/value&gt; &lt;/param&gt; &lt;/params&gt;&lt;/methodCall&gt;]]></content>
      <categories>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
        <tag>POST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式锁]]></title>
    <url>%2F2018%2F05%2F06%2Fdistlock%2F</url>
    <content type="text"><![CDATA[前言进入正题之前，先来说说问题的产生背景。目前绝大多数的业务都跑在高并发的情境下，而数据库（大多数是mysql）数据一致性的问题是不可避免的，笔者同样遇到了这种问题： 高并发情境下数据库的数据重复写入问题 数据的重复写入实际上就是对共享资源的竞争操作，导致数据出现不一致，给线上业务带来影响。解决这种问题，最直接的思路是在数据库层面寻求解决方案，借助数据库的锁机制，或者在业务上通过数据库语句的限制避免重复数据，如： insert ignore into 语句； on duplicate key update 语句； insert … select … where not exist 语句； replace into 语句。 以上语句在简单业务中避免mysql插入重复数据是有一定效果的，但是在复杂场景下并不能满足需求。因为高并发业务除了要求数据一致性外，对性能、稳定性的要求也较高，单纯使用数据库语句很难达到预期效果，因此，从单机锁发展而来的分布式锁提供了一种新的解决方案。 SQL语句解决方案insert ignore into 123456789101112131415INSERT [LOW_PRIORITY | HIGH_PRIORITY] [IGNORE] [INTO] tbl_name [PARTITION (partition_name [, partition_name] ...)] [(col_name [, col_name] ...)] SELECT ... [ON DUPLICATE KEY UPDATE assignment_list]value: &#123;expr | DEFAULT&#125;assignment: col_name = valueassignment_list: assignment [, assignment] ... Specify IGNORE to ignore rows that would cause duplicate-key violations. 通过mysql给出的官方解释：IGNORE 关键字会忽略重复键的行。当插入数据时，如出现错误（重复数据），将不返回错误，只以警告形式返回。所以使用 IGNORE 请确保语句本身没有问题，否则语句的其他错误也会被忽略掉。如果使用 IGNORE 关键字，则插入的字段必须是主键或唯一索引。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647mysql&gt; show tables;+----------------------------+| Tables_in_openlivedb |+----------------------------+| admin || auth_group || auth_group_permissions || auth_permission || auth_user || auth_user_groups || auth_user_user_permissions || cmd_template || cutter_node || django_admin_log || django_content_type || django_migrations || django_session || flv_slice || mp4_slice || test_table || ts_slice |+----------------------------+17 rows in set (0.00 sec)mysql&gt; select * from admin;+----+-------------------+----------------------------------+| id | username | password |+----+-------------------+----------------------------------+| 1 | liuxiaoyang | 96e79218965eb72c92a549dd5a330112 || 4 | liuxiaoyang_test2 | |+----+-------------------+----------------------------------+2 rows in set (0.00 sec)mysql&gt; INSERT IGNORE INTO admin (username) VALUES (&apos;liuxiaoyang_test2&apos;);Query OK, 0 rows affected, 2 warnings (0.09 sec)mysql&gt; select * from admin;+----+-------------------+----------------------------------+| id | username | password |+----+-------------------+----------------------------------+| 1 | liuxiaoyang | 96e79218965eb72c92a549dd5a330112 || 4 | liuxiaoyang_test2 | |+----+-------------------+----------------------------------+2 rows in set (0.00 sec)mysql&gt; INSERT IGNORE INTO admin (username) VALUES (&apos;liuxiaoyang_test3&apos;);Query OK, 1 row affected, 1 warning (0.11 sec) 以上操作可以看出，插入重复数据时，显示 Query OK, 0 rows affected, 2 warnings (0.09 sec) 表明出现了警告，当在业务中直接执行时： 12345am = AdminManager()r = am.execute(&quot;select * from admin&quot;)for i in r: print(i.username)r2 = am.execute(&quot;INSERT IGNORE INTO admin (username, password) VALUES (&apos;liuxiaoyang_test2&apos;, &apos;123&apos;)&quot;) 结果如下： 1234liuxiaoyangliuxiaoyang_test2/usr/local/lib/python3.5/dist-packages/pymysql/cursors.py:165: Warning: (1062, &quot;Duplicate entry &apos;liuxiaoyang_test2&apos; for key &apos;username_unique&apos;&quot;) result = self._query(query) 同样返回了警告信息。 on duplicate key update If you specify an ON DUPLICATE KEY UPDATE clause and a row to be inserted would cause a duplicate value in a UNIQUE index or PRIMARY KEY, an UPDATE of the old row occurs. 当遇到重复键时，对旧行执行 UPDATE 语句，否则插入新行。 1234567891011121314151617181920mysql&gt; select * from admin;+----+-------------------+----------------------------------+| id | username | password |+----+-------------------+----------------------------------+| 1 | liuxiaoyang | 96e79218965eb72c92a549dd5a330112 || 4 | liuxiaoyang_test2 | 123 |+----+-------------------+----------------------------------+2 rows in set (0.02 sec)mysql&gt; INSERT IGNORE INTO admin (username) VALUES (&apos;liuxiaoyang_test2&apos;) ON DUPLICATE KEY UPDATE password=&apos;345&apos;;Query OK, 2 rows affected, 1 warning (0.09 sec)mysql&gt; select * from admin;+----+-------------------+----------------------------------+| id | username | password |+----+-------------------+----------------------------------+| 1 | liuxiaoyang | 96e79218965eb72c92a549dd5a330112 || 4 | liuxiaoyang_test2 | 345 |+----+-------------------+----------------------------------+2 rows in set (0.00 sec) 对于on duplicate key update语句，必需主键或唯一索引。 insert … select … where not exist 该方法利用了子查询，写法比较繁琐，性能较差，不推荐使用。 replace into 1234567891011121314151617REPLACE [LOW_PRIORITY | DELAYED] [INTO] tbl_name [PARTITION (partition_name [, partition_name] ...)] [(col_name [, col_name] ...)] &#123;VALUES | VALUE&#125; (value_list) [, (value_list)] ...value: &#123;expr | DEFAULT&#125;value_list: value [, value] ...assignment: col_name = valueassignment_list: assignment [, assignment] ... REPLACE works exactly like INSERT, except that if an old row in the table has the same value as a new row for a PRIMARY KEY or a UNIQUE index, the old row is deleted before the new row is inserted. replace into 语句与insert语句类似，只不过它在插入时，如果存在主键或唯一键相同的记录，则会先删除，然后再插入新记录。 1234567891011121314151617181920mysql&gt; select * from admin;+----+-------------------+----------------------------------+| id | username | password |+----+-------------------+----------------------------------+| 1 | liuxiaoyang | 96e79218965eb72c92a549dd5a330112 || 5 | liuxiaoyang_test2 | 789 |+----+-------------------+----------------------------------+2 rows in set (0.00 sec)mysql&gt; replace into admin(username, password) values(&apos;liuxiaoyang_test2&apos;, &apos;1111&apos;);Query OK, 2 rows affected (0.10 sec)mysql&gt; select * from admin;+----+-------------------+----------------------------------+| id | username | password |+----+-------------------+----------------------------------+| 1 | liuxiaoyang | 96e79218965eb72c92a549dd5a330112 || 6 | liuxiaoyang_test2 | 1111 |+----+-------------------+----------------------------------+2 rows in set (0.00 sec) MySQL uses the following algorithm for REPLACE: try to insert the new row into the table While the insertion fails because a duplicate-key error occurs for a primary key or unique index: Delete from the table the conflicting row that has the duplicate key value; try again to insert the new row into the table. 以上方法也会有些限制，如目前大多数业务的删除只是逻辑删除，并不做物理删除，在这种场景下就需要寻找其他方案。除了以上方法，mysql还提供了共享锁、排他锁等机制，mysql的锁机制会单独进行介绍，在此就不赘述。 分布式锁解决方案何为锁在单进程的系统中，当存在多个线程可以同时改变某个变量（可变共享变量）时，就需要对变量或代码块做同步，使其在修改这种变量时能够线性执行消除并发修改变量； 而同步的本质是通过锁来实现的。为了实现多个线程在一个时刻同一个代码块只能有一个线程可执行，那么需要在某个地方做个标记，这个标记必须每个线程都能看到，当标记不存在时可以设置该标记，其余后续线程发现已经有标记了则等待拥有标记的线程结束同步代码块取消标记后再去尝试设置标记。这个标记可以理解为锁，不同地方实现锁的方式也不一样，只要能满足所有线程都能看得到标记即可； 除了利用内存数据做锁，其实任何互斥的都能做锁（只考虑互斥情况），如流水表中流水号与时间结合做幂等校验可以看作是一个不会释放的锁，或者使用某个文件是否存在作为锁等。只需要满足在对标记进行修改能保证原子性和内存可见性即可。 什么是分布式分布式的 CAP 理论告诉我们： 任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。 目前很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。基于 CAP理论，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证最终一致性。 分布式场景 此处主要指集群模式下，多个相同服务同时运行 在许多的场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。很多时候我们需要保证一个方法在同一时间内只能被同一个线程执行。在单机环境中，通过线程安全队列或者其他方式解决，但是在分布式环境下，这并不是一件容易的事儿。 分布式与单机情况下最大的不同在于其不是多线程而是多进程； 多线程由于可以共享堆内存，因此可以简单的采取内存作为标记存储位置。而进程之间甚至可能都不在同一台物理机上，因此需要将标记存储在一个所有进程都能看到的地方。 分布式锁 当在分布式模型下，数据只有一份（或有限制），此时需要利用锁的技术控制某一时刻修改数据的进程数； 与单机模式下的锁不仅需要保证进程可见，还需要考虑进程与锁之间的网络问题； 分布式锁还是可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存如 Redis、Memcache。至于利用数据库、文件等做锁与单机的实现是一样的，只要保证标记能互斥就行。 我们需要怎样的分布式锁 在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行； 高可用的获取锁与释放锁； 高性能的获取锁与释放锁； 具备可重入特性； 具备锁失效机制，防止死锁； 具备非阻塞锁特性，即没有获取到锁将直接返回获取锁失败。 分布式锁-基于数据库乐观锁 - 基于表主键唯一做分布式锁 利用主键唯一的特性，如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，当方法执行完毕之后，想要释放锁的话，删除这条数据库记录即可。 这种简单的实现有以下几个问题： 这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用； 这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁； 这把锁只能是非阻塞的，因为数据的 insert 操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作； 这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了； 这把锁是非公平锁，所有等待锁的线程凭运气去争夺锁； 在 mysql 数据库中采用主键冲突防重，在大并发情况下有可能会造成锁表现象。 当然，我们也可以有其他方式解决上面的问题。 数据库是单点？搞两个数据库，数据之前双向同步，一旦挂掉快速切换到备库上； 没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍； 非阻塞的？搞一个 while 循环，直到 insert 成功再返回成功； 非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了； 非公平的？再建一张中间表，将等待锁的线程全记录下来，并根据创建时间排序，只有最先创建的允许获取锁； 比较好的办法是在程序中生产主键进行防重。 乐观锁 - 基于表字段版本号做分布式锁 这个策略源于 mysql 的 mvcc 机制，使用这个策略其实本身没有什么问题，唯一的问题就是对数据表侵入较大，我们要为每个表设计一个版本号字段，然后写一条判断 sql 每次进行判断，增加了数据库操作的次数，在高并发的要求下，对数据库连接的开销也是无法忍受的。 悲观锁 - 基于数据库排他锁做分布式锁 在查询语句后面增加 for update，数据库会在查询过程中给数据库表增加排他锁 （注意： InnoDB 引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。这里我们希望使用行级锁，就要给要执行的方法字段名添加索引，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上）。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。 我们可以认为获得排他锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，通过 connection.commit() 操作来释放锁。 这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。 阻塞锁？ for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功； 锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。 但是还是无法直接解决数据库单点和可重入问题。虽然我们对方法字段名使用了唯一索引，并且显示使用 for update来使用行级锁。但是，MySQL 会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。 还有一个问题，就是我们要使用排他锁来进行分布式锁的 lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆。 分布式锁-基于RedisRedis分布式锁主要使用其SETNX()、EXPIRE() 方法实现。 SETNX key value : 将 key 的值设为 value ，当且仅当 key 不存在。若给定的 key 已经存在，则 ＳETNX 不做任何动作。SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。 EXPIRE key seconds : 为给定 key 设置生存时间，当 key 过期时(生存时间为 0 )，它会被自动删除。 但是从 Redis 2.6.12 版本开始， SET 命令的行为可以通过一系列参数来修改： EX second ：设置键的过期时间为 second 秒。 SET key value EX second 效果等同于 SETEX key second value 。 PX millisecond ：设置键的过期时间为 millisecond 毫秒。 SET key value PX millisecond 效果等同于 PSETEX key millisecondvalue 。 NX ：只在键不存在时，才对键进行设置操作。 SET key value NX 效果等同于 SETNX key value 。 XX ：只在键已经存在时，才对键进行设置操作。 所以SETNX()、EXPIRE() 方法被融合进了SET方法，而官网 给出 因为 SET 命令可以通过参数来实现和 SETNX 、 SETEX 和 PSETEX 三个命令的效果，所以将来的 Redis 版本可能会废弃并最终移除SETNX 、 SETEX 和 PSETEX 这三个命令。 在使用Redis实现分布式锁时，我们可以通过SET来实现。 设计思路 获取锁： 调用 set 尝试获取锁，如果设置成功，表示获取到了锁； 设置失败， 等待一定时间后，再次尝试获取锁； 若已达到最大获取锁时间未获取到锁，获取锁失败。 释放锁： 确保每个客户端释放的是自己的锁； 确保释放锁操作的原子性。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import timeimport tracebackfrom contextlib import contextmanagerfrom lib.utils import get_loggerfrom tornado import genDEFAULT_EXPIRES = 3ACQUIRE_TIMEOUT = 1lock_logger = get_logger(&quot;redis_netlock&quot;)@contextmanagerdef dist_lock(key, client, unique_id acquire_timeout=ACQUIRE_TIMEOUT): key = &apos;lock_%s&apos; % key try: t = _acquire_lock(key, client, unique_id, acquire_timeout) yield t finally: _release_lock(key, client, t)def _acquire_lock(key, client, unique_id, acquire_timeout): end = time.time() + acquire_timeout while time.time() &lt; end: if client.set(key, unique_id, ex=DEFAULT_EXPIRES, nx=True): return unique_id else: gen.sleep(.01) return Nonedef _release_lock(key, client, t): try: lua_command = &quot;&quot;&quot; if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1]) else return 0 end &quot;&quot;&quot; lua_script = client.register_script(lua_command) lock_logger.debug(lua_script(keys=[key], args=[t])) except Exception as e: lock_logger.error(traceback.format_exc()) unique_id：uuid1() 提供的全球唯一值，保证每个客户端释放的是自己的锁。 使用lua脚本释放锁是为了保证释放锁操作的原子性： Atomicity of scripts Redis uses the same Lua interpreter to run all the commands. Also Redis guarantees that a script is executed in an atomic way: no other script or Redis command will be executed while a script is being executed. This semantic is similar to the one of MULTI / EXEC. From the point of view of all the other clients the effects of a script are either still not visible or already completed. However this also means that executing slow scripts is not a good idea. It is not hard to create fast scripts, as the script overhead is very low, but if you are going to use slow scripts you should be aware that while the script is running no other client can execute commands. 获取锁时，给定超时时间是１s，避免出现死锁现象。 使用redis分布式锁的问题是，失效时间的设置。如果设置的失效时间太短，方法没等执行完，锁就自动释放了，那么就会产生并发问题。 分布式锁-基于RedLockRedlock 是 Redis 的作者 antirez 给出的集群模式的 Redis 分布式锁，它基于 N 个完全独立的 Redis 节点（通常情况下 N 可以设置成 5）。 算法的步骤如下： 客户端获取当前时间，以毫秒为单位； 客户端尝试获取 N 个节点的锁，（每个节点获取锁的方式和前面说的缓存锁一样），N 个节点以相同的 key 和 value 获取锁。客户端需要设置接口访问超时，接口超时时间需要远远小于锁超时时间，比如锁自动释放的时间是 10s，那么接口超时大概设置 5-50ms。这样可以在有 redis 节点宕机后，访问该节点时能尽快超时，而减小锁的正常使用； 客户端计算在获得锁的时候花费了多少时间，方法是用当前时间减去在步骤一获取的时间，只有客户端获得了超过 3 个节点的锁，而且获取锁的时间小于锁的超时时间，客户端才获得了分布式锁； 客户端获取的锁的时间为设置的锁超时时间减去步骤三计算出的获取锁花费时间； 如果客户端获取锁失败了，客户端会依次删除所有的锁。 使用 Redlock 算法，可以保证在挂掉最多 2 个节点的时候，分布式锁服务仍然能工作，这相比之前的数据库锁和缓存锁大大提高了可用性，由于 redis 的高效性能，分布式缓存锁性能并不比数据库锁差。 分布式锁-基于ZookeeperZooKeeper的架构通过冗余服务实现高可用性。因此，如果第一次无应答，客户端就可以询问另一台ZooKeeper主机。ZooKeeper节点将它们的数据存储于一个分层的命名空间，非常类似于一个文件系统或一个前缀树结构。客户端可以在节点读写，从而以这种方式拥有一个共享的配置服务，更新是全序的。 上图为系统架构，左边区域表示一个ZooKeeper集群，locker是ZooKeeper的一个持久节点，node_1、node_2、node_3是locker这个持久节点下面的临时顺序节点。client_1、client_2、client_3表示多个客户端，Share_Service表示需要互斥访问的共享资源。 Zookeeper实现分布式锁的思路： 多个客户端竞争创建 lock 临时节点； 其中某个客户端成功创建 lock 节点，其他客户端对 lock 节点设置 watcher； 持有锁的客户端删除 lock 节点或该客户端崩溃，由 Zookeeper 删除 lock 节点； 其他客户端获得 lock 节点被删除的通知； 重复上述4个步骤，直至无客户端在等待获取锁了。 Zookeeper方式性能上可能并没有缓存服务那么高，因为每次在创建锁和释放锁的过程中，都要动态创建、销毁临时节点来实现锁功能。ZK 中创建和删除节点只能通过 Leader 服务器来执行，然后将数据同步到所有的 Follower 机器上。 由于目前理解有限，一些细节需要进一步确定和实践，不当之处请指正。 Read More: 分布式锁的几种实现方式 Redis分布式锁的正确实现方式（Java版） 基于Redis的分布式锁到底安全吗（上）？ 分布式锁看这篇就够了 基于 Zookeeper 的分布式锁实现 Redis命令参考 Redis Lua scripting mysql document]]></content>
      <categories>
        <category>分布式锁</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>分布式锁</tag>
        <tag>redis</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka文件存储机制]]></title>
    <url>%2F2018%2F04%2F30%2Fkafkasave%2F</url>
    <content type="text"><![CDATA[Kafka Kafka是最初由 Linkedin 公司开发，是一个分布式、分区的、多副本的、多订阅者，基于 zookeeper 协调的分布式日志系统（也可以当做 MQ 系统），常见可以用于 web/nginx 日志、访问日志，消息服务等等， Linkedin 于2010年贡献给了 Apache 基金会并成为顶级开源项目。 一个商业化消息队列的性能好坏，其文件存储机制设计是衡量一个消息队列服务技术水平和最关键指标之一。下面将从 Kafka 文件存储机制和物理结构角度，分析 Kafka 是如何实现高效文件存储，及实际应用效果。 Kafka文件存储机制Kafka部分名词解释如下： Broker：消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群； Topic：一类消息，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发； Partition：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列； Segment：partition物理上由多个segment组成； offset：每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset,用于partition唯一标识一条消息。 分析过程分为以下4个步骤： topic中partition存储分布； partiton中文件存储方式； partiton中segment文件存储结构； 在partition中如何通过offset查找message。 通过上述4过程详细分析，我们就可以清楚认识到kafka文件存储机制的奥秘。 topic中partition存储分布假设实验环境中Kafka集群只有一个broker，xxx/message-folder为数据文件存储根目录，在Kafka broker中server.properties文件配置（参数log.dirs=xxx/message-folder），例如创建2个topic名称分别为report_push、launch_info, partitions数量都为partitions=4，存储路径和目录规则为：xxx/message-folder。 12345678|--report_push-0|--report_push-1|--report_push-2|--report_push-3|--launch_info-0|--launch_info-1|--launch_info-2|--launch_info-3 在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。 如果是多broker分布情况，请参考kafka集群partition分布原理分析 partiton中文件存储方式下面示意图形象说明了partition中文件存储方式： 每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除； 每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。 这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。 partiton中segment文件存储结构 segment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，后缀”.index”和”.log”分别表示为segment索引文件、数据文件； segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。 下面文件列表是笔者在Kafka broker上做的一个实验，创建一个topicXXX包含1 partition，设置每个segment大小为500MB，并启动producer向Kafka broker写入大量数据，如下图所示segment文件列表形象说明了上述2个规则： 以上述图2中一对segment file文件为例，说明segment中index&lt;—&gt;data file对应关系物理结构如下： 上述图3中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。 其中以索引文件中元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移地址为497。 从上图了解到segment data file由许多message组成，下面详细说明message物理结构如下： 关键字 解释 8 byte offset 在parition内的每条消息都有一个有序的id号，这个id号被称为偏移，它可以唯一确定每条消息在parition内的位置。即offset表示partiion的第多少message 4 byte message size message大小 4 byte CRC32 用crc32校验message 1 byte “magic” 表示本次发布Kafka服务程序协议版本号 1 byte “attributes” 表示为独立版本、或标识压缩类型、或编码类型。 4 byte key length 表示key的长度,当key为-1时，K byte key字段不填 K byte key 可选 value bytes payload 表示实际消息数据 在partition中如何通过offset查找message例如读取offset=368776的message，需要通过下面2个步骤查找。 第一步查找segment file上述图为例，其中00000000000000000000.index表示最开始的文件，起始偏移量（offset）为0，第二个文件00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1，同样，第三个文件00000000000000737337.index的起始偏移量为737338=737337 + 1，其他后续文件依次类推，以起始偏移量命名并排序这些文件，只要根据offset二分查找文件列表，就可以快速定位到具体文件。当offset=368776时定位到00000000000000368769.index|log； 第二步通过segment file查找message通过第一步定位到segment file，当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址，然后再通过00000000000000368769.log顺序查找直到offset=368776为止。 这样做的优点，segment index file采取稀疏索引存储方式，它减少索引文件大小，通过mmap可以直接内存操作，稀疏索引为数据文件的每个对应message设置一个元数据指针，它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。 实际运行效果实验环境： Kafka集群：由2台虚拟机组成 cpu：4核 物理内存：8GB 网卡：千兆网卡 jvm heap: 4GB 详细Kafka服务端配置及其优化请参考：kafka server.properties配置详解 从上图可以看出，Kafka运行时很少有大量读磁盘的操作，主要是定期批量写磁盘操作，因此操作磁盘很高效。这跟Kafka文件存储中读写message的设计是息息相关的。Kafka中读写message有如下特点: 写message 消息从堆转入page cache（即物理内存）； 由异步线程刷盘,消息从page cache刷入磁盘。 读message 消息直接从page cache转入socket发送出去； 当从page cache没有找到相应数据时，此时会产生磁盘IO，从磁盘Load消息到page cache，然后直接从socket发出去。 总结Kafka高效文件存储设计特点 Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用； 通过索引信息可以快速定位message和确定response的最大大小； 通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作； 通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。 Read More: Kafka文件存储机制那些事]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>文件存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-ls]]></title>
    <url>%2F2018%2F04%2F15%2Flinuxls%2F</url>
    <content type="text"><![CDATA[Linux ls命令用于显示指定工作目录下之内容（列出目前工作目录所含之文件及子目录），在Linux中是使用率较高的命令，ls命令的输出信息可以进行彩色加亮显示，以区分不同类型的文件。 命令格式1ls [OPTION]... [FILE]... 命令功能列出目标目录中所有的子目录和文件。 命令参数 参数 说明 -a 显示所有文件及目录（ls内定将文件名或目录名称开头为”.”的视为隐藏档，不会列出） -A 同 -a ，但不列出 “.”（当前目录）及 “..”（父目录） -C 多列显示输出结果。这是默认选项 -l 除文件名称外，亦将文件型态、权限、拥有者、文件大小等资讯详细列出 -F 在列出的文件名称后加一符号；例如可执行档则加 “*”, 目录则加 “/“ -c 与”-lt”选项连用时，按照文件状态时间排序输出目录内容，排序的依据是文件的索引节点中的ctime字段。与”-l”选项连用时，则排序的一句是文件的状态改变时间 -d 仅显示目录名，而不显示目录下的内容列表。显示符号链接文件本身，而不显示其所指向的目录列表 -f 此参数的效果和同时指定”aU”参数相同，并关闭”lst”参数的效果； -i 显示文件索引节点号（inode）。一个索引节点代表一个文件 -k 以KB（千字节）为单位显示文件大小 -m 用”,”号区隔每个文件和目录的名称 -n 以用户识别码和群组识别码替代其名称 -r 将文件以相反次序显示(原定依英文字母次序) -s 显示文件和目录的大小，以区块为单位 -t 将文件依建立时间之先后次序列出 -L 如果遇到性质为符号链接的文件或目录，直接列出该链接所指向的原始文件或目录 -R 递归列出该目录中的所有文件 –full-time 列出完整的日期与时间 –color[=WHEN] (WHEN=never/always/auto) 使用不同的颜色高亮显示不同类型的 -g 类似 -l，但不列出所有者 -h 以容易理解的格式列出文件大小 (例如 1K 234M 2G) -S 根据文件大小排序 -X 根据扩展名排序 -x 逐行列出项目而不是逐栏列出 使用示例 ls -l ly 1234567891011121314[root@hcdn-others-worker-dev100-bjlt home]# ls -l lytotal 48drwxr-xr-x 8 root root 4096 Dec 14 11:49 analysiswebdrwxr-xr-x 9 root root 4096 Dec 28 16:19 atsbtspdrwxr-xr-x 2 root root 4096 Sep 12 14:37 auth_server_ratiodrwxr-xr-x 7 root root 4096 Jan 2 10:10 datawritehbasedrwxr-xr-x 10 root root 4096 Dec 13 14:54 hbasequerywebdrwxr-xr-x 7 root root 4096 Oct 9 18:46 live_alarm_monitordrwxr-xr-x 9 root root 4096 Oct 27 11:59 openrestyworkdrwxr-xr-x 9 root root 4096 Dec 27 16:20 prepullvidcsdrwxr-xr-x 2 root root 4096 Nov 23 20:26 push_ffmpegdrwxr-xr-x 3 root root 4096 Jan 4 18:46 testkafkadrwxr-xr-x 3 root root 4096 Nov 29 14:32 virtualenvdrwxr-xr-x 5 root root 4096 Oct 12 10:27 xiu_server_stat 单列显示某目录。 ls 12[root@hcdn-others-worker-dev100-bjlt home]# lsly zabbix 显示当前目录下非影藏文件与目录 ls -a 12[root@hcdn-others-worker-dev100-bjlt home]# ls -a. .. ly zabbix 显示当前目录下包括影藏文件在内的所有文件列表 ls -l 12345[root@hcdn-others-worker-dev100-bjlt home]# [root@hcdn-others-worker-dev100-bjlt home]# ls -ltotal 8drwxr-xr-x 15 root root 4096 Jan 3 18:05 lydrwxr-xr-x. 2 zabbix zabbix 4096 Dec 21 18:05 zabbix 输出长格式列表 ls -i -l 1234[root@hcdn-others-worker-dev100-bjlt home]# ls -i -ltotal 8393222 drwxr-xr-x 15 root root 4096 Jan 3 18:05 ly393219 drwxr-xr-x. 2 zabbix zabbix 4096 Dec 21 18:05 zabbix 输出文件的inode信息 ls -m 12[root@hcdn-others-worker-dev100-bjlt home]# ls -mly, zabbix 水平输出文件列表 ls -t -l 1234567891011121314151617181920212223242526272829303132333435[root@hcdn-others-worker-dev100-bjlt ~]# ls -t -ltotal 303312drwxr-xr-x 2 root root 4096 Jan 4 17:54 testtardrwxrwxr-x 10 root root 4096 Jan 4 15:21 librdkafka-0.11.1drwxr-xr-x 11 root root 4096 Jan 4 14:26 pykafkadrwxr-xr-x 8 root root 12288 Jan 3 13:31 saramadrwxr-xr-x 10 root root 4096 Jan 3 13:26 go_kafka_client-rw-r--r-- 1 root root 859238 Jan 3 13:01 librdkafka-0.11.1.tar.gzdrwxrwxr-x 7 500 500 4096 Jan 2 11:40 node-v8.9.3-linux-x64drwxr-xr-x 10 502 games 4096 Jan 2 11:13 node-v8.9.3-rw-r--r-- 1 root root 31121503 Dec 8 23:23 node-v8.9.3.tar.gz-rw-r--r-- 1 root root 11395380 Dec 8 22:11 node-v8.9.3-linux-x64.tar.xz-rw-r--r-- 1 root root 1723533 Dec 5 01:02 redis-4.0.6.tar.gzdrwxrwxr-x 6 root root 4096 Dec 5 01:01 redis-4.0.6drwxrwxr-x 6 1000 1000 4096 Oct 27 11:35 openresty-1.11.2.5-rw-r--r-- 1 root root 4183884 Oct 27 10:55 openresty-1.11.2.5.tar.gzdrwxr-xr-x 9 1001 1001 4096 Aug 30 20:05 nginx-1.12.1drwxr-xr-x 9 1169 1169 12288 Aug 30 20:02 pcre-8.38-rw-r--r-- 1 root root 2053336 Aug 30 19:55 pcre-8.38.tar.gzdrwxr-xr-x 18 1000 1000 4096 Aug 28 15:15 Python-2.7.13drwxr-xr-x 11 1000 ftp 4096 Aug 22 12:01 thrift-0.10.0drwxr-xr-x 26 root root 4096 Aug 22 11:04 hbase-1.2.6drwxr-xr-x 7 501 games 4096 Aug 11 09:26 Twisted-17.5.0drwxr-xr-x 18 501 501 4096 Aug 11 09:13 Python-3.6.1-rw-r--r-- 1 root root 981093 Jul 11 23:45 nginx-1.12.1.tar.gz-rw-r--r-- 1 root root 194351104 Jul 7 15:25 long.flv-rw-r--r-- 1 root root 3340748 Jun 20 2017 thrift-0.10.0.tar.gz-rw-r--r-- 1 root root 2993816 Jun 11 2017 Twisted-17.5.0.tar.bz2-rw-r--r-- 1 root root 16054584 May 29 2017 hbase-1.2.6-src.tar.gz-rw-r--r-- 1 root root 22540566 Mar 21 2017 Python-3.6.1.tgz-rw-r--r-- 1 root root 17076672 Dec 18 2016 Python-2.7.13.tgz-rw-r--r-- 1 root root 1595408 Nov 7 2016 get-pip.py-rw-------. 1 root root 1267 Mar 12 2014 anaconda-ks.cfg-rw-r--r-- 1 root root 197981 Apr 25 2009 pyOpenSSL-0.9.tar.gzdrwxr-xr-x 9 1125 1125 4096 Apr 25 2009 pyOpenSSL-0.9 最近修改的文件显示在最前面 ls -F -l 1234567891011121314151617181920212223242526272829303132333435[root@hcdn-others-worker-dev100-bjlt ~]# ls -F -ltotal 303312-rw-------. 1 root root 1267 Mar 12 2014 anaconda-ks.cfg-rw-r--r-- 1 root root 1595408 Nov 7 2016 get-pip.pydrwxr-xr-x 10 root root 4096 Jan 3 13:26 go_kafka_client/drwxr-xr-x 26 root root 4096 Aug 22 11:04 hbase-1.2.6/-rw-r--r-- 1 root root 16054584 May 29 2017 hbase-1.2.6-src.tar.gzdrwxrwxr-x 10 root root 4096 Jan 4 15:21 librdkafka-0.11.1/-rw-r--r-- 1 root root 859238 Jan 3 13:01 librdkafka-0.11.1.tar.gz-rw-r--r-- 1 root root 194351104 Jul 7 15:25 long.flvdrwxr-xr-x 9 1001 1001 4096 Aug 30 20:05 nginx-1.12.1/-rw-r--r-- 1 root root 981093 Jul 11 23:45 nginx-1.12.1.tar.gzdrwxr-xr-x 10 502 games 4096 Jan 2 11:13 node-v8.9.3/drwxrwxr-x 7 500 500 4096 Jan 2 11:40 node-v8.9.3-linux-x64/-rw-r--r-- 1 root root 11395380 Dec 8 22:11 node-v8.9.3-linux-x64.tar.xz-rw-r--r-- 1 root root 31121503 Dec 8 23:23 node-v8.9.3.tar.gzdrwxrwxr-x 6 1000 1000 4096 Oct 27 11:35 openresty-1.11.2.5/-rw-r--r-- 1 root root 4183884 Oct 27 10:55 openresty-1.11.2.5.tar.gzdrwxr-xr-x 9 1169 1169 12288 Aug 30 20:02 pcre-8.38/-rw-r--r-- 1 root root 2053336 Aug 30 19:55 pcre-8.38.tar.gzdrwxr-xr-x 11 root root 4096 Jan 4 14:26 pykafka/drwxr-xr-x 9 1125 1125 4096 Apr 25 2009 pyOpenSSL-0.9/-rw-r--r-- 1 root root 197981 Apr 25 2009 pyOpenSSL-0.9.tar.gzdrwxr-xr-x 18 1000 1000 4096 Aug 28 15:15 Python-2.7.13/-rw-r--r-- 1 root root 17076672 Dec 18 2016 Python-2.7.13.tgzdrwxr-xr-x 18 501 501 4096 Aug 11 09:13 Python-3.6.1/-rw-r--r-- 1 root root 22540566 Mar 21 2017 Python-3.6.1.tgzdrwxrwxr-x 6 root root 4096 Dec 5 01:01 redis-4.0.6/-rw-r--r-- 1 root root 1723533 Dec 5 01:02 redis-4.0.6.tar.gzdrwxr-xr-x 8 root root 12288 Jan 3 13:31 sarama/drwxr-xr-x 2 root root 4096 Jan 4 17:54 testtar/drwxr-xr-x 11 1000 ftp 4096 Aug 22 12:01 thrift-0.10.0/-rw-r--r-- 1 root root 3340748 Jun 20 2017 thrift-0.10.0.tar.gzdrwxr-xr-x 7 501 games 4096 Aug 11 09:26 Twisted-17.5.0/-rw-r--r-- 1 root root 2993816 Jun 11 2017 Twisted-17.5.0.tar.bz2 安装特殊字符对文件进行分类 ls -l --color=auto | ls -l --color=never | ls -l --color=always 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105[root@hcdn-others-worker-dev100-bjlt ~]# ls -l --color=autototal 303312-rw-------. 1 root root 1267 Mar 12 2014 anaconda-ks.cfg-rw-r--r-- 1 root root 1595408 Nov 7 2016 get-pip.pydrwxr-xr-x 10 root root 4096 Jan 3 13:26 go_kafka_clientdrwxr-xr-x 26 root root 4096 Aug 22 11:04 hbase-1.2.6-rw-r--r-- 1 root root 16054584 May 29 2017 hbase-1.2.6-src.tar.gzdrwxrwxr-x 10 root root 4096 Jan 4 15:21 librdkafka-0.11.1-rw-r--r-- 1 root root 859238 Jan 3 13:01 librdkafka-0.11.1.tar.gz-rw-r--r-- 1 root root 194351104 Jul 7 15:25 long.flvdrwxr-xr-x 9 1001 1001 4096 Aug 30 20:05 nginx-1.12.1-rw-r--r-- 1 root root 981093 Jul 11 23:45 nginx-1.12.1.tar.gzdrwxr-xr-x 10 502 games 4096 Jan 2 11:13 node-v8.9.3drwxrwxr-x 7 500 500 4096 Jan 2 11:40 node-v8.9.3-linux-x64-rw-r--r-- 1 root root 11395380 Dec 8 22:11 node-v8.9.3-linux-x64.tar.xz-rw-r--r-- 1 root root 31121503 Dec 8 23:23 node-v8.9.3.tar.gzdrwxrwxr-x 6 1000 1000 4096 Oct 27 11:35 openresty-1.11.2.5-rw-r--r-- 1 root root 4183884 Oct 27 10:55 openresty-1.11.2.5.tar.gzdrwxr-xr-x 9 1169 1169 12288 Aug 30 20:02 pcre-8.38-rw-r--r-- 1 root root 2053336 Aug 30 19:55 pcre-8.38.tar.gzdrwxr-xr-x 11 root root 4096 Jan 4 14:26 pykafkadrwxr-xr-x 9 1125 1125 4096 Apr 25 2009 pyOpenSSL-0.9-rw-r--r-- 1 root root 197981 Apr 25 2009 pyOpenSSL-0.9.tar.gzdrwxr-xr-x 18 1000 1000 4096 Aug 28 15:15 Python-2.7.13-rw-r--r-- 1 root root 17076672 Dec 18 2016 Python-2.7.13.tgzdrwxr-xr-x 18 501 501 4096 Aug 11 09:13 Python-3.6.1-rw-r--r-- 1 root root 22540566 Mar 21 2017 Python-3.6.1.tgzdrwxrwxr-x 6 root root 4096 Dec 5 01:01 redis-4.0.6-rw-r--r-- 1 root root 1723533 Dec 5 01:02 redis-4.0.6.tar.gzdrwxr-xr-x 8 root root 12288 Jan 3 13:31 saramadrwxr-xr-x 2 root root 4096 Jan 4 17:54 testtardrwxr-xr-x 11 1000 ftp 4096 Aug 22 12:01 thrift-0.10.0-rw-r--r-- 1 root root 3340748 Jun 20 2017 thrift-0.10.0.tar.gzdrwxr-xr-x 7 501 games 4096 Aug 11 09:26 Twisted-17.5.0-rw-r--r-- 1 root root 2993816 Jun 11 2017 Twisted-17.5.0.tar.bz2[root@hcdn-others-worker-dev100-bjlt ~]# ls -l --color=nevertotal 303312-rw-------. 1 root root 1267 Mar 12 2014 anaconda-ks.cfg-rw-r--r-- 1 root root 1595408 Nov 7 2016 get-pip.pydrwxr-xr-x 10 root root 4096 Jan 3 13:26 go_kafka_clientdrwxr-xr-x 26 root root 4096 Aug 22 11:04 hbase-1.2.6-rw-r--r-- 1 root root 16054584 May 29 2017 hbase-1.2.6-src.tar.gzdrwxrwxr-x 10 root root 4096 Jan 4 15:21 librdkafka-0.11.1-rw-r--r-- 1 root root 859238 Jan 3 13:01 librdkafka-0.11.1.tar.gz-rw-r--r-- 1 root root 194351104 Jul 7 15:25 long.flvdrwxr-xr-x 9 1001 1001 4096 Aug 30 20:05 nginx-1.12.1-rw-r--r-- 1 root root 981093 Jul 11 23:45 nginx-1.12.1.tar.gzdrwxr-xr-x 10 502 games 4096 Jan 2 11:13 node-v8.9.3drwxrwxr-x 7 500 500 4096 Jan 2 11:40 node-v8.9.3-linux-x64-rw-r--r-- 1 root root 11395380 Dec 8 22:11 node-v8.9.3-linux-x64.tar.xz-rw-r--r-- 1 root root 31121503 Dec 8 23:23 node-v8.9.3.tar.gzdrwxrwxr-x 6 1000 1000 4096 Oct 27 11:35 openresty-1.11.2.5-rw-r--r-- 1 root root 4183884 Oct 27 10:55 openresty-1.11.2.5.tar.gzdrwxr-xr-x 9 1169 1169 12288 Aug 30 20:02 pcre-8.38-rw-r--r-- 1 root root 2053336 Aug 30 19:55 pcre-8.38.tar.gzdrwxr-xr-x 11 root root 4096 Jan 4 14:26 pykafkadrwxr-xr-x 9 1125 1125 4096 Apr 25 2009 pyOpenSSL-0.9-rw-r--r-- 1 root root 197981 Apr 25 2009 pyOpenSSL-0.9.tar.gzdrwxr-xr-x 18 1000 1000 4096 Aug 28 15:15 Python-2.7.13-rw-r--r-- 1 root root 17076672 Dec 18 2016 Python-2.7.13.tgzdrwxr-xr-x 18 501 501 4096 Aug 11 09:13 Python-3.6.1-rw-r--r-- 1 root root 22540566 Mar 21 2017 Python-3.6.1.tgzdrwxrwxr-x 6 root root 4096 Dec 5 01:01 redis-4.0.6-rw-r--r-- 1 root root 1723533 Dec 5 01:02 redis-4.0.6.tar.gzdrwxr-xr-x 8 root root 12288 Jan 3 13:31 saramadrwxr-xr-x 2 root root 4096 Jan 4 17:54 testtardrwxr-xr-x 11 1000 ftp 4096 Aug 22 12:01 thrift-0.10.0-rw-r--r-- 1 root root 3340748 Jun 20 2017 thrift-0.10.0.tar.gzdrwxr-xr-x 7 501 games 4096 Aug 11 09:26 Twisted-17.5.0-rw-r--r-- 1 root root 2993816 Jun 11 2017 Twisted-17.5.0.tar.bz2[root@hcdn-others-worker-dev100-bjlt ~]# ls -l --color=alwaystotal 303312-rw-------. 1 root root 1267 Mar 12 2014 anaconda-ks.cfg-rw-r--r-- 1 root root 1595408 Nov 7 2016 get-pip.pydrwxr-xr-x 10 root root 4096 Jan 3 13:26 go_kafka_clientdrwxr-xr-x 26 root root 4096 Aug 22 11:04 hbase-1.2.6-rw-r--r-- 1 root root 16054584 May 29 2017 hbase-1.2.6-src.tar.gzdrwxrwxr-x 10 root root 4096 Jan 4 15:21 librdkafka-0.11.1-rw-r--r-- 1 root root 859238 Jan 3 13:01 librdkafka-0.11.1.tar.gz-rw-r--r-- 1 root root 194351104 Jul 7 15:25 long.flvdrwxr-xr-x 9 1001 1001 4096 Aug 30 20:05 nginx-1.12.1-rw-r--r-- 1 root root 981093 Jul 11 23:45 nginx-1.12.1.tar.gzdrwxr-xr-x 10 502 games 4096 Jan 2 11:13 node-v8.9.3drwxrwxr-x 7 500 500 4096 Jan 2 11:40 node-v8.9.3-linux-x64-rw-r--r-- 1 root root 11395380 Dec 8 22:11 node-v8.9.3-linux-x64.tar.xz-rw-r--r-- 1 root root 31121503 Dec 8 23:23 node-v8.9.3.tar.gzdrwxrwxr-x 6 1000 1000 4096 Oct 27 11:35 openresty-1.11.2.5-rw-r--r-- 1 root root 4183884 Oct 27 10:55 openresty-1.11.2.5.tar.gzdrwxr-xr-x 9 1169 1169 12288 Aug 30 20:02 pcre-8.38-rw-r--r-- 1 root root 2053336 Aug 30 19:55 pcre-8.38.tar.gzdrwxr-xr-x 11 root root 4096 Jan 4 14:26 pykafkadrwxr-xr-x 9 1125 1125 4096 Apr 25 2009 pyOpenSSL-0.9-rw-r--r-- 1 root root 197981 Apr 25 2009 pyOpenSSL-0.9.tar.gzdrwxr-xr-x 18 1000 1000 4096 Aug 28 15:15 Python-2.7.13-rw-r--r-- 1 root root 17076672 Dec 18 2016 Python-2.7.13.tgzdrwxr-xr-x 18 501 501 4096 Aug 11 09:13 Python-3.6.1-rw-r--r-- 1 root root 22540566 Mar 21 2017 Python-3.6.1.tgzdrwxrwxr-x 6 root root 4096 Dec 5 01:01 redis-4.0.6-rw-r--r-- 1 root root 1723533 Dec 5 01:02 redis-4.0.6.tar.gzdrwxr-xr-x 8 root root 12288 Jan 3 13:31 saramadrwxr-xr-x 2 root root 4096 Jan 4 17:54 testtardrwxr-xr-x 11 1000 ftp 4096 Aug 22 12:01 thrift-0.10.0-rw-r--r-- 1 root root 3340748 Jun 20 2017 thrift-0.10.0.tar.gzdrwxr-xr-x 7 501 games 4096 Aug 11 09:26 Twisted-17.5.0-rw-r--r-- 1 root root 2993816 Jun 11 2017 Twisted-17.5.0.tar.bz2 单列列出文件并标记颜色 ls -l n* 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[root@hcdn-others-worker-dev100-bjlt ~]# ls -l n*-rw-r--r-- 1 root root 981093 Jul 11 23:45 nginx-1.12.1.tar.gz-rw-r--r-- 1 root root 11395380 Dec 8 22:11 node-v8.9.3-linux-x64.tar.xz-rw-r--r-- 1 root root 31121503 Dec 8 23:23 node-v8.9.3.tar.gznginx-1.12.1:total 732drwxr-xr-x 6 1001 1001 4096 Aug 30 20:00 auto-rw-r--r-- 1 1001 1001 277349 Jul 11 21:24 CHANGES-rw-r--r-- 1 1001 1001 422542 Jul 11 21:24 CHANGES.rudrwxr-xr-x 2 1001 1001 4096 Aug 30 20:00 conf-rwxr-xr-x 1 1001 1001 2481 Jul 11 21:24 configuredrwxr-xr-x 4 1001 1001 4096 Aug 30 20:00 contribdrwxr-xr-x 2 1001 1001 4096 Aug 30 20:00 html-rw-r--r-- 1 1001 1001 1397 Jul 11 21:24 LICENSE-rw-r--r-- 1 root root 376 Aug 30 20:11 Makefiledrwxr-xr-x 2 1001 1001 4096 Aug 30 20:00 mandrwxr-xr-x 3 root root 4096 Aug 30 20:12 objs-rw-r--r-- 1 1001 1001 49 Jul 11 21:24 READMEdrwxr-xr-x 9 1001 1001 4096 Aug 30 20:00 srcnode-v8.9.3:total 596-rwxr-xr-x 1 502 games 1944 Dec 8 23:22 android-configure-rw-r--r-- 1 502 games 62552 Dec 8 23:22 AUTHORSdrwxr-xr-x 32 502 games 4096 Jan 2 11:13 benchmark-rw-r--r-- 1 502 games 263 Dec 8 23:22 BSDmakefile-rw-r--r-- 1 502 games 13117 Dec 8 23:22 BUILDING.md-rw-r--r-- 1 502 games 53576 Dec 8 23:22 CHANGELOG.md-rw-r--r-- 1 502 games 181 Dec 8 23:22 CODE_OF_CONDUCT.md-rw-r--r-- 1 502 games 30242 Dec 8 23:22 COLLABORATOR_GUIDE.md-rw-r--r-- 1 502 games 14659 Dec 8 23:22 common.gypi-rw-r--r-- 1 root root 2748 Jan 2 11:13 config.gypi-rw-r--r-- 1 root root 223 Jan 2 11:13 config.mk-rwxr-xr-x 1 502 games 50245 Dec 8 23:22 configure-rw-r--r-- 1 502 games 37017 Dec 8 23:22 CONTRIBUTING.md-rw-r--r-- 1 502 games 3454 Dec 8 23:22 CPP_STYLE_GUIDE.mddrwxr-xr-x 13 502 games 4096 Jan 2 11:13 depsdrwxr-xr-x 6 502 games 4096 Jan 2 11:13 doc-rw-r--r-- 1 502 games 5832 Dec 8 23:22 GOVERNANCE.md-rw-r--r-- 1 root root 60153 Jan 2 11:13 icu_config.gypidrwxr-xr-x 3 502 games 4096 Jan 2 11:13 lib-rw-r--r-- 1 502 games 59059 Dec 8 23:22 LICENSE-rw-r--r-- 1 502 games 37468 Dec 8 23:22 Makefile-rw-r--r-- 1 502 games 24881 Dec 8 23:23 node.gyp-rw-r--r-- 1 502 games 10585 Dec 8 23:23 node.gypidrwxr-xr-x 5 root root 4096 Jan 2 11:14 out-rw-r--r-- 1 502 games 26525 Dec 8 23:22 README.mddrwxr-xr-x 4 502 games 4096 Jan 2 11:13 srcdrwxr-xr-x 22 502 games 4096 Jan 2 11:13 testdrwxr-xr-x 9 502 games 4096 Jan 2 11:13 tools-rw-r--r-- 1 502 games 24869 Dec 8 23:23 vcbuild.batnode-v8.9.3-linux-x64:total 164drwxrwxr-x 2 500 500 4096 Jan 2 12:14 bin-rw-rw-r-- 1 500 500 53576 Dec 8 22:10 CHANGELOG.mddrwxr-xr-x 2 root root 4096 Jan 2 11:40 etcdrwxrwxr-x 3 500 500 4096 Dec 8 22:10 includedrwxrwxr-x 3 500 500 4096 Dec 8 22:10 lib-rw-rw-r-- 1 500 500 59059 Dec 8 22:10 LICENSE-rw-rw-r-- 1 500 500 26525 Dec 8 22:10 README.mddrwxrwxr-x 5 500 500 4096 Dec 8 22:10 share 单列列出以某个字符开头的文件和目录的详细内容 Read More: 每天一个linux命令(1)：ls命令 Linux ls命令 ls命令]]></content>
      <categories>
        <category>Linux/Unix</category>
      </categories>
      <tags>
        <tag>命令</tag>
        <tag>Linux</tag>
        <tag>ls</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-cd]]></title>
    <url>%2F2018%2F04%2F15%2Flinuxcd%2F</url>
    <content type="text"><![CDATA[Linux cd命令用于切换当前工作目录至目标目录。其中目标目录表示法可为绝对路径或相对路径，若目录名称省略，则变换至使用者的 home 目录。 另外，”~” 也表示为 home 目录的意思，”.” 则是表示当前所在的目录，”..” 则表示目前目录位置的上一层目录。 命令格式1cd [dir](dir表示要切换的目录) 命令功能切换当前目录至dir。 命令参数暂无 使用示例 cd / | cd // 12345[root@hcdn-others-worker-dev100-bjlt local]# pwd/root/usr/local[root@hcdn-others-worker-dev100-bjlt local]# cd /[root@hcdn-others-worker-dev100-bjlt /]# pwd/ 进入系统根目录 cd .. 12345[root@hcdn-others-worker-dev100-bjlt local]# pwd/root/usr/local[root@hcdn-others-worker-dev100-bjlt local]# cd ..[root@hcdn-others-worker-dev100-bjlt usr]# pwd/root/usr 切换到父目录 cd - 1234567[root@hcdn-others-worker-dev100-bjlt local]# pwd/root/usr/local[root@hcdn-others-worker-dev100-bjlt local]# cd -/root/usr[root@hcdn-others-worker-dev100-bjlt usr]# cd[root@hcdn-others-worker-dev100-bjlt ~]# cd -/root/usr 切换到上次使用的目录 cd 12345[root@hcdn-others-worker-dev100-bjlt local]# pwd/root/usr/local[root@hcdn-others-worker-dev100-bjlt local]# cd[root@hcdn-others-worker-dev100-bjlt ~]# pwd/root 切换到当前用户主目录 cd ~ 1234[root@hcdn-others-worker-dev100-bjlt ~]# cd /root/usr/local/[root@hcdn-others-worker-dev100-bjlt local]# cd ~[root@hcdn-others-worker-dev100-bjlt ~]# pwd/root 切换到当前用户主目录 cd /root/usr/local/ 123[root@hcdn-others-worker-dev100-bjlt ~]# cd /root/usr/local/[root@hcdn-others-worker-dev100-bjlt local]# pwd/root/usr/local 切换到指定目录 cd !$ 123456[root@hcdn-others-worker-dev100-bjlt ~]# cd /root/usr/local/[root@hcdn-others-worker-dev100-bjlt local]# cd -/root[root@hcdn-others-worker-dev100-bjlt ~]# cd !$cd -/root/usr/local 将上一命令得参数作为 cd 参数使用 Read More: 每天一个linux命令(2)：cd命令 Linux cd命令]]></content>
      <categories>
        <category>Linux/Unix</category>
      </categories>
      <tags>
        <tag>命令</tag>
        <tag>Linux</tag>
        <tag>cd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[librdkafka的安装和使用]]></title>
    <url>%2F2018%2F01%2F04%2Flibrdkafkainstallanduse%2F</url>
    <content type="text"><![CDATA[kafka 是一种高吞吐量的分布式发布订阅消息系统。现在它已被 多家不同类型的公司 作为多种类型的数据管道和消息系统使用。Python对kafka的操作库主要有 pykafka 、confluent-kafka-python 、librdkafka ，pykafka是Python内置的kafka操作模块，纯Python编写，提供 simpleconsumer 、balancedconsumer 两种消费方式。 其实严格意义上来说，librdkafka并不是kafka的操作库，它是Apache Kafka协议的C库实现，包含Producer和Consumer支持。 librdkafka is a C library implementation of the Apache Kafka protocol, containing both Producer and Consumer support. It was designed with message delivery reliability and high performance in mind, current figures exceed 1 million msgs/second for the producer and 3 million msgs/second for the consumer. 由于项目的需要，原本使用pykafka作为数据的读取库，但是可能是达到了Python的读瓶颈，pykafka无法满足目前的要求，所以准备采用librdkafka来提高从kafka中读数据的效率。网上对Python环境中librdkafka的安装并没有详细的教程，通过自己的摸索和踩坑，终于是安装成功了。 安装librdkafka执行 1git clone https://github.com/edenhill/librdkafka.git 将librdkafka源码库下载到本地，如果没有安装git，请自行搜索安装。 如果github上无法下载，请转到百度网盘：链接: https://pan.baidu.com/s/1sl0uj2d 密码: af7k 下载的是0.11.1版本，本文安装的也是这个版本。 本文直接下载到/root/目录下，执行cd librdkafka-0.11.1进行目录 依次执行一下命令进行安装： 12./configuremake &amp;&amp; make install librdkafka依赖的环境如下，如果未安装请自行安装 123456The GNU toolchainGNU makepthreadszlib (optional, for gzip compression support)libssl-dev (optional, for SSL and SASL SCRAM support)libsasl2-dev (optional, for SASL GSSAPI support) 如果安装成功，会在/usr/local/lib目录中出现以下文件 12345678910[root@hcdn-others-worker-dev100-bjlt librdkafka-0.11.1]# cd /usr/local/lib[root@hcdn-others-worker-dev100-bjlt lib]# lltotal 15200-rwxr-xr-x 1 root root 8122982 Jan 4 15:21 librdkafka.a-rwxr-xr-x 1 root root 2305704 Jan 4 15:21 librdkafka++.alrwxrwxrwx 1 root root 15 Jan 4 15:21 librdkafka.so -&gt; librdkafka.so.1lrwxrwxrwx 1 root root 17 Jan 4 15:21 librdkafka++.so -&gt; librdkafka++.so.1-rwxr-xr-x 1 root root 4225307 Jan 4 15:21 librdkafka.so.1-rwxr-xr-x 1 root root 899617 Jan 4 15:21 librdkafka++.so.1drwxr-xr-x 2 root root 4096 Jan 4 15:21 pkgconfig librdkafka.so、librdkafka.so.1是librdkafka的动态链接库，因为Linux默认的动态链接库路径是/lib、/usr/lib，而librdkafka默认安装到了/usr/local/lib，为了使Python解释器可以找到librdkafka的动态链接库，本文采取的方法是修改系统文件/etc/ld.so.conf，这个文件中指定了默认的动态链接库查找路径，默认只有一行配置，如下 1include ld.so.conf.d/*.conf 可以看到，/etc/ld.so.conf包含了ld.so.conf.d目录中的所有.conf文件，所以我们新建一个配置文件 12345[root@hcdn-others-worker-dev100-bjlt ld.so.conf.d]# lltotal 24...-rw-r--r-- 1 root root 15 Jan 4 11:02 librdkafka.conf... 在librdkafka.conf文件中将/usr/local/lib路径包含进去 1/usr/local/lib 保存退出后执行以下命令更新动态链接库的搜索配置文件 1ldconfig 通过以上操作，Python解释器就可以找到刚才安装的librdkafka的动态链接库了。 安装pykafkaPython可以通过pip工具安装pykafka 1pip install pykafka 但是我们需要使用librdkafka扩展，使用pip工具是无法安装librdkafka的，所以我们需要源码编译安装pykafka。 执行 1git clone https://github.com/Parsely/pykafka.git 下载pykafka的源码包到本地，这时别着急执行./configure，文档中介绍：要使用librdkafka扩展，需要确保头文件和共享库是Python可以找到它们的地方，无论是在构建扩展还是在运行时。 PyKafka includes a C extension that makes use of librdkafka to speed up producer and consumer operation. To use the librdkafka extension, you need to make sure the header files and shared library are somewhere where python can find them, both when you build the extension (which is taken care of by setup.py develop) and at run time. Typically, this means that you need to either install librdkafka in a place conventional for your system, or declare C_INCLUDE_PATH, LIBRARY_PATH, and LD_LIBRARY_PATH in your shell environment to point to the installation location of the librdkafka shared objects. You can find this location with locate librdkafka.so. 我们已经将librdkafka的动态链接库加载到系统的搜索路径中，所以这个没问题，但是头文件在哪里？ 在pykafka/pykafka/rdkafka/_rd_kafkamodule.c文件中有这样一行代码 1#include &lt;librdkafka/rdkafka.h&gt; 所以我们需要在刚才下载的pykafka源码包中新建一个目录，将rdkafka.h放进去，保证我们在编译安装时能够找到它 12345[root@hcdn-others-worker-dev100-bjlt pykafka]# lltotal 128...drwxr-xr-x 2 root root 4096 Jan 4 14:22 librdkafka... rdkafka.h文件可以在刚才下载的librdkafka源码包中找到，即librdkafka/src/目录下 执行 1cp /root/librdkafka-0.11.1/src/rdkafka.h /root/pykafka/librdkafka/ 注意：/root代表librdkafka-0.11.1的存放目录 完成以上工作，依次执行 123cd pykafka/python setup.py buildpython setup.py install 不出意外的话，就可以安装成功，这时在Python的安装目录的site-packages/目录下会产生pykafka-2.7.0.dev2-py3.6-linux-x86_64.egg目录，内容如下 12345[root@hcdn-others-worker-dev100-bjlt pykafka-2.7.0.dev2-py3.6-linux-x86_64.egg]# lltotal 12drwxr-xr-x 2 root root 4096 Jan 4 15:22 EGG-INFOdrwxr-xr-x 7 root root 4096 Jan 4 15:22 pykafkadrwxr-xr-x 4 root root 4096 Jan 4 15:22 tests 表明安装成功了，在pykafka/rdkafka/目录中可以看到 12345678910[root@hcdn-others-worker-dev100-bjlt rdkafka]# lltotal 172-rw-r--r-- 1 root root 1188 Jan 4 15:22 helpers.py-rw-r--r-- 1 root root 89 Jan 4 15:22 __init__.py-rw-r--r-- 1 root root 8343 Jan 4 15:22 producer.pydrwxr-xr-x 2 root root 4096 Jan 4 16:07 __pycache__-rwxr-xr-x 1 root root 83815 Jan 4 15:22 _rd_kafka.cpython-36m-x86_64-linux-gnu.so-rw-r--r-- 1 root root 41405 Jan 4 15:22 _rd_kafkamodule.c-rw-r--r-- 1 root root 314 Jan 4 15:22 _rd_kafka.py-rw-r--r-- 1 root root 12674 Jan 4 16:04 simple_consumer.py 踩的坑 使用执行报错1 12pykafka.rdkafka#consumer-1 [PROTOERR] [thrd:openlive-kafka-online005-bjlt.qiyi.virtual:9092/bootstrap]: openlive-kafka-online005-bjlt.qiyi.virtual:9092/5: expected 36732025 bytes &gt; 1048618 remaining bytespykafka.rdkafka#consumer-1 [PROTOERR] [thrd:openlive-kafka-online005-bjlt.qiyi.virtual:9092/bootstrap]: openlive-kafka-online005-bjlt.qiyi.virtual:9092/5: Protocol parse failure at 8/1048626 (rd_kafka_fetch_reply_handle:2496) (incorrect broker.version.fallback?) 错误给出的提示是incorrect broker.version.fallback?，一番google后，找到了问题所在，请看 Broker version compatibility ，librdkafka不同版本的broker对程序的配置要求不同，主要是api.version.request 、broker.version.fallback这两个参数的配置，在pykafka/rdkafka/simple_consumer.py文件中有这样的代码： 123456789101112...ver10 = parse_version(self._broker_version) &gt;= parse_version(&quot;0.10.0&quot;)...conf = &#123; ... &quot;api.version.request&quot;: ver10, ...&#125;...if not ver10: conf[&quot;broker.version.fallback&quot;] = self._broker_version... 所以在使用时加上了broker_version参数，这个参数需要知道kafka集群的broker的版本，这个非常重要！ 1self.client = KafkaClient(zookeeper_hosts=zookeeper_hosts, broker_version=&quot;0.8.2&quot;) 这样使用就没有问题了。 但是在实际测试中，裸pykafka和pykafka-with-librdkafka的读性能并没有差太多，以下是我在实际中测试从kafka中读取50w条数据的耗时比较： pykafka pykafka-with-librdkafka 43.136744260787964 35.90807628631592 44.06431531906128 37.58351707458496 43.47287678718567 37.0159432888031 理论上，使用了C库的读取，其性能应该有很大提升才对，但是测试表明性能提升并不到，这个问题还有待探讨。]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>librdkafka</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-tar]]></title>
    <url>%2F2018%2F01%2F04%2Flinuxtar%2F</url>
    <content type="text"><![CDATA[通过SSH访问服务器，难免会要用到压缩、解压缩、打包、解包等操作，这时候tar命令是必不可少的一个功能强大的工具。 tar命令可以为Linux的文件和目录创建档案。利用tar可以为某一特定文件创建档案（备份文件），也可以在档案中改变文件，或者向档案中加入新的文件。tar最初被用来在磁带上创建档案，现在，用户可以在任何设备上创建档案。 首先区分一下打包和压缩的概念： 打包是将一大堆文件或目录变成一个总的文件； 压缩是将一个大的文件通过一些压缩算法变成一个小文件。 为什么要区分这两个概念呢？这源于Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令），然后再用压缩程序进行压缩（gzip、bzip2命令）。 Linux下最常用的打包程序就是tar了，使用tar程序打出来的包我们常称为tar包，tar包文件的命令通常都是以.tar结尾的。生成tar包后，就可以用其它的程序来进行压缩。 命令格式1tar [参数] [文件] 命令功能用来压缩和解压文件，tar本身不具有压缩功能，是调用压缩功能实现的压缩。 命令参数 参数 说明 -A 新增文件到以存在的备份文件 -B 设置区块大小 -c 建立新的备份文件 -C &lt;目录&gt; 这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项 -d 记录文件的差别 -r 添加文件到已经压缩的文件 -u 添加改变了和现有的文件到已经存在的压缩文件 -x 从备份文件中还原文件 -t 列出备份文件的内容 -z 通过gzip指令处理备份文件 -j 支持bzip2解压文件 -Z 通过compress指令处理备份文件 -v 显示操作过程 -l 文件系统边界设置 -k 保留原有文件不覆盖 -m 保留文件不被覆盖 -w 确认压缩文件的正确性 -f &lt;备份文件&gt; 指定备份文件 -b &lt;区块数目&gt; 设置每笔记录的区块数目，每个区块大小为12Bytes -p 用原来的文件权限还原文件 -P 文件名使用绝对名称，不移除文件名称前的”/“号 -N 只将较指定日期更新的文件保存到备份文件里 –version 显示版本信息 –exclude &lt;文件&gt; 排除某个文件 常见的解压/压缩命令.tar文件 12解包：tar xvf FileName.tar打包：tar cvf FileName.tar DirName .gz文件 123解压1：gunzip FileName.gz解压2：gzip -d FileName.gz压缩：gzip FileName .tar.gz文件 和 .tgz文件 12解压：tar zxvf FileName.tar.gz压缩：tar zcvf FileName.tar.gz DirName .bz2文件 123解压1：bzip2 -d FileName.bz2解压2：bunzip2 FileName.bz2压缩： bzip2 -z FileName .tar.bz2文件 12解压：tar jxvf FileName.tar.bz2压缩：tar jcvf FileName.tar.bz2 DirName .bz文件 123解压1：bzip2 -d FileName.bz解压2：bunzip2 FileName.bz压缩：未知 .tar.bz文件 12解压：tar jxvf FileName.tar.bz压缩：未知 .Z文件 12解压：uncompress FileName.Z压缩：compress FileName .tar.Z文件 12解压：tar Zxvf FileName.tar.Z压缩：tar Zcvf FileName.tar.Z DirName .zip文件 12解压：unzip FileName.zip压缩：zip FileName.zip DirName .rar文件 12解压：rar x FileName.rar压缩：rar a FileName.rar DirName 使用示例 tar -cvf testlog.tar testlog.log | tar -zcvf testlog.tar.gz testlog.log | tar -jcvf testlog.tar.bz2 testlog.log 123456789101112131415[root@hcdn-others-worker-dev100-bjlt testtar]# lltotal 4-rw-r--r-- 1 root root 94 Jan 4 17:07 testlog.log[root@hcdn-others-worker-dev100-bjlt testtar]# tar -cvf testlog.tar testlog.log testlog.log[root@hcdn-others-worker-dev100-bjlt testtar]# tar -zcvf testlog.tar.gz testlog.log testlog.log[root@hcdn-others-worker-dev100-bjlt testtar]# tar -jcvf testlog.tar.bz2 testlog.log testlog.log[root@hcdn-others-worker-dev100-bjlt testtar]# lltotal 24-rw-r--r-- 1 root root 94 Jan 4 17:07 testlog.log-rw-r--r-- 1 root root 10240 Jan 4 17:08 testlog.tar-rw-r--r-- 1 root root 128 Jan 4 17:08 testlog.tar.bz2-rw-r--r-- 1 root root 125 Jan 4 17:08 testlog.tar.gz 将文件只打包不压缩、以gzip压缩、以bzip2压缩。 tar -ztvf testlog.tar.gz 12[root@hcdn-others-worker-dev100-bjlt testtar]# tar -ztvf testlog.tar.gz -rw-r--r-- root/root 94 2018-01-04 17:07 testlog.log 查看testlog.tar.gz中有哪些文件。 tar -zxvf testlog.tar.gz 123456789101112131415[root@hcdn-others-worker-dev100-bjlt testtar]# lltotal 24drwxr-xr-x 2 root root 4096 Jan 4 17:17 testlog-rw-r--r-- 1 root root 10240 Jan 4 17:08 testlog.tar-rw-r--r-- 1 root root 128 Jan 4 17:08 testlog.tar.bz2-rw-r--r-- 1 root root 125 Jan 4 17:08 testlog.tar.gz[root@hcdn-others-worker-dev100-bjlt testtar]# tar -zxvf testlog.tar.gztestlog.log[root@hcdn-others-worker-dev100-bjlt testtar]# lltotal 28drwxr-xr-x 2 root root 4096 Jan 4 17:17 testlog-rw-r--r-- 1 root root 94 Jan 4 17:07 testlog.log-rw-r--r-- 1 root root 10240 Jan 4 17:08 testlog.tar-rw-r--r-- 1 root root 128 Jan 4 17:08 testlog.tar.bz2-rw-r--r-- 1 root root 125 Jan 4 17:08 testlog.tar.gz 将testlog.tar.gz解压缩。 tar -zcvf testlogs.tar.gz testlog1.log 1234567891011121314151617181920212223242526272829303132333435[root@hcdn-others-worker-dev100-bjlt testtar]# tar -zcvf testlogs.tar.gz testlog1.log testlog2.log testlog1.logtestlog2.log[root@hcdn-others-worker-dev100-bjlt testtar]# lltotal 36drwxr-xr-x 2 root root 4096 Jan 4 17:17 testlog-rw-r--r-- 1 root root 13 Jan 4 17:21 testlog1.log-rw-r--r-- 1 root root 25 Jan 4 17:21 testlog2.log-rw-r--r-- 1 root root 152 Jan 4 17:21 testlogs.tar.gz-rw-r--r-- 1 root root 10240 Jan 4 17:08 testlog.tar-rw-r--r-- 1 root root 128 Jan 4 17:08 testlog.tar.bz2-rw-r--r-- 1 root root 125 Jan 4 17:08 testlog.tar.gz[root@hcdn-others-worker-dev100-bjlt testtar]# tar -ztvf testlogs.tar.gz -rw-r--r-- root/root 13 2018-01-04 17:21 testlog1.log-rw-r--r-- root/root 25 2018-01-04 17:21 testlog2.log[root@hcdn-others-worker-dev100-bjlt testtar]# rm -rf testlog1.log [root@hcdn-others-worker-dev100-bjlt testtar]# lltotal 32drwxr-xr-x 2 root root 4096 Jan 4 17:17 testlog-rw-r--r-- 1 root root 25 Jan 4 17:21 testlog2.log-rw-r--r-- 1 root root 152 Jan 4 17:21 testlogs.tar.gz-rw-r--r-- 1 root root 10240 Jan 4 17:08 testlog.tar-rw-r--r-- 1 root root 128 Jan 4 17:08 testlog.tar.bz2-rw-r--r-- 1 root root 125 Jan 4 17:08 testlog.tar.gz[root@hcdn-others-worker-dev100-bjlt testtar]# tar -zxvf testlogs.tar.gz testlog1.logtestlog1.log[root@hcdn-others-worker-dev100-bjlt testtar]# lltotal 36drwxr-xr-x 2 root root 4096 Jan 4 17:17 testlog-rw-r--r-- 1 root root 13 Jan 4 17:21 testlog1.log-rw-r--r-- 1 root root 25 Jan 4 17:21 testlog2.log-rw-r--r-- 1 root root 152 Jan 4 17:21 testlogs.tar.gz-rw-r--r-- 1 root root 10240 Jan 4 17:08 testlog.tar-rw-r--r-- 1 root root 128 Jan 4 17:08 testlog.tar.bz2-rw-r--r-- 1 root root 125 Jan 4 17:08 testlog.tar.gz 从压缩包中解压出指定文件。 tar -N “2018/01/03” -zcvf testlog.tar.gz test: 指定文件夹中的文件比某个日期新的才备份 tar –exclude testlog/testlog1.log -zcvf testlogs.tar.gz testlog/*: 排除文件夹中的某个文件 Read More： 每天一个linux命令（28）：tar命令 Linux tar命令 tar]]></content>
      <categories>
        <category>Linux/Unix</category>
      </categories>
      <tags>
        <tag>命令</tag>
        <tag>Linux</tag>
        <tag>tar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-ln]]></title>
    <url>%2F2018%2F01%2F04%2Flinuxln%2F</url>
    <content type="text"><![CDATA[Linux ln命令是一个非常重要命令，它的功能是为某一个文件在另外一个位置建立一个同步的链接。当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在 其它的目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。 命令格式1ln [参数] [源文件或目录] [目标文件或目录] 命令功能Linux文件系统中，有所谓的链接（link），我们可以将其视为档案的别名，而链接又可分为两种：硬链接（hard link）与软链接（symbolic link），硬链接的意思是一个档案可以有多个名称，而软链接的方式则是产生一个特殊的档案，该档案的内容是指向另一个档案的位置。硬链接是存在同一个文件系统中，而软链接却可以跨越不同的文件系统。 不论是硬链接或软链接都不会将原本的档案复制一份，只会占用非常少量的磁碟空间。 软链接 软链接，以路径的形式存在。类似于Windows操作系统中的快捷方式 软链接可以 跨文件系统 ，硬链接不可以 软链接可以对一个不存在的文件名进行链接 软链接可以对目录进行链接 硬链接 硬链接，以文件副本的形式存在。但不占用实际空间 不允许给目录创建硬链接 硬链接只有在同一个文件系统中才能创建 注意 ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化； ln的链接又分软链接和硬链接两种，软链接就是ln –s 源文件 目标文件，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接ln 源文件 目标文件，没有参数-s，它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化； ln指令用在链接文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则会把前面指定的所有文件或目录复制到该目录中。若同时指定多个文件或目录，且最后的目的地并非是一个已存在的目录，则会出现错误信息。 命令参数 参数 说明 -b 删除，覆盖以前建立的链接 -d 允许超级用户制作目录的硬链接 -f 强制执行 -i 交互模式，文件存在则提示用户是否覆盖 -n 把符号链接视为一般目录 -s 软链接(符号链接) -v 显示详细的处理过程 -S “-S&lt;字尾备份字符串&gt; “或 “–suffix=&lt;字尾备份字符串&gt;” -V “-V&lt;备份方式&gt;”或”–version-control=&lt;备份方式&gt;” –help 显示帮助信息 –version 显示版本信息 使用示例 ln -s cplog.log cplog-link.log 12345[root@hcdn-others-worker-dev100-bjlt testln]# ln -s cplog.log cplog-link.log [root@hcdn-others-worker-dev100-bjlt testln]# lltotal 4lrwxrwxrwx 1 root root 9 Jan 3 20:25 cplog-link.log -&gt; cplog.log-rw-r--r-- 1 root root 28 Jan 3 20:00 cplog.log 为cplog.log文件创建软链接cplog-link.log，如果cplog.log丢失，其软链接cplog-link.log也将失效。 ln cplog.log cplog-hard.log 123456[root@hcdn-others-worker-dev100-bjlt testln]# ln cplog.log cplog-hard.log [root@hcdn-others-worker-dev100-bjlt testln]# lltotal 8-rw-r--r-- 2 root root 28 Jan 3 20:00 cplog-hard.loglrwxrwxrwx 1 root root 9 Jan 3 20:25 cplog-link.log -&gt; cplog.log-rw-r--r-- 2 root root 28 Jan 3 20:00 cplog.log 为cplog.log创建硬链接cplog-hard.log，cplog.log与cplog-hard.log的各项属性相同。 ln cplog.log test/ 1234567891011121314151617[root@hcdn-others-worker-dev100-bjlt testln]# ln cplog.log test/[root@hcdn-others-worker-dev100-bjlt testln]# lltotal 12-rw-r--r-- 3 root root 28 Jan 3 20:00 cplog-hard.loglrwxrwxrwx 1 root root 9 Jan 3 20:25 cplog-link.log -&gt; cplog.log-rw-r--r-- 3 root root 28 Jan 3 20:00 cplog.logdrwxr-xr-x 2 root root 4096 Jan 3 20:31 test[root@hcdn-others-worker-dev100-bjlt testln]# vi cplog.log [root@hcdn-others-worker-dev100-bjlt testln]# lltotal 12-rw-r--r-- 3 root root 72 Jan 3 20:31 cplog-hard.loglrwxrwxrwx 1 root root 9 Jan 3 20:25 cplog-link.log -&gt; cplog.log-rw-r--r-- 3 root root 72 Jan 3 20:31 cplog.logdrwxr-xr-x 2 root root 4096 Jan 3 20:31 test[root@hcdn-others-worker-dev100-bjlt testln]# ll test/total 4-rw-r--r-- 3 root root 72 Jan 3 20:31 cplog.log 将文件cplog.log创建硬链接到当前目录中已存在的目录，当修改./test/cplog.log文件时，会同步到源文件中。 ln -sv /root/testln/test /root/testln/test2 1234567891011121314151617[root@hcdn-others-worker-dev100-bjlt testln]# ll test/total 0[root@hcdn-others-worker-dev100-bjlt testln]# ll test2/total 0[root@hcdn-others-worker-dev100-bjlt testln]# ln -sv test test2`test2/test&apos; -&gt; `test&apos;[root@hcdn-others-worker-dev100-bjlt testln]# ll test2total 0lrwxrwxrwx 1 root root 4 Jan 3 20:38 test -&gt; test[root@hcdn-others-worker-dev100-bjlt testln]# rm -rf test2/test [root@hcdn-others-worker-dev100-bjlt testln]# ll test2total 0[root@hcdn-others-worker-dev100-bjlt testln]# ln -sv /root/testln/test /root/testln/test2`/root/testln/test2/test&apos; -&gt; `/root/testln/test&apos;[root@hcdn-others-worker-dev100-bjlt testln]# ll test2total 0lrwxrwxrwx 1 root root 17 Jan 3 20:39 test -&gt; /root/testln/test 创建test目录到test2目录的软链接时，必须使用绝对路径，如果使用相对路径，则目的路径中的链接不断闪烁，表示链接失败。 目录链接只能是软链接，在链接目标目录中修改文件都会在源文件目录中同步变化。 Read More: 每天一个linux命令（35）：ln 命令 Linux ln命令]]></content>
      <categories>
        <category>Linux/Unix</category>
      </categories>
      <tags>
        <tag>命令</tag>
        <tag>Linux</tag>
        <tag>ln</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-cp]]></title>
    <url>%2F2018%2F01%2F04%2Flinuxcp%2F</url>
    <content type="text"><![CDATA[Linux cp命令主要用于复制文件或目录，一般情况下，shell会设置一个别名，在命令行下复制文件时，如果目标文件已经存在，就会询问是否覆盖，不管你是否使用-i参数。但是如果是在shell脚本中执行cp时，没有-i参数时不会询问是否覆盖。这说明命令行和shell脚本的执行方式有些不同。 命令格式12cp [options] source destcp [options] source... directory 命令功能将源文件复制至目标文件，或将多个源文件复制至目标目录。 命令参数 参数 说明 -a 此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容 -b 为每个已存在的目标文件创建备份 -d 复制时保留链接。这里所说的链接相当于Windows系统中的快捷方式 -f 覆盖已经存在的目标文件而不给出提示 -i 与-f选项相反，在覆盖目标文件之前给出提示 -H 跟随源文件中的命令行符号链接 -l 不复制文件，只是生成链接文件 -L 总是跟随符号链接 -n 不要覆盖已存在的文件 -P 不跟随源文件中的符号链接 -p 除复制文件的内容外，还把修改时间和访问权限也复制到新文件中 -R / -r 若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件 -s 复制时创建新的链接，类似于Windows系统中的快捷方式 使用示例 cp cplog.log test1/ 12345678910111213复制前：[root@hcdn-others-worker-dev100-bjlt testcp]# lltotal 12-rw-r--r-- 1 root root 28 Jan 3 20:00 cplog.logdrwxr-xr-x 2 root root 4096 Jan 3 20:00 test1drwxr-xr-x 2 root root 4096 Jan 3 20:00 test2[root@hcdn-others-worker-dev100-bjlt testcp]# ll test1/total 0复制后：[root@hcdn-others-worker-dev100-bjlt testcp]# ll test1/total 4-rw-r--r-- 1 root root 28 Jan 3 20:01 cplog.log 在没有带-a参数时，两个文件的时间是不一样的。在带了-a参数时，两个文件的时间是一致的。 cp cplog.log test1/ 12345[root@hcdn-others-worker-dev100-bjlt testcp]# cp cplog.log test1/cp: overwrite `test1/cplog.log&apos;? y[root@hcdn-others-worker-dev100-bjlt testcp]# ll test1/total 4-rw-r--r-- 1 root root 28 Jan 3 20:04 cplog.log 在目标目录中存在同名文件时，会询问是否覆盖，这是因为cp是cp -i的别名。目标文件存在时，即使加了-f标志，也还会询问是否覆盖。 cp -a test1 test2 123456789101112131415161718192021222324目的目录存在：[root@hcdn-others-worker-dev100-bjlt testcp]# lltotal 12-rw-r--r-- 1 root root 28 Jan 3 20:00 cplog.logdrwxr-xr-x 2 root root 4096 Jan 3 20:01 test1drwxr-xr-x 3 root root 4096 Jan 3 20:06 test2[root@hcdn-others-worker-dev100-bjlt testcp]# ll test1total 4-rw-r--r-- 1 root root 28 Jan 3 20:04 cplog.log[root@hcdn-others-worker-dev100-bjlt testcp]# ll test2total 4drwxr-xr-x 2 root root 4096 Jan 3 20:01 test1目的目录不存在：[root@hcdn-others-worker-dev100-bjlt testcp]# cp -a test1 test3[root@hcdn-others-worker-dev100-bjlt testcp]# lltotal 16-rw-r--r-- 1 root root 28 Jan 3 20:00 cplog.logdrwxr-xr-x 2 root root 4096 Jan 3 20:01 test1drwxr-xr-x 3 root root 4096 Jan 3 20:06 test2drwxr-xr-x 2 root root 4096 Jan 3 20:01 test3[root@hcdn-others-worker-dev100-bjlt testcp]# ll test3/total 4-rw-r--r-- 1 root root 28 Jan 3 20:04 cplog.log 注意目标目录存在与否结果是不一样的。目标目录存在时，整个源目录被复制到目标目录里面。 cp -s cplog.log cplog-1.log 12345678[root@hcdn-others-worker-dev100-bjlt testcp]# cp -s cplog.log cplog-1.log [root@hcdn-others-worker-dev100-bjlt testcp]# lltotal 16lrwxrwxrwx 1 root root 9 Jan 3 20:09 cplog-1.log -&gt; cplog.log-rw-r--r-- 1 root root 28 Jan 3 20:00 cplog.logdrwxr-xr-x 2 root root 4096 Jan 3 20:01 test1drwxr-xr-x 3 root root 4096 Jan 3 20:06 test2drwxr-xr-x 2 root root 4096 Jan 3 20:01 test3 cplog-1.log是由 -s 的参数造成的，建立的是一个快捷方式，所以会看到在文件的最右边，显示这个文件是连结到的目的地。 Read More: 每天一个linux命令（8）：cp 命令 Linux cp命令]]></content>
      <categories>
        <category>Linux/Unix</category>
      </categories>
      <tags>
        <tag>命令</tag>
        <tag>Linux</tag>
        <tag>cp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows查看端口占用并关闭进程]]></title>
    <url>%2F2018%2F01%2F02%2Fwindowskillprocess%2F</url>
    <content type="text"><![CDATA[开始–&gt;运行–&gt;cmd，进入命令提示符输入netstat -ano 即可看到所有连接的PID，然后在任务管理器中找到这个PID所对应的程序，如果任务管理器中没有PID这一项，可以在任务管理器中选”查看” -&gt; “选择列”。 经常，我们在启动应用的时候发现系统需要的端口被别的程序占用，如何知道谁占有了我们需要的端口，下面介绍一种非常简单的方法。 假设我们需要确定哪个进程占用了8083端口。 查看所有端口的占用情况netstat -ano，结果如图 查看指定端口的占用情况netstat -aon | findstr &quot;9050&quot;，结果如图 可以看到8083端口被进程号为2648的进程占用。 查看PID对应的进程tasklist | findstr &quot;2648&quot;，结果如图 杀死相应进程tasklist|findstr &quot;2016&quot;，结果如图]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>端口占用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python上下文管理器]]></title>
    <url>%2F2018%2F01%2F02%2Fpythoncontextmanager%2F</url>
    <content type="text"><![CDATA[何为上下文管理器上下文管理器是管理上下文的，负责冲锋和垫后，而让开发人员专心完成自己的事情。我们在编写程序的时候，通常会将一系列操作放到一个语句块中，当某一条件为真时执行该语句快。有时候，我们需要再执行一个语句块时保持某种状态，并且在离开语句块后结束这种状态。 例如对文件的操作，我们在打开一个文件进行读写操作时需要保持文件处于打开状态，而等操作完成之后要将文件关闭。所以，上下文管理器的任务是：代码块执行前准备，代码块执行后收拾。上下文管理器是在Python2.5加入的功能，它能够让你的代码可读性更强并且错误更少。 需求的产生在正常的管理各种系统资源（文件、锁定和连接），在涉及到异常时通常是个棘手的问题。异常很可能导致控制流跳过负责释放关键资源的语句。例如打开一个文件进行操作时，如果意外情况发生（磁盘已满、特殊的终端信号让其终止等），就会抛出异常，这样可能最后的文件关闭操作就不会执行。如果这样的问题频繁出现，则可能耗尽系统资源。 在没有接触到上下文管理器之前，我们可以用”try/finally”语句来解决这样的问题。或许在有些人看来，”try/finally”语句显得有些繁琐。上下文管理器就是被设计用来简化”try/finally”语句的，这样可以让程序更加简洁。 上下文管理协议那么在Python中怎么实现一个上下文管理器呢？主要依靠__enter__、__exit__这两个”魔术方法”。 __enter__(self) Defines what the context manager should do at the beginning of the block created by the with statement. Note that the return value of __enter__ is bound to the target of the with statement, or the name after the as. __exit__(self, exception_type, exception_value, traceback) Defines what the context manager should do after its block has been executed (or terminates). It can be used to handle exceptions, perform cleanup, or do something always done immediately after the action in the block. If the block executes successfully, exception_type, exceptionvalue, and traceback will be None. Otherwise, you can choose to handle the exception or let the user handle it; if you want to handle it, make sure _exit__ returns True after all is said and done. If you don’t want the exception to be handled by the context manager, just let it happen. 也就是说，当我们需要创建一个上下文管理器类型的时候，就需要实现__enter__和__exit__方法，这对方法就称为上下文管理协议（Context Manager Protocol），定义了一种运行时上下文环境。 with语句在Python中，可以通过with语句来方便的使用上下文管理器，with语句可以在代码块运行前进入一个运行时上下文（执行__enter__方法），并在代码块结束后退出该上下文（执行__exit__方法）。 with语句的语法如下： 12with context_expr [as var]: with_suite context_expr: 支持上下文管理协议的对象，也就是上下文管理器对象，负责维护上下文环境 as var: 可选部分，通过变量方式保存上下文管理器对象 with_suite: 需要放在上下文环境中执行的语句块 在Python的内置类型中，很多类型都是支持上下文管理协议的，例如file、thread.LockType、threading.Lock等等。这里我们就以file类型为例，看看with语句的使用。 12with File(&apos;demo.txt&apos;, &apos;w&apos;) as opened_file: opened_file.write(&apos;Hola!&apos;) with语句先暂存了File类的__exit__()方法，然后它调用File类的__enter__()方方法，__enter__方法打开文件并返回给with语句，打开的文件句柄被传递给opened_file参数，然后使用.write()来写文件，with语句调用之前暂存的__exit__方法关闭了文件。 自定义上下文管理器了解上下文管理器的执行流程和方法后，我们可以通过实现__enter__()、__exit__()函数来自定义上下文管理器，如下： 1234567891011121314151617181920filename = &apos;my_file.txt&apos;mode = &apos;w&apos; # Mode that allows to write to the filewriter = open(filename, mode)class PypixOpen(object): def __init__(self, filename, mode): self.filename = filename self.mode = mode def __enter__(self): self.openedFile = open(self.filename, self.mode) return self.openedFile def __exit__(self, *unused): self.openedFile.close()# Script starts from herewith PypixOpen(filename, mode) as writer: writer.write(&quot;Hello World from our new Context Manager!&quot;) 异常处理上下文管理器根据__exit__() 方法的返回值来决定是否抛出异常，如果没有返回值或者返回值为 False ，则异常由上下文管理器处理，如果为 True 则由用户自己处理。 __exit__ ()接受三个参数，exception_type、exception_value、traceback，我们可以根据这些值来决定是否处理异常 下面这个例子，捕捉了 AttributeError 的异常，并打印出警告: 12345678910111213141516class Open: def __init__(self, file, mode): self.open_file = open(file, mode) def __enter__(self): return self.open_file def __exit__(self, type, value, tb): self.open_file.close() if type is AttributeError: print(&apos;handing some exception&apos;) return True with Open(&apos;aaa&apos;, &apos;w&apos;) as f: f.writeee(&apos;aaaa&apos;) &gt;&gt;&gt; handing some exception contextmanager由于上下文管理非常有用，Python 中有一个专门用于实现上下文管理的标准库，这就是 contextlib。 有了 contextlib 创建上下文管理的最好方式就是使用 contextmanager 装饰器，通过 contextmanager 装饰一个生成器函数，yield 语句前面的部分被认为是__enter__() 方法的代码，后面的部分被认为是 __exit__()方法的代码。 123456from contextlib import contextmanager@contextmanagerdef file(path, mode): open_file = open(path, mode) yield open_file open_file.close() Python解释器遇到了yield关键字。因为这个缘故它创建了一个生成器而不是一个普通的函数。因为这个装饰器，contextmanager会被调用并传入函数名（file）作为参数。contextmanager函数返回一个以GeneratorContextManager对象封装过的生成器。这个GeneratorContextManager被赋值给file函数，我们实际上是在调用GeneratorContextManager对象。 Read More: 上下文管理器(Context managers) Python上下文管理器与with语句 Python上下文管理器 Python - 上下文管理]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>上下文管理器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-netstat]]></title>
    <url>%2F2018%2F01%2F02%2Flinuxnetstat%2F</url>
    <content type="text"><![CDATA[命令格式1netstat [-acCeFghilMnNoprstuvVwx][-A&lt;网络类型&gt;][--ip] 命令功能Linux netstat命令用于显示网络状态，一般用于检验本机各端口的网络连接情况。netstat是在内核中访问网络及相关信息的程序，它能提供TCP连接，TCP和UDP监听，进程内存管理的相关报告。 命令参数 参数 说明 -a / -all 显示所有连接中的Socket -A&lt;网络类型&gt; / –&lt;网络类型&gt; 列出该网络类型连接中的相关地址 -c / –continuous 持续列出网络状态 -C / –cache 显示路由器配置的快取信息 -e / –extend 显示网络其他相关信息 -F / –fib 显示FIB -g / –groups 显示多重广播功能群组组员名单 -h / –help 在线帮助 -i / –interfaces 显示网络界面信息表单 -l / –listening 显示监控中的服务器的Socket -M / –masquerade 显示伪装的网络连线 -n / –numeric 直接使用IP地址，而不通过域名服务器 -N / –netlink / –symbolic 显示网络硬件外围设备的符号连接名称 -o / –timers 显示计时器 -p / –programs 显示正在使用Socket的程序识别码和程序名称 -r / –route 显示路由表 -s / –statistice 显示网络工作信息统计表 -t / –tcp 显示TCP传输协议的连线状况 -u / –udp 显示UDP传输协议的连线状况 -v / –verbose 显示指令执行过程 -V / –version 显示版本信息 -w / –raw 显示RAW传输协议的连线状况 -x / –unix 此参数的效果和指定”-A unix”参数相同 –ip / –inet 此参数的效果和指定”-A inet”参数相同 使用示例 netstat 1234567891011121314Active Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 hcdn-others-worker-de:62710 10.15.207.143:8433 ESTABLISHED tcp 0 0 hcdn-others-worker-de:30881 openlive-hbase-online:websm ESTABLISHED tcp 0 0 hcdn-others-worker-dev:9546 10.153.149.218:XmlIpcRegSvc ESTABLISHED tcp 0 0 hcdn-others-worker-de:53486 10.153.149.192:XmlIpcRegSvc ESTABLISHED tcp 0 0 hcdn-others-worker-de:38198 10.153.149.213:XmlIpcRegSvc ESTABLISHED tcp 0 0 hcdn-others-worker-de:24007 10.153.149.214:XmlIpcRegSvc ESTABLISHED Active UNIX domain sockets (w/o servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ] DGRAM 17227 @/org/freedesktop/hal/udev_eventunix 2 [ ] DGRAM 12672370 @/org/kernel/udev/udevdunix 3 [ ] STREAM CONNECTED 253334334 unix 3 [ ] STREAM CONNECTED 253334333 netstat 命令的输出结果可以分为两个部分：Active Internet connections（有源TCP连接），其中”Recv-Q”和”Send-Q”指的是接收队列和发送队列，这些数字一般都应该是0，如果不是则表示软件包正在队列中堆积；Active UNIX domain sockets（有源Unix域套接口），和网络套接字一样，但只能用于本机通信，性能可以提高一倍。 Proto显示连接使用的协议，RefCnt显示连接到本套接口上的进程号，Type显示套接口的类型，State显示套接口当前的状态，Path表示连接到套接口的其它进程使用的路径名。 状态 说明 LISTEN 侦听来自远方的TCP端口的连接请求 SYN-SENT 在发送连接请求后等待匹配的连接请求 SYN-RECEIVED 在收到和发送一个连接请求后等待对方对连接请求的确认 ESTABLISHED 一个打开的连接 FIN-WAIT-1 等待远程TCP连接中断请求，或先前的连接中断请求的确认 FIN-WAIT-2 从远程TCP等待连接中断请求 CLOSE-WAIT 等待从本地用户发来的连接中断请求 CLOSING 等待远程TCP对连接中断的确认 LAST-ACK 等待原来的发向远程TCP的连接中断请求的确认 TIME-WAIT 等待足够的时间以确保远程TCP接收到连接中断请求的确认 CLOSED 没有任何连接状态 netstat -a 1234567891011121314Active Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 *:tproxy *:* LISTEN tcp 0 0 *:us-cli *:* LISTEN tcp 0 0 *:us-srv *:* LISTEN tcp 0 0 *:intu-ec-svcdisc *:* LISTEN Active UNIX domain sockets (servers and established)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ACC ] STREAM LISTENING 12688527 /var/run/abrt/abrt.socketunix 2 [ ACC ] STREAM LISTENING 288268830 /tmp/supervisor.sock.3241unix 2 [ ACC ] STREAM LISTENING 7341 @/com/ubuntu/upstartunix 2 [ ACC ] STREAM LISTENING 17201 @/var/run/hald/dbus-vA2CSxY4CUunix 2 [ ACC ] STREAM LISTENING 17196 @/var/run/hald/dbus-5pcejJb6npunix 2 [ ] DGRAM 17227 @/org/freedesktop/hal/udev_event 显示所有有效连接的列表，包括ESTABLISHED、LISTENING的连接。 netstat -nu 12Active Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address State 显示当前UDP的连接情况 netstat -apu 1234567891011Active Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name udp 0 0 *:947 *:* 1200/rpcbind udp 0 0 *:domain *:* 9306/dnsmasq udp 0 0 *:983 *:* 1231/rpc.statd udp 0 0 *:sunrpc *:* 1200/rpcbind udp 0 0 *:ipp *:* 1141/portreserve udp 0 0 hcdn-others-worker-dev10:ntp *:* 4247/ntpd udp 0 0 localhost:ntp *:* 4247/ntpd udp 0 0 *:ntp *:* 4247/ntpd udp 0 0 *:62630 *:* 1231/rpc.statd 显示UDP端口号的使用情况 netstat -i 1234Kernel Interface tableIface MTU Met RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgeth0 1500 0 2334710561 0 0 0 2493027803 0 0 0 BMRUlo 65536 0 199796252 0 0 0 199796252 0 0 0 LRU 显示网卡列表 netstat -g 12345IPv6/IPv4 Group MembershipsInterface RefCnt Group--------------- ------ ---------------------lo 1 all-systems.mcast.neteth0 1 all-systems.mcast.net 显示组播组的关系 netstat -s 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104Ip: 2402314477 total packets received 0 forwarded 0 incoming packets discarded 2331780718 incoming packets delivered 2692824123 requests sent outIcmp: 484304 ICMP messages received 0 input ICMP message failed. ICMP input histogram: destination unreachable: 159 timeout in transit: 3 echo requests: 83130 echo replies: 400987 timestamp request: 25 505861 ICMP messages sent 0 ICMP messages failed ICMP output histogram: destination unreachable: 21437 echo request: 401269 echo replies: 83130 timestamp replies: 25IcmpMsg: InType0: 400987 InType3: 159 InType8: 83130 InType11: 3 InType13: 25 OutType0: 83130 OutType3: 21437 OutType8: 401269 OutType14: 25Tcp: 62593872 active connections openings 177834 passive connection openings 153086 failed connection attempts 999 connection resets received 89 connections established 2250692557 segments received 2608392230 segments send out 3325843 segments retransmited 132 bad segments received. 610130 resets sentUdp: 80565656 packets received 21451 packets to unknown port received. 0 packet receive errors 80600189 packets sentUdpLite:TcpExt: 1100 invalid SYN cookies received 361 resets received for embryonic SYN_RECV sockets 466 packets pruned from receive queue because of socket buffer overrun 61819153 TCP sockets finished time wait in fast timer 2987 TCP sockets finished time wait in slow timer 660268 delayed acks sent 260 delayed acks further delayed because of locked socket Quick ack mode was activated 1051984 times 25934905 packets directly queued to recvmsg prequeue. 911172308 packets directly received from backlog 3173278514 packets directly received from prequeue 1186216864 packets header predicted 3170682 packets header predicted and directly queued to user 357381509 acknowledgments not containing data received 411743100 predicted acknowledgments 1093102 times recovered from packet loss due to SACK data Detected reordering 12642 times using FACK Detected reordering 14884 times using SACK Detected reordering 11 times using time stamp 13885 congestion windows fully recovered 252 congestion windows partially recovered using Hoe heuristic TCPDSACKUndo: 193430 51009 congestion windows recovered after partial ack 168112 TCP data loss events TCPLostRetransmit: 14829 108524 timeouts after SACK recovery 4274 timeouts in loss state 2496500 fast retransmits 158451 forward retransmits 230933 retransmits in slow start 280735 other TCP timeouts 38730 sack retransmits failed 23520 packets collapsed in receive queue due to low socket buffer 1051963 DSACKs sent for old packets 20 DSACKs sent for out of order packets 365231 DSACKs received 692 DSACKs for out of order packets received 414 connections reset due to unexpected data 896 connections reset due to early user close 761 connections aborted due to timeout TCPDSACKIgnoredOld: 509 TCPDSACKIgnoredNoUndo: 425 TCPSackShiftFallback: 22772083 TCPBacklogDrop: 2192 TCPChallengeACK: 125 TCPSYNChallenge: 125 TCPFromZeroWindowAdv: 85 TCPToZeroWindowAdv: 85 TCPWantZeroWindowAdv: 163030IpExt: InBcastPkts: 1232 InOctets: 1540584137119 OutOctets: 1474239597014 InBcastOctets: 191368 显示网络统计信息 netstat -l 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 *:tproxy *:* LISTEN tcp 0 0 *:us-cli *:* LISTEN tcp 0 0 *:us-srv *:* LISTEN tcp 0 0 *:intu-ec-svcdisc *:* LISTEN tcp 0 0 *:intu-ec-client *:* LISTEN tcp 0 0 *:domain *:* LISTEN tcp 0 0 *:oa-system *:* LISTEN tcp 0 0 *:ssh *:* LISTEN tcp 0 0 *:8023 *:* LISTEN tcp 0 0 localhost:smtp *:* LISTEN tcp 0 0 *:12602 *:* LISTEN tcp 0 0 hcdn-others-worker-de:27162 *:* LISTEN tcp 0 0 *:30050 *:* LISTEN tcp 0 0 *:xmltec-xmlmail *:* LISTEN tcp 0 0 hcdn-others-worker-de:21988 *:* LISTEN tcp 0 0 *:eforward *:* LISTEN tcp 0 0 *:svn *:* LISTEN tcp 0 0 *:16010 *:* LISTEN tcp 0 0 *:6379 *:* LISTEN tcp 0 0 *:23212 *:* LISTEN tcp 0 0 hcdn-others-worker-de:28271 *:* LISTEN tcp 0 0 *:sunrpc *:* LISTEN tcp 0 0 *:http *:* LISTEN udp 0 0 *:947 *:* udp 0 0 *:domain *:* udp 0 0 *:983 *:* udp 0 0 *:sunrpc *:* udp 0 0 *:ipp *:* udp 0 0 hcdn-others-worker-dev10:ntp *:* udp 0 0 localhost:ntp *:* udp 0 0 *:ntp *:* udp 0 0 *:62630 *:* Active UNIX domain sockets (only servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ACC ] STREAM LISTENING 12688527 /var/run/abrt/abrt.socketunix 2 [ ACC ] STREAM LISTENING 288268830 /tmp/supervisor.sock.3241unix 2 [ ACC ] STREAM LISTENING 7341 @/com/ubuntu/upstartunix 2 [ ACC ] STREAM LISTENING 17201 @/var/run/hald/dbus-vA2CSxY4CUunix 2 [ ACC ] STREAM LISTENING 17196 @/var/run/hald/dbus-5pcejJb6npunix 2 [ ACC ] STREAM LISTENING 9429 /var/run/rpcbind.sockunix 2 [ ACC ] STREAM LISTENING 9644 /var/run/cgred.socketunix 2 [ ACC ] STREAM LISTENING 16960 /var/run/dbus/system_bus_socketunix 2 [ ACC ] STREAM LISTENING 17142 /var/run/acpid.socketunix 2 [ ACC ] STREAM LISTENING 17754 /var/run/mcelog-clientunix 2 [ ACC ] STREAM LISTENING 18360 public/cleanupunix 2 [ ACC ] STREAM LISTENING 18367 private/tlsmgrunix 2 [ ACC ] STREAM LISTENING 18376 private/rewriteunix 2 [ ACC ] STREAM LISTENING 18384 private/bounceunix 2 [ ACC ] STREAM LISTENING 18388 private/deferunix 2 [ ACC ] STREAM LISTENING 18392 private/traceunix 2 [ ACC ] STREAM LISTENING 18397 private/verifyunix 2 [ ACC ] STREAM LISTENING 18401 public/flushunix 2 [ ACC ] STREAM LISTENING 18405 private/proxymapunix 2 [ ACC ] STREAM LISTENING 18409 private/proxywriteunix 2 [ ACC ] STREAM LISTENING 18413 private/smtpunix 2 [ ACC ] STREAM LISTENING 18417 private/relayunix 2 [ ACC ] STREAM LISTENING 18421 public/showqunix 2 [ ACC ] STREAM LISTENING 18425 private/errorunix 2 [ ACC ] STREAM LISTENING 18429 private/retryunix 2 [ ACC ] STREAM LISTENING 18433 private/discardunix 2 [ ACC ] STREAM LISTENING 18437 private/localunix 2 [ ACC ] STREAM LISTENING 18441 private/virtualunix 2 [ ACC ] STREAM LISTENING 18445 private/lmtpunix 2 [ ACC ] STREAM LISTENING 18449 private/anvilunix 2 [ ACC ] STREAM LISTENING 18453 private/scache 显示监听的套接口 netstat -ap | grep python3 12tcp 0 0 *:us-srv *:* LISTEN 15484/python3 tcp 0 0 *:xmltec-xmlmail *:* LISTEN 14886/python3 显示程序运行的端口 然后利用ps -ef | grep &#39;15484&#39; 命令可查找该端口运行的程序 12root 6262 946 0 15:45 pts/0 00:00:00 grep 15484root 15484 1 0 2017 ? 00:00:01 python3 /home/ly/atsbtsp/app.py Read More： 每天一个linux命令（56）：netstat命令 Linux netstat命令]]></content>
      <categories>
        <category>Linux/Unix</category>
      </categories>
      <tags>
        <tag>命令</tag>
        <tag>Linux</tag>
        <tag>netstat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-ping]]></title>
    <url>%2F2018%2F01%2F02%2Flinuxping%2F</url>
    <content type="text"><![CDATA[命令格式1ping [参数] [主机名称或IP地址] 命令功能ping命令用来测试主机之间网络的连通性。执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。 ping 命令每秒发送一个数据报并且为每个接收到的响应打印一行输出。ping 命令计算信号往返时间和（信息）包丢失情况的统计信息，并且在完成之后显示一个简要总结。 命令参数 参数 说明 -d 使用Socket的SO_DEBUG功能 -c &lt;完成次数&gt; 设置完成要求回应的次数 -f 极限检测 -i &lt;间隔秒数&gt; 指定收发信息的间隔时间 -l &lt;网络界面&gt; 使用指定的网络界面送出数据包 -l &lt;前置载入&gt; 设置在送出要求信息之前，先行发出的数据包 -n 只输出数值 -p 设置填满数据包的范本样式 -q 不显示指令执行过程，开头和结尾的相关信息除外 -r 忽略普通的Routing Table，直接将数据包送到远端主机上 -R 记录路由过程 -s &lt;数据包大小&gt; 设置数据包的大小 -t &lt;存活时间&gt; 设置存活数值TTL的大小 -v 详细显示指令的执行过程 使用示例 ping 10.3.14.6 123456789101112PING 10.3.14.62 (10.3.14.62) 56(84) bytes of data.64 bytes from 10.3.14.62: icmp_seq=1 ttl=119 time=1.00 ms64 bytes from 10.3.14.62: icmp_seq=2 ttl=119 time=0.921 ms64 bytes from 10.3.14.62: icmp_seq=3 ttl=119 time=0.848 ms64 bytes from 10.3.14.62: icmp_seq=4 ttl=119 time=0.862 ms64 bytes from 10.3.14.62: icmp_seq=5 ttl=119 time=0.912 ms^C--- 10.3.14.62 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4990msrtt min/avg/max/mdev = 0.848/0.909/1.004/0.061 ms需要手动Ctrl+c终止 ping通主机 ping 10.3.14.6 1234567891011PING 10.3.14.6 (10.3.14.6) 56(84) bytes of data.From 10.13.41.63 icmp_seq=1 Destination Host UnreachableFrom 10.13.41.634 icmp_seq=2 Destination Host UnreachableFrom 10.13.41.63 icmp_seq=3 Destination Host UnreachableFrom 10.13.41.63 icmp_seq=4 Destination Host UnreachableFrom 10.13.41.63 icmp_seq=5 Destination Host UnreachableFrom 10.13.41.63 icmp_seq=6 Destination Host Unreachable--- 10.3.14.6 ping statistics ---8 packets transmitted, 0 received, +6 errors, 100% packet loss, time 7005ms, pipe 4 ping不通主机 ping -c 10 10.3.14.62 123456789101112131415PING 10.3.14.62 (10.3.14.62) 56(84) bytes of data.64 bytes from 10.3.14.62: icmp_seq=1 ttl=119 time=0.914 ms64 bytes from 10.3.14.62: icmp_seq=2 ttl=119 time=0.525 ms64 bytes from 10.3.14.62: icmp_seq=3 ttl=119 time=1.09 ms64 bytes from 10.3.14.62: icmp_seq=4 ttl=119 time=1.01 ms64 bytes from 10.3.14.62: icmp_seq=5 ttl=119 time=0.956 ms64 bytes from 10.3.14.62: icmp_seq=6 ttl=119 time=1.11 ms64 bytes from 10.3.14.62: icmp_seq=7 ttl=119 time=0.870 ms64 bytes from 10.3.14.62: icmp_seq=8 ttl=119 time=0.891 ms64 bytes from 10.3.14.62: icmp_seq=9 ttl=119 time=0.922 ms64 bytes from 10.3.14.62: icmp_seq=10 ttl=119 time=1.02 ms--- 10.3.14.62 ping statistics ---10 packets transmitted, 10 received, 0% packet loss, time 9005msrtt min/avg/max/mdev = 0.525/0.932/1.115/0.158 ms ping指定次数 ping -c 10 -i 0.5 10.3.14.62 123456789101112131415PING 10.3.14.62 (10.3.14.62) 56(84) bytes of data.64 bytes from 10.3.14.62: icmp_seq=1 ttl=119 time=1.00 ms64 bytes from 10.3.14.62: icmp_seq=2 ttl=119 time=0.889 ms64 bytes from 10.3.14.62: icmp_seq=3 ttl=119 time=0.893 ms64 bytes from 10.3.14.62: icmp_seq=4 ttl=119 time=0.865 ms64 bytes from 10.3.14.62: icmp_seq=5 ttl=119 time=0.893 ms64 bytes from 10.3.14.62: icmp_seq=6 ttl=119 time=0.868 ms64 bytes from 10.3.14.62: icmp_seq=7 ttl=119 time=0.881 ms64 bytes from 10.3.14.62: icmp_seq=8 ttl=119 time=0.885 ms64 bytes from 10.3.14.62: icmp_seq=9 ttl=119 time=0.605 ms64 bytes from 10.3.14.62: icmp_seq=10 ttl=119 time=0.569 ms--- 10.3.14.62 ping statistics ---10 packets transmitted, 10 received, 0% packet loss, time 4506msrtt min/avg/max/mdev = 0.569/0.835/1.003/0.130 ms ping指定次数和时间间隔 ping -c 5 www.zhihu.com 12345678910PING 6ej19t5k0le6q937.alicloudlayer.com (47.95.51.100) 56(84) bytes of data.64 bytes from 47.95.51.100: icmp_seq=1 ttl=45 time=3.08 ms64 bytes from 47.95.51.100: icmp_seq=2 ttl=45 time=3.02 ms64 bytes from 47.95.51.100: icmp_seq=3 ttl=45 time=3.01 ms64 bytes from 47.95.51.100: icmp_seq=4 ttl=45 time=3.05 ms64 bytes from 47.95.51.100: icmp_seq=5 ttl=45 time=3.03 ms--- 6ej19t5k0le6q937.alicloudlayer.com ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4006msrtt min/avg/max/mdev = 3.012/3.042/3.088/0.074 ms 通过域名ping公网地址 ping -i 3 -s 1024 -t 255 10.3.14.62 12345678910111213141516171819202122PING 10.3.14.62 (10.3.14.62) 1024(1052) bytes of data.1032 bytes from 10.3.14.62: icmp_seq=1 ttl=119 time=1.18 ms1032 bytes from 10.3.14.62: icmp_seq=2 ttl=119 time=1.21 ms1032 bytes from 10.3.14.62: icmp_seq=3 ttl=119 time=1.22 ms1032 bytes from 10.3.14.62: icmp_seq=4 ttl=119 time=1.19 ms1032 bytes from 10.3.14.62: icmp_seq=5 ttl=119 time=1.20 ms1032 bytes from 10.3.14.62: icmp_seq=6 ttl=119 time=1.23 ms1032 bytes from 10.3.14.62: icmp_seq=7 ttl=119 time=1.22 ms1032 bytes from 10.3.14.62: icmp_seq=8 ttl=119 time=1.22 ms1032 bytes from 10.3.14.62: icmp_seq=9 ttl=119 time=1.21 ms1032 bytes from 10.3.14.62: icmp_seq=10 ttl=119 time=1.21 ms1032 bytes from 10.3.14.62: icmp_seq=11 ttl=119 time=1.19 ms1032 bytes from 10.3.14.62: icmp_seq=12 ttl=119 time=1.28 ms1032 bytes from 10.3.14.62: icmp_seq=13 ttl=119 time=1.20 ms1032 bytes from 10.3.14.62: icmp_seq=14 ttl=119 time=1.23 ms1032 bytes from 10.3.14.62: icmp_seq=15 ttl=119 time=1.21 ms1032 bytes from 10.3.14.62: icmp_seq=16 ttl=119 time=1.21 ms1032 bytes from 10.3.14.62: icmp_seq=17 ttl=119 time=1.18 ms^C--- 10.3.14.62 ping statistics ---17 packets transmitted, 17 received, 0% packet loss, time 48798msrtt min/avg/max/mdev = 1.187/1.215/1.285/0.038 ms ping多参数使用 Read More： 每天一个linux命令（54）：ping命令 Linux ping命令]]></content>
      <categories>
        <category>Linux/Unix</category>
      </categories>
      <tags>
        <tag>命令</tag>
        <tag>Linux</tag>
        <tag>ping</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-ps]]></title>
    <url>%2F2018%2F01%2F02%2Flinuxps%2F</url>
    <content type="text"><![CDATA[Linux中的ps命令是Process Status的缩写。用来列出系统中当前运行的那些进程。ps命令列出的是当前那些进程的快照，就是执行ps命令的那个时刻的那些进程，如果想要动态的显示进程信息，就可以使用top命令。 要对进程进行监测和控制，首先必须要了解当前进程的情况，也就是需要查看当前进程，ps 命令是最基本同时也是非常强大的进程查看命令。使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等。总之大部分信息都是可以通过执行该命令得到的。 Linux上的进程状态 运行（正在运行或在运行队列中等待） 中断（休眠中，受阻，在等待某个条件的形成或接受到信号） 不可中断（收到信号不唤醒和不可运行，进程必须等待直到有中断发生） 僵死（进程已终止，但进程描述符存在，直到父进程调用wait4()系统调用后释放） 停止（进程收到SIGSTOP、SIGSTP、SIGTIN、SIGTOU信号后停止运行运行） ps命令标识进程状态码 R: 运行 S: 中断 D: 不可中断 Z: 僵死 T: 停止 命令格式1ps [参数] 命令功能ps命令用来显示当前进程的状态。 命令参数 参数 说明 a 显示所有进程 -a 显示同一终端下的所有程序 -A 显示所有进程 c 显示进程的真实名称 -N 反向选择 -e 等于”-A” e 显示环境变量 f 显示程序间的关系 -H 显示树状结构 r 显示当前终端的进程 T 显示当前终端的所有程序 u 指定用户的所有进程 -au 显示较详细的资讯 -aux 显示所有包含其他使用者的行程 -C &lt;命令&gt;d 列出指定命令的状况 –lines &lt;行数&gt; 每页显示的行数 –width &lt;字符数&gt; 每页显示的字符数 –help 显示帮助信息 –version 显示版本显示 使用示例 ps -A 1234567891011121314PID TTY TIME CMD 1 ? 00:00:16 init 2 ? 00:00:00 kthreadd 3 ? 00:00:21 migration/0 4 ? 00:00:36 ksoftirqd/0 5 ? 00:00:00 stopper/0 6 ? 00:00:08 watchdog/0 7 ? 00:00:21 migration/1 8 ? 00:00:00 stopper/1 9 ? 00:00:35 ksoftirqd/1 10 ? 00:00:07 watchdog/1 11 ? 00:00:21 migration/2 12 ? 00:00:00 stopper/2 13 ? 00:00:34 ksoftirqd/2 显示所有进程 ps -u root 1234567891011PID TTY TIME CMD 1 ? 00:00:16 init 2 ? 00:00:00 kthreadd 3 ? 00:00:21 migration/0 4 ? 00:00:36 ksoftirqd/0 5 ? 00:00:00 stopper/0 6 ? 00:00:08 watchdog/0 7 ? 00:00:21 migration/1 8 ? 00:00:00 stopper/1 9 ? 00:00:35 ksoftirqd/1 10 ? 00:00:07 watchdog/1 显示特定用户的进程信息 ps -ef 123456789UID PID PPID C STIME TTY TIME CMDroot 12426 10831 0 16:47 pts/0 00:00:00 ps -efroot 14886 1 0 2017 ? 00:00:01 python3 /home/ly/prepullvidcs/app.pyroot 15484 1 0 2017 ? 00:00:01 python3 /home/ly/atsbtsp/app.pyroot 17080 1 0 2017 ? 00:00:00 bash /usr/local/hbase-1.2.6/bin/hbase-daemon.sh --config /usr/local/hbase-1.2.6/bin/../conf foreground_start masterroot 17094 17080 0 2017 ? 04:35:00 /usr/lib/java-1.8/jdk1.8.0_144/bin/java -Dproc_master -XX:OnOutOfMemoryError=kill -9 %p -XX:+UseConcMarkSweepGC -XX:PermSize=128m -XX:MaxPermSize=128m -Dhbase.log.dir=/usr/local/hbase-1.2.6/root 17890 1 0 2017 ? 00:05:22 /usr/local/zabbix/share/zabbix/externalscripts/zabbix-agentd-updaterroot 17973 18967 0 2017 ? 00:00:00 /sbin/udevd -droot 18634 1 0 2017 ? 00:18:54 python27 /home/ly/datawritehbase/tasks/live_stream_ugcppcpgc/stream_list_old.py 显示所有进程信息（包括命令行） ps -ef | grep python 12345678910111213141516root 3244 1 0 2017 ? 00:02:29 /usr/bin/python27 /usr/local/python2.7/bin/supervisord -c /etc/supervisor/supervisord.confroot 3245 3244 0 2017 ? 00:00:01 python27 /home/ly/analysisweb/app.py --port=8021root 3246 3244 0 2017 ? 00:00:01 python27 /home/ly/analysisweb/app.py --port=8020root 3247 3244 0 2017 ? 00:00:00 python27 /home/ly/analysisweb/app.py --port=8023root 3248 3244 0 2017 ? 00:00:00 python27 /home/ly/analysisweb/app.py --port=8022root 8472 1 0 2017 ? 00:01:20 python3 monitor.py tasks/live_stream_length_task.py850 12515 9226 1 16:48 ? 00:00:00 /opt/cloud-agent/agent/plugin/env/bin/python plugin/python/sys/basic/60_ifstat.pyroot 12567 10831 0 16:48 pts/0 00:00:00 grep pythonroot 14886 1 0 2017 ? 00:00:01 python3 /home/ly/prepullvidcs/app.pyroot 15484 1 0 2017 ? 00:00:01 python3 /home/ly/atsbtsp/app.pyroot 18634 1 0 2017 ? 00:18:54 python27 /home/ly/datawritehbase/tasks/live_stream_ugcppcpgc/stream_list_old.pyroot 18635 1 99 2017 ? 28-05:56:25 python27 /home/ly/datawritehbase/tasks/multicdn_kafka_hbase/mcdn_main.pyroot 18636 1 99 2017 ? 28-20:46:31 python27 /home/ly/datawritehbase/tasks/nginx_kafka_hbase/nginx_main.pyroot 18638 1 3 2017 ? 20:43:58 python27 /home/ly/datawritehbase/tasks/stream_hbase/stream_main_old.pyroot 20253 1 0 2017 ? 00:01:00 python27 /home/ly/hbasequeryweb/app.pyroot 30087 1 1 2017 ? 11:27:44 python27 /home/ly/datawritehbase/tasks/rtmp_kafka_hbase/rtmp_main.py 查找特定进程 ps aux 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 19356 1544 ? Ss 2017 0:16 /sbin/initroot 2 0.0 0.0 0 0 ? S 2017 0:00 [kthreadd]root 3 0.0 0.0 0 0 ? S 2017 0:21 [migration/0]root 4 0.0 0.0 0 0 ? S 2017 0:36 [ksoftirqd/0]root 5 0.0 0.0 0 0 ? S 2017 0:00 [stopper/0]root 6 0.0 0.0 0 0 ? S 2017 0:08 [watchdog/0]root 7 0.0 0.0 0 0 ? S 2017 0:21 [migration/1]root 8 0.0 0.0 0 0 ? S 2017 0:00 [stopper/1]root 9 0.0 0.0 0 0 ? S 2017 0:35 [ksoftirqd/1]root 10 0.0 0.0 0 0 ? S 2017 0:07 [watchdog/1]root 11 0.0 0.0 0 0 ? S 2017 0:21 [migration/2]......root 1392 0.0 0.0 0 0 ? S 2017 1:29 [flush-252:0]root 3244 0.0 0.0 209168 14424 ? Ss 2017 2:29 /usr/bin/python27 /usr/local/python2.7/bin/supervisord -c /etc/supervisor/supervisord.confroot 3245 0.0 0.2 248368 37396 ? S 2017 0:01 python27 /home/ly/analysisweb/app.py --port=8021root 3246 0.0 0.2 253408 42800 ? S 2017 0:01 python27 /home/ly/analysisweb/app.py --port=8020root 3247 0.0 0.2 243556 32756 ? S 2017 0:00 python27 /home/ly/analysisweb/app.py --port=8023root 3248 0.0 0.2 245124 34528 ? S 2017 0:00 python27 /home/ly/analysisweb/app.py --port=8022dbus 3857 0.0 0.0 21540 1364 ? Ss 2017 0:00 dbus-daemon --systemroot 3886 0.0 0.0 4076 648 ? Ss 2017 0:00 /usr/sbin/acpid68 3895 0.0 0.0 25068 3848 ? Ss 2017 0:37 haldroot 3896 0.0 0.0 18104 1132 ? S 2017 0:00 hald-runnerroot 3924 0.0 0.0 20220 1080 ? S 2017 0:00 hald-addon-input: Listening on /dev/input/event2 /dev/input/event068 3934 0.0 0.0 17804 1036 ? S 2017 0:00 hald-addon-acpi: listening on acpid socket /var/run/acpid.socketroot 3981 0.0 0.0 6244 292 ? Ss 2017 0:00 /usr/sbin/mcelog --daemonroot 4093 0.0 0.0 78728 3312 ? Ss 2017 0:30 /usr/libexec/postfix/masterpostfix 4108 0.0 0.0 78980 3444 ? S 2017 0:04 qmgr -l -t fifo -uroot 4159 0.0 0.0 21452 476 ? Ss 2017 0:00 /usr/sbin/atd......root 10826 0.0 0.0 101072 4024 ? Ss 16:30 0:00 sshd: root@pts/0 root 10831 0.0 0.0 109096 2556 pts/0 Ss 16:30 0:00 -bashroot 12865 0.0 0.0 110240 1136 pts/0 R+ 16:51 0:00 ps auxroot 14886 0.0 0.2 533372 40212 ? Sl 2017 0:01 python3 /home/ly/prepullvidcs/app.pyroot 15484 0.0 0.1 233608 30092 ? S 2017 0:01 python3 /home/ly/atsbtsp/app.pyroot 17080 0.0 0.0 106100 1380 ? S 2017 0:00 bash /usr/local/hbase-1.2.6/bin/hbase-daemon.sh --config /usr/local/hbase-1.2.6/bin/../conf foreground_start masterroot 17094 0.1 9.1 6203172 1486980 ? Sl 2017 275:00 /usr/lib/java-1.8/jdk1.8.0_144/bin/java -Dproc_master -XX:OnOutOfMemoryError=kill -9 %p -XX:+UseConcMarkSweepGC -XX:PermSize=128m -XX:MaxPermSize=128m -Dhbase.log.dir=/usr/lroot 17890 0.0 0.0 587052 10368 ? Sl 2017 5:22 /usr/local/zabbix/share/zabbix/externalscripts/zabbix-agentd-updaterroot 17973 0.0 0.0 11540 676 ? S&lt; 2017 0:00 /sbin/udevd -d......root 25290 0.0 0.0 175824 1008 ? Ss 2017 0:00 svnserve -dr /var/svndata/root 25793 0.0 0.0 76792 1384 ? S 03:28 0:00 /usr/local/zabbix/sbin/zabbix_agentd_ops root 25794 0.0 0.0 76792 1472 ? S 03:28 0:07 /usr/local/zabbix/sbin/zabbix_agentd_ops: collector [idle 1 sec]root 25795 0.0 0.0 76792 1040 ? S 03:28 0:00 /usr/local/zabbix/sbin/zabbix_agentd_ops: listener #1 [waiting for connection]root 25796 0.0 0.0 78984 2008 ? S 03:28 0:06 /usr/local/zabbix/sbin/zabbix_agentd_ops: active checks #1 [idle 1 sec]root 25797 0.0 0.0 78984 1996 ? S 03:28 0:06 /usr/local/zabbix/sbin/zabbix_agentd_ops: active checks #2 [idle 1 sec]root 26648 0.0 0.0 125472 7688 ? Ssl 2017 5:33 ./redis-server *:6379 root 30087 1.7 0.6 7316724 106248 ? Sl 2017 687:47 python27 /home/ly/datawritehbase/tasks/rtmp_kafka_hbase/rtmp_main.py 列出目前所有正在内存中的进程 参数 说明 USER 进程所属账号 PID 进程id号 %CPU 进程所占CPU资源百分比 %MEM 进程所占物理内存百分比 VSZ 进程所占虚拟内存大小 RSS 进程所占固定内存大小 TTY 进程在哪个终端机上运行。若与终端机无关，则显示 ？；tty1-tty6 是本机上面的登入者程序；若为 pts/0 等等的，则表示为由网络连接进主机的程序 STAT 进程当前状态 START 进程被触发启动的时间 TIME 进程实际使用CPU的时间 COMMAND 进程的启动指令 ps aux | egrep ‘(cron|syslog)’ 123root 9278 0.0 0.0 117256 1276 ? Ss 11:10 0:00 crondroot 13816 0.0 0.0 101016 844 pts/0 S+ 17:00 0:00 egrep (cron|syslog)root 22016 0.0 0.0 159736 5856 ? Sl 2017 2:06 /sbin/rsyslogd -i /var/run/syslogd.pid -c 5 显示与 cron、syslog服务有关进程的PID号 ps -aux | more : 分页查看 ps -aux &gt; ps001.txt : 输出到文件中 ps -o pid,ppid,pgrp,session,tpgid,comm : 输出指定字段 Read More： 每天一个linux命令（41）：ps命令 Linux ps命令]]></content>
      <categories>
        <category>Linux/Unix</category>
      </categories>
      <tags>
        <tag>命令</tag>
        <tag>Linux</tag>
        <tag>ps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text 3 添加右键菜单]]></title>
    <url>%2F2017%2F10%2F11%2Fsublimetext3rightclick%2F</url>
    <content type="text"><![CDATA[每次安装 Sublime Text 3 后邮件菜单总是没有，但是 notepad++ 安装后会自动加到邮件菜单，对于 Sublime Text 3 的忠实粉丝来说简直丧心病狂，所以搜罗了一些方法来完成这件事儿。 方式1123456789101112[Version]Signature=&quot;$Windows NT$&quot;[DefaultInstall]AddReg=SublimeText3[SublimeText3]hkcr,&quot;*\\shell\\SublimeText3&quot;,,,&quot;Edit with Sublime Text 3&quot;hkcr,&quot;*\\shell\\SublimeText3\\command&quot;,,,&quot;&quot;&quot;%1%\sublime_text.exe&quot;&quot; &quot;&quot;%%1&quot;&quot; %%*&quot;hkcr,&quot;Directory\shell\SublimeText3&quot;,,,&quot;Edit with Sublime Text 3&quot;hkcr,&quot;*\\shell\\SublimeText3&quot;,&quot;Icon&quot;,0x20000,&quot;%1%\sublime_text.exe, 0&quot;hkcr,&quot;Directory\shell\SublimeText3\command&quot;,,,&quot;&quot;&quot;%1%\sublime_text.exe&quot;&quot; &quot;&quot;%%1&quot;&quot;&quot; 把以上代码，复制到 Sublime Text 3 的安装目录，然后重命名为：sublime_addright.inf，注意文件编码方式为ANSI，右击安装就可以了。 重命名文件之前，需要先在 工具-&gt;文件夹选项-&gt;查看，把隐藏已知文件类型的扩展名前边的复选框不勾选。 方式2123456789101112131415Windows Registry Editor Version 5.00[HKEY_CLASSES_ROOT\*\shell\SublimeText3]@=&quot;Edit with Sublime Text 3&quot;&quot;Icon&quot;=&quot;D:\\Sublime Text 3\\sublime_text.exe,0&quot;[HKEY_CLASSES_ROOT\*\shell\SublimeText3\command]@=&quot;D:\\Sublime Text 3\\sublime_text.exe %1&quot;[HKEY_CLASSES_ROOT\Directory\shell\SublimeText3]@=&quot;Edit with Sublime Text 3&quot;&quot;Icon&quot;=&quot;D:\\Sublime Text 3\\sublime_text.exe,0&quot;[HKEY_CLASSES_ROOT\Directory\shell\SublimeText3\command]@=&quot;D:\\Sublime Text 3\\sublime_text.exe %1&quot; 把以上代码，复制到 Sublime Text 3 的安装目录，然后重命名为：sublime_addright.reg，注意文件编码方式为ANSI，然后双击就可以了。 需要把里边的 Sublime 的安装目录，替换成实际的 Sublime 安装目录。 删除右键菜单脚本： 123Windows Registry Editor Version 5.00[-HKEY_CLASSES_ROOT\*\shell\SublimeText3][-HKEY_CLASSES_ROOT\Directory\shell\SublimeText3] 把以上代码，复制到 Sublime Text 3 的安装目录，然后重命名为：sublime_delright.reg，文件编码方式依然为ANSI，然后双击就可以了。 Read More： 将Sublime Text3添加到右键菜单中]]></content>
      <categories>
        <category>辅助研发杂记</category>
      </categories>
      <tags>
        <tag>Sublime Text 3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git常用命令的简单使用]]></title>
    <url>%2F2017%2F10%2F09%2Fgitcommand%2F</url>
    <content type="text"><![CDATA[git addgit add 命令将文件内容添加到索引（将修改添加到暂存区）。也就是将要提交的文件的信息添加到索引库中。 描述将要提交的文件的信息添加到索引库中（将修改添加到暂存区），以准备为下一次提交分段的内容。 它通常将现有路径的当前内容作为一个整体添加，但是通过一些选项，它也可以用于添加内容，只对所应用的工作树文件进行一些更改，或删除工作树中不存在的路径了。 该命令可以在提交之前多次执行。它只在运行 git add 命令时添加指定文件的内容; 如果希望随后的更改包含在下一个提交中，那么必须再次运行 git add 将新的内容添加到索引。 示例12345git add . # 将所有修改添加到暂存区git add * # Ant风格添加修改git add *Controller # 将以Controller结尾的文件的所有修改添加到暂存区git add Hello* # 将所有以Hello开头的文件的修改添加到暂存区git add Hello? # 将以Hello开头后面只有一位的文件的修改提交到暂存区 例如:Hello1.txt,HelloA.java git commitgit commit 命令用于将更改记录提交到存储库。将索引的当前内容与描述更改的用户和日志消息一起存储在新的提交中。 描述git commit 命令将索引的当前内容与描述更改的用户和日志消息一起存储在新的提交中。 要添加的内容可以通过以下几种方式指定： 在使用 git commit 命令之前，通过使用 git add 对索引进行递增的“添加”更改（注意：修改后的文件的状态必须为added）； 通过使用 git rm 从工作树和索引中删除文件，再次使用 git commit 命令; 通过将文件作为参数列出到 git commit 命令（不使用 --interactive 或 --patch 选项），在这种情况下，提交将忽略索引中分段的更改，而是记录列出的文件的当前内容（必须已知到Git的内容）； 通过使用带有 -a 选项的 git commit 命令来自动从所有已知文件（即所有已经在索引中列出的文件）中添加“更改”，并自动从已从工作树中删除索引中的 rm 文件 ，然后执行实际提交； 通过使用 --interactive 或 --patch 选项与 git commit 命令一起确定除了索引中的内容之外哪些文件应该是提交的一部分，然后才能完成操作。 如果提交后立即发现错误，可以使用 git reset 命令恢复。 示例1git commit -m &quot;the commit message&quot; # git add 存储到暂存区之后将内容提交 git pushgit push 命令用于将本地分支的更新，推送到远程主机。 描述使用本地引用更新远程引用，同时发送完成给定引用所需的对象。可以在每次推入存储库时，通过在那里设置挂钩触发一些事件。当命令行不指定使用 &lt;repository&gt; 参数推送的位置时，将查询当前分支的 branch.*.remote 配置以确定要在哪里推送。 如果配置丢失，则默认为origin。 示例1234git push origin master # 将本地的master分支推送到origin主机的master分支。如果master不存在，则会被新建。git push origin # 将当前分支推送到origin主机的对应分支。如果当前分支只有一个追踪分支，那么主机名都可以省略。git push -u origin master # 将本地的master分支推送到origin主机，同时指定origin为默认主机。git push --all origin # 将所有本地分支都推送到origin主机。 git clonegit clone 命令将存储库克隆到新目录中。 描述将存储库克隆到新创建的目录中，为克隆的存储库中的每个分支创建远程跟踪分支，并从克隆检出的存储库作为当前活动分支的初始分支。 示例12git clone &lt;版本库的网址&gt;如：git clone http://github.com/jquery/jquery.git 该命令会在本地主机生成一个目录，与远程主机的版本库同名。 1git clone &lt;版本库的网址&gt; &lt;本地目录名&gt; 该命令可指定不同的目录名。 git fetchgit fetch 命令用于从另一个存储库下载对象和引用。 描述从一个或多个其他存储库中获取分支或标签以及完成其历史所必需的对象。远程跟踪分支已更新，需要将这些更新取回本地，这时就要用到git fetch 命令。 示例更新远程代码到本地仓库： 12345678910111213方式一：1. 查看远程仓库：git remote -v2. 从远程origin仓库的master分支获取最新版本到本地：git fetch origin master3. 比较本地仓库和远程仓库的区别：git log -p master.. origin/master4. 远程仓库和本地仓库的合并：git merge origin/master方式二：1. 查看远程仓库：git remote -v2. 从远程的origin仓库的master分支下载到本地并新建一个分支temp：git fetch origin master:temp3. 比较本地仓库和远程仓库的区别：git diff temp4. 合并temp分支到master分支：git merge temp5. 删除temp分支：git branch -d temp注意：如果该分支没有合并到主分支会报错，可以用以下命令强制删除 git branch -D &lt;分支名&gt;]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目部署Nginx+Tornado+Supervisor]]></title>
    <url>%2F2017%2F09%2F03%2Fnginxtornadosupervisor%2F</url>
    <content type="text"><![CDATA[在项目中应用到了 Tornado 框架，开始时只是单进程裸跑 Tornado 实例，但是在高并发的情况下，单进程无法满足上千的连接请求。Nginx 是我们熟知的反向代理服务器，它可以支持上万的平行连接，在与 PHP、Python 集成后可以应用在大规模的集群上，Nginx 同时也可以用作负载平衡器，以减小单点服务器的压力。 Supervisor 是一个进程管理工具，我们可以使用它管理多个 Tornado 实例，非常方便。 总的思路是：Nginx 通过唯一监听端口接收大量请求，然后将请求分发到不同的 Tornado 实例上，Supervisor 将多个 Tornado 实例进程统一管理。 TornadoTornado 和现在的主流 Web 服务器框架（包括大多数 Python 的框架）有着明显的区别：它是非阻塞式服务器，而且速度相当快。得利于其非阻塞的方式和对 epoll 的运用，Tornado 每秒可以处理数以千计的连接，这意味着对于实时 Web 服务来说，Tornado 是一个理想的 Web 框架。Tornado 由于内置了支持 epoll/kqueue 等高效网络库，而具备了处理高并发的能力。 程序入口： 12345678910111213141516171819202122232425262728293031323334353637383940# -*- coding: utf-8 -*-import os.pathimport tornado.escapeimport tornado.ioloopimport tornado.webfrom tornado.httpserver import HTTPServerfrom tornado.options import define, optionsfrom handlers import stream_handlerdefine(&quot;port&quot;, default=8888, help=&quot;run on the given port&quot;, type=int)define(&quot;debug&quot;, default=False, help=&quot;run in debug mode&quot;)class Application(tornado.web.Application): def __init__(self): handlers = [ # (r&quot;/&quot;, main_handler.MainHandler), (r&quot;/stream.*?&quot;, stream_handler.StreamHandler), ] settings = dict( template_path=os.path.join(os.path.dirname(__file__), &quot;templates&quot;), static_path=os.path.join(os.path.dirname(__file__), &quot;static&quot;), debug=options.debug, ) super(Application, self).__init__(handlers, **settings) passdef main(): tornado.options.parse_command_line() http_server = HTTPServer(Application()) http_server.listen(options.port) tornado.ioloop.IOLoop.instance().start()if __name__ == &quot;__main__&quot;: main() 执行以下命令运行： 1python27 app.py --port=8081 注意： 因为我的机器上安装了不同版本的 Python 解释器，python27 是我为 Python 2.7.13 创建的软连接。 以下一行代码必须有： 1tornado.options.parse_command_line() 因为我们运行程序脚本时，需要指定不同的端口，这是在执行 Tornado 的解析命令行。 Supervisor 管理 Tornado 进程Supervisor 的安装可以在官方文章中找到，可使用 yum pip easy_insatll 等方法。最简单的就是： 1pip27 install supervisor 注意： 因为我的机器上安装了不同版本的 pip 包，pip27 是我为 Python 2.7.13 使用的。 创建文件夹： 1mkdir /etc/supervisor 创建配置文件： 1echo_supervisord_conf &gt; /etc/supervisor/supervisord.conf 修改 /etc/supervisor/supervisord.conf 文件内容，在文件结尾 [include] 节点处 1234567; [include]; files = relative/directory/*.ini=&gt;[include]files = conf.d/*.conf 保存并退出 在 /etc/supervisor/ 下创建 conf.d 文件夹，及 ProjectName.conf (以项目名称命名的，例如：tornados.conf) 执行： 1vi tornados.conf 添加以下内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980[group:tornados]programs=tornado-0,tornado-1,tornado-2,tornado-3,tornado-4,tornado-5,tornado-6,tornado-7[program:tornado-0]# 执行的命令command=python27 /home/ly/analysisweb/app.py --port=8020# 项目目录directory=/home/ly/analysisweb/# 用户user=root# 自动重启autorestart=trueredirect_stderr=true# log文件路径stdout_logfile=/home/ly/analysisweb/log/supervisor/tornado0.log# log级别loglevel=info[program:tornado-1]command=python27 /home/ly/analysisweb/app.py --port=8021directory=/home/ly/analysisweb/user=rootautorestart=trueredirect_stderr=truestdout_logfile=/home/ly/analysisweb/log/supervisor/tornado1.logloglevel=info[program:tornado-2]command=python27 /home/ly/analysisweb/app.py --port=8022directory=/home/ly/analysisweb/user=rootautorestart=trueredirect_stderr=truestdout_logfile=/home/ly/analysisweb/log/supervisor/tornado2.logloglevel=info[program:tornado-3]command=python27 /home/ly/analysisweb/app.py --port=8023directory=/home/ly/analysisweb/user=rootautorestart=trueredirect_stderr=truestdout_logfile=/home/ly/analysisweb/log/supervisor/tornado3.logloglevel=info[program:tornado-4]command=python27 /home/ly/analysisweb/app.py --port=8024directory=/home/ly/analysisweb/user=rootautorestart=trueredirect_stderr=truestdout_logfile=/home/ly/analysisweb/log/supervisor/tornado4.logloglevel=info[program:tornado-5]command=python27 /home/ly/analysisweb/app.py --port=8025directory=/home/ly/analysisweb/user=rootautorestart=trueredirect_stderr=truestdout_logfile=/home/ly/analysisweb/log/supervisor/tornado5.logloglevel=info[program:tornado-6]command=python27 /home/ly/analysisweb/app.py --port=8026directory=/home/ly/analysisweb/user=rootautorestart=trueredirect_stderr=truestdout_logfile=/home/ly/analysisweb/log/supervisor/tornado6.logloglevel=info[program:tornado-7]command=python27 /home/ly/analysisweb/app.py --port=8027directory=/home/ly/analysisweb/user=rootautorestart=trueredirect_stderr=truestdout_logfile=/home/ly/analysisweb/log/supervisor/tornado7.logloglevel=info 保存并退出 执行以下命令启动 Supervisor ： 1supervisord -c /etc/supervisor/supervisord.conf 坑1：出现错误 Error: Another program is already listening on a port that one of our HTTP servers is configured to use. Shut this program down first before starting supervisord.For help, use /usr/bin/supervisord –h 解决办法：是因为有一个使用 supervisor 配置的应用程序正在运行。执行 ps -ef | grep supervisor 查看进程，杀掉 supervisor 的所有进程，重新运行 参考这里 坑2：出现错误 Unlinking stale socket /tmp/supervisor.sock 解决办法：执行 unlink /tmp/supervisor.sock 参考这里 如果执行成功，ps -ef | grep python 结果如下： 123456789root 21314 1 0 15:22 ? 00:00:00 /usr/bin/python27 /usr/local/python2.7/bin/supervisord -c /etc/supervisor/supervisord.confroot 21315 21314 0 15:22 ? 00:00:00 python27 /home/ly/analysisweb/app.py --port=8021root 21316 21314 0 15:22 ? 00:00:00 python27 /home/ly/analysisweb/app.py --port=8020root 21317 21314 0 15:22 ? 00:00:00 python27 /home/ly/analysisweb/app.py --port=8023root 21318 21314 0 15:22 ? 00:00:00 python27 /home/ly/analysisweb/app.py --port=8022root 21319 21314 0 15:22 ? 00:00:00 python27 /home/ly/analysisweb/app.py --port=8025root 21320 21314 0 15:22 ? 00:00:00 python27 /home/ly/analysisweb/app.py --port=8024root 21321 21314 0 15:22 ? 00:00:00 python27 /home/ly/analysisweb/app.py --port=8027root 21322 21314 0 15:22 ? 00:00:00 python27 /home/ly/analysisweb/app.py --port=8026 Nginx 作反向代理 安装好 Nginx 后，编辑 nginx.conf 文件： 123456789101112131415161718192021222324252627282930313233343536373839http &#123; # 我启动了8个Tornado实例，配置不同端口 upstream tornados &#123; server 127.0.0.1:8020; server 127.0.0.1:8021; server 127.0.0.1:8022; server 127.0.0.1:8023; server 127.0.0.1:8024; server 127.0.0.1:8025; server 127.0.0.1:8026; server 127.0.0.1:8027; &#125; proxy_next_upstream error; server &#123; # nginx 监听8081端口 listen 8081; # 静态文件的路径 location /static/&#123; alias /home/ly/analysisweb/static/; expires 24h; &#125; location / &#123; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_pass http://tornados; &#125; &#125; server &#123; ...... &#125;&#125; 可参考 运行和部署 以上，就完成了Nginx+Tornado+Supervisor 的项目部署，分别启动 nginx、supervisord ，就可以达到不错的负载均衡效果（我的项目中没有访问静态资源的情景，所以主要还是作负载均衡用）]]></content>
      <categories>
        <category>工程</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>Tornado</tag>
        <tag>Supervisor</tag>
        <tag>项目部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 开机启动]]></title>
    <url>%2F2017%2F09%2F03%2Fnginxstartpoweron%2F</url>
    <content type="text"><![CDATA[Nginx 是一个很强大的高性能 Web 和反向代理服务器。虽然使用命令行可以对 nginx 进行各种操作，比如启动等，但是还是根据不太方便。下面介绍在 Linux 下安装后，如何设置开机自启动。 首先，在 Linux 系统的 /etc/init.d/ 目录下创建 nginx 文件，使用如下命令： 1vim /etc/init.d/nginx 在脚本中添加如下命令： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#!/bin/bash# nginx Startup script for the Nginx HTTP Server# it is v.0.0.2 version.# chkconfig: - 85 15# description: Nginx is a high-performance web and proxy server.# It has a lot of features, but it&apos;s not for everyone.# processname: nginx# pidfile: /var/run/nginx.pid# config: /usr/local/nginx/conf/nginx.conf# 启动 nginx 需要执行的命令nginxd=/usr/local/nginx/sbin/nginx# nginx 的配置文件路径nginx_config=/usr/local/nginx/conf/nginx.confnginx_pid=/var/run/nginx.pidRETVAL=0prog=&quot;nginx&quot;# Source function library.. /etc/rc.d/init.d/functions# Source networking configuration.. /etc/sysconfig/network# Check that networking is up.[ $&#123;NETWORKING&#125; = &quot;no&quot; ] &amp;&amp; exit 0[ -x $nginxd ] || exit 0# Start nginx daemons functions.start() &#123;if [ -e $nginx_pid ];then echo &quot;nginx already running....&quot; exit 1fi echo -n $&quot;Starting $prog: &quot; daemon $nginxd -c $&#123;nginx_config&#125; RETVAL=$? echo [ $RETVAL = 0 ] &amp;&amp; touch /var/lock/subsys/nginx return $RETVAL&#125;# Stop nginx daemons functions.stop() &#123; echo -n $&quot;Stopping $prog: &quot; killproc $nginxd RETVAL=$? echo [ $RETVAL = 0 ] &amp;&amp; rm -f /var/lock/subsys/nginx /var/run/nginx.pid&#125;# reload nginx service functions.reload() &#123; echo -n $&quot;Reloading $prog: &quot; #kill -HUP `cat $&#123;nginx_pid&#125;` killproc $nginxd -HUP RETVAL=$? echo&#125;# See how we were called.case &quot;$1&quot; instart) start ;;stop) stop ;;reload) reload ;;restart) stop start ;;status) status $prog RETVAL=$? ;;*) echo $&quot;Usage: $prog &#123;start|stop|restart|reload|status|help&#125;&quot; exit 1esacexit $RETVAL 以下是根据 nginx 具体安装路径填写的： 1234# 启动 nginx 需要执行的命令nginxd=/usr/local/nginx/sbin/nginx# nginx 的配置文件路径nginx_config=/usr/local/nginx/conf/nginx.conf 保存脚本文件后设置文件的执行权限： 1chmod a+x /etc/init.d/nginx 然后，就可以通过该脚本对 nginx 服务进行管理了： 12/etc/init.d/nginx start/etc/init.d/nginx stop 上面的方法完成了用脚本管理 nginx 服务的功能，但是还是不太方便，比如要设置 nginx 开机启动等。这时可以使用 chkconfig 来设置。 chkconfig 命令主要用来更新（启动或停止）和查询系统服务的运行级信息。谨记 chkconfig 不是立即自动禁止或激活一个服务，它只是简单的改变了符号连接。 先将 nginx 服务加入 chkconfig 管理列表： 1chkconfig --add /etc/init.d/nginx 加完这个之后，就可以使用 service 对 nginx 进行启动，重启等操作了： 12service nginx startservice nginx stop 设置终端模式开机启动： 1chkconfig nginx on]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库事物及其隔离级别]]></title>
    <url>%2F2017%2F08%2F28%2Fmysqltransaction%2F</url>
    <content type="text"><![CDATA[学习 MySQL 、SQL Server 或者 Oracle 这些关系型数据库的时候，必须了解的概念之一就是事务，在各种大中小型项目中，事务的使用非常频繁和广泛。本文简单介绍 MySQL 中事务的四大特性，以及在不考虑事务隔离时，对数据的不同操作会产生的一些影响，同时介绍了事务的四中隔离级别。 事务 ACID 特性原子性：Atomicity事务必须是原子工作单元；对于其执行的数据修改操作，要么全都执行，要么全都不执行。通常，与某个事务关联的操作具有共同的目标，并且是相互依赖的。如果系统只执行这些操作的一个子集，则可能会破坏事务的总体目标。原子性消除了系统处理操作子集的可能性。 一致性：Consistency事务在完成时，必须使所有的数据都保持一致状态。在相关数据库中，所有规则都必须应用于事务的修改，以保持所有数据的完整性。事务结束时，所有的内部数据结构（如 B 树索引或双向链表）都必须是正确的。某些维护一致性的责任由应用程序开发人员承担，他们必须确保应用程序已强制所有已知的完整性约束。例如，当开发用于转帐的应用程序时，应避免在转帐过程中任意移动小数点。假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 隔离性：Isolation由并发事务所作的修改必须与任何其它并发事务所作的修改隔离。事务查看数据时数据所处的状态，要么是另一并发事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看中间状态的数据。这称为隔离性，因为它能够重新装载起始数据，并且重播一系列事务，以使数据结束时的状态与原始事务执行的状态相同。当事务可序列化时将获得最高的隔离级别。在此级别上，从一组可并行执行的事务获得的结果与通过连续运行每个事务所获得的结果相同。由于高度隔离会限制可并行执行的事务数，所以一些应用程序降低隔离级别以换取更大的吞吐量。 即要达到这么一种效果：对于任意两个并发的事务 T1 和 T2，在事务 T1 看来，T2 要么在 T1 开始之前就已经结束，要么在 T1 结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 持久性：Durability事务完成之后，它对于系统的影响是永久性的。该修改即使出现致命的系统故障也将一直保持。 例如我们在操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。 以上介绍完事务的 ACID 特性，现在重点来说明下事务的隔离性，当多个线程都开启事务操作数据库中的数据时，数据库系统要能进行隔离操作，以保证各个线程获取数据的准确性，在介绍数据库提供的各种隔离级别之前，我们先看看如果不考虑事务的隔离性，会发生的几种问题： 并发事务的问题丢失更新当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，会发生丢失更新问题。每个事务都不知道其它事务的存在。最后的更新将重写由其它事务所做的更新，这将导致数据丢失。 例如，两个编辑人员制作了同一文档的电子复本。每个编辑人员独立地更改其复本，然后保存更改后的复本，这样就覆盖了原始文档。最后保存其更改复本的编辑人员覆盖了第一个编辑人员所做的更改。如果在第一个编辑人员完成之后第二个编辑人员才能进行更改，则可以避免该问题。 脏读读“脏”数据是指事务 T1 修改某一数据，并将其写回磁盘，事务 T2 读取同一数据后，T1 由于某种原因被除撤消，而此时T1 把已修改过的数据又恢复原值，T2 读到的数据与数据库的数据不一致，则 T2 读到的数据就为“脏”数据，即不正确的数据。简单来说就是，事务 T2 读取了事务 T1 还未提交的更新。 例如：一个编辑人员正在更改电子文档。在更改过程中，另一个编辑人员复制了该文档（该复本包含到目前为止所做的全部更改）并将其分发给预期的用户。此后，第一个编辑人员认为所做的更改是错误的，于是删除了所做的编辑并保存了文档。分发给用户的文档包含不再存在的编辑内容，并且这些编辑内容应认为从未存在过。如果在第一个编辑人员确定最终更改前任何人都不能读取更改的文档，则可以避免该问题。 不可重复读指事务 T1 读取数据后，事务 T2 执行更新操作，使 T1 无法读取前一次结果。 幻读按一定条件从数据库中读取了某些记录后，T2 删除了其中部分记录，当 T1 再次按相同条件读取数据时，发现某些记录消失；T1 按一定条件从数据库中读取某些数据记录后，T2 插入了一些记录，当 T1 再次按相同条件读取数据时，发现多了一些记录。 事务隔离级别读取未提交内容：Read Uncommitted 所有事务都可以看到其他未提交事务的执行结果 一个事物更新的时候不允许另一更新，但允许另一事务读取，不会出现丢失更新 该级别引发的问题是 - 脏读：读取到了未提交的数据 该隔离级别可以通过“排他写锁”实现。 读取提交内容：Read Committed 这是大多数数据库系统的默认隔离级别（但不是 MySQL 默认的） 一个事物更新的时候不允许读取，必须等到更新事物提交后才能读取 这种隔离级别出现的问题是 - 不可重复读：不可重复读意味着我们在同一个事务中执行完全相同的select语句时可能看到不一样的结果。导致这种情况的原因可能有： 有一个交叉的事务有新的commit，导致了数据的改变 一个数据库被多个实例操作时，同一事务的其他实例在该实例处理其间可能会有新的commit 可重复读：Repeatable Read 这是 MySQL 的默认事务隔离级别 一个事物读取时，不允许更新，但允许插入 此级别可能出现的问题 - 幻读：当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行 InnoDB 和 Falcon 存储引擎通过多版本并发控制(MVCC，Multiversion Concurrency Control)机制解决了该问题 序列化：Serializable 这是最高的隔离级别 它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁 在这个级别，可能导致大量的超时现象和锁竞争 Read More： 数据库事物隔离级别通俗理解 数据库事务 事务隔离级别 更改MySQL的默认事务隔离级别 数据库事务的四大特性以及事务的隔离级别 [MySQL]–通过例子理解事务的4中隔离级别]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>事物</tag>
        <tag>事物隔离</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例模式]]></title>
    <url>%2F2017%2F08%2F27%2Fsingleinstancepython%2F</url>
    <content type="text"><![CDATA[单例模式属于创建型模式，它提供了一种创建对象的最佳方式。这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。特点： 单例类只能有一个实例； 单例类必须自己创建自己的唯一实例； 单例类必须给所有其他对象提供这一实例。 使用场景： Python 的 logger 就是一个单例模式，用以日志记录； Windows 的资源管理器是一个单例模式； 线程池，数据库连接池等资源池一般也用单例模式； 网站计数器。 优点： 在内存中只有一个对象，节省内存空间； 避免频繁地创建销毁对象，可以提高性能； 避免对共享资源的多重占用； 可以全局访问。 非线程安全维护类中的共享变量的方式： 123456789101112class Singleton(object): _instance = None # 这里不能使用__init__，因为__init__是在instance已经生成以后才去调用的 # __new__负责创建新的实例并返回新的实例，__init__负责新创建实例的初始化工作无返回 def __new__(cls, *args, **kwargs): if cls.__instance is None: cls._instance = super(Singleton, cls).__new__(cls, *args, **kwargs) return cls._instance pass 创建实例时把所有实例的__dict__指向同一个字典，这样它们具有相同的属性和方法： 12345678class Singleton(object): _state = &#123;&#125; def __new__(cls, *args, **kw): ob = super(Singleton, cls).__new__(cls, *args, **kw) ob.__dict__ = cls._state return ob pass Python的装饰器方式，每次执行类中方法时，都会先执行装饰器方法，获取类实例： 1234567891011def singleton(cls, *args, **kw): instances = &#123;&#125; def getinstance(): if cls not in instances: instances[cls] = cls(*args, **kw) return instances[cls] return getinstance@singletonclass MyClass: ... 以上都是非线程安全的方法，一般我们在使用时已经足够。但是在多线程程序中，可能出现多个线程同时访问造成单例失败，我们可以引入锁机制来解决这个问题。 线程安全方式一： 12345678910111213141516import threadingclass Singleton(object): _instance = None lock = threading.RLock() def __new__(cls): cls.lock.acquire() if cls._instance is None: cls._instance = super(Singleton, cls).__new__(cls) cls.lock.release() return cls._instance pass 方式二： 123456789101112131415161718192021222324252627import threadingtry: from synchronize import make_synchronizedexcept ImportError: def make_synchronized(func): import threading func.__lock__ = threading.Lock() def synced_func(*args, **kws): with func.__lock__: return func(*args, **kws) return synced_funcclass Singleton(object): instance = None @make_synchronized def __new__(cls, *args, **kwargs): if cls.instance is None: cls.instance = object.__new__(cls, *args, **kwargs) return cls.instance def __init__(self): pass pass 设计模式的分类 类型 设计模式 创建型 工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式 结构型 适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式 行为型 策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式 设计模式的六大原则：总原则：开闭原则 开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，而是要扩展原有代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类等，后面的具体设计中我们会提到这点。 单一职责原则 不要存在多于一个导致类变更的原因，也就是说每个类应该实现单一的职责，如若不然，就应该把类拆分。 里氏替换原则 里氏代换原则面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。 里氏替换原则中，子类对父类的方法尽量不要重写和重载。因为父类代表了定义好的结构，通过这个规范的接口与外界交互，子类不应该随便破坏它。 依赖倒转原则 这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。写代码时用到具体类时，不与具体类交互，而与具体类的上层接口交互。 接口隔离原则 这个原则的意思是：每个接口中不存在子类用不到却必须实现的方法，如果不然，就要将接口拆分。使用多个隔离的接口，比使用单个接口（多个接口方法集合到一个的接口）要好。 迪米特法则（最少知道原则） 就是说：一个类对自己依赖的类知道的越少越好。也就是说无论被依赖的类多么复杂，都应该将逻辑封装在方法的内部，通过public方法提供给外部。这样当被依赖的类变化时，才能最小的影响该类。 最少知道原则的另一个表达方式是：只与直接的朋友通信。类之间只要有耦合关系，就叫朋友关系。耦合分为依赖、关联、聚合、组合等。我们称出现为成员变量、方法参数、方法返回值中的类为直接朋友。局部变量、临时变量则不是直接的朋友。我们要求陌生的类不要作为局部变量出现在类中。 合成复用原则 原则是尽量首先使用合成/聚合的方式，而不是使用继承。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>单例模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阻塞|非阻塞 - 异步|同步]]></title>
    <url>%2F2017%2F07%2F20%2Fzsfzsybtb%2F</url>
    <content type="text"><![CDATA[最近一直在看网络编程中同步异步、阻塞与非阻塞方面的东西，发现网上各种说法解释不一，而且有些解释不透彻，感觉解释人自己都没明白。于是在知乎上找到了能让人豁然开朗的答案，如下。 阻塞 与 非阻塞 、同步 与 异步不能简单的从字面理解，提供一个从分布式系统角度的回答。 同步与异步同步和异步关注的是消息通信机制。 所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由调用者主动等待这个调用的结果。而异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。 典型的异步编程模型比如 Node.js。举个通俗的例子： 你打电话问书店老板有没有《分布式系统》这本书，如果是同步通信机制，书店老板会说，你稍等，”我查一下”，然后开始查啊查，等查好了（可能是5秒，也可能是一天）告诉你结果（返回结果）。而异步通信机制，书店老板直接告诉你我查一下啊，查好了打电话给你，然后直接挂电话了（不返回结果）。然后查好了，他会主动打电话给你。在这里老板通过“回电”这种方式来回调。 阻塞与非阻塞阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态。 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 还是上面的例子，你打电话问书店老板有没有《分布式系统》这本书，你如果是阻塞式调用，你会一直把自己“挂起”，直到得到这本书有没有的结果，如果是非阻塞式调用，你不管老板有没有告诉你，你自己先一边去玩了， 当然你也要偶尔过几分钟check一下老板有没有返回结果。在这里阻塞与非阻塞与是否同步异步无关。跟老板通过什么方式回答你结果无关。 参考文章： 作者：严肃链接：https://www.zhihu.com/question/19732473/answer/20851256来源：知乎]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>阻塞</tag>
        <tag>非阻塞</tag>
        <tag>异步</tag>
        <tag>同步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux的用户态和内核态]]></title>
    <url>%2F2017%2F07%2F16%2Flinuxyhtnht%2F</url>
    <content type="text"><![CDATA[内核态:：CPU 可以访问内存所有数据，包括外围设备：硬盘、网卡。CPU 也可以将自己从一个程序切换到另一个程序。 用户态：只能受限的访问内存，且不允许访问外围设备。占用 CPU 的能力被剥夺，CPU 资源可以被其他程序获取。 Unix/Linux的体系架构 如上图所示，从宏观上来看，Linux 操作系统的体系架构分为用户态和内核态（或者用户空间和内核）。 内核从本质上看是一种软件—控制计算机的硬件资源，并提供上层应用程序运行的环境。 用户态即上层应用程序的活动空间，应用程序的执行必须依托于内核提供的资源，包括 CPU 资源、存储资源、I/O 资源等。为了使上层应用能够访问到这些资源，内核必须为上层应用提供访问的接口：即系统调用。 系统调用是操作系统的最小功能单位，这些系统调用根据不同的应用场景可以进行扩展和裁剪，现在各种版本的Unix实现都提供了不同数量的系统调用，如 Linux 的不同版本提供了 240-260 个系统调用，FreeBSD 大约提供了 320 个（reference：UNIX环境高级编程）。我们可以把系统调用看成是一种不能再化简的操作（类似于原子操作，但是不同概念），有人把它比作一个汉字的一个“笔画”，而一个“汉字”就代表一个上层应用。因此，有时候如果要实现一个完整的汉字（给某个变量分配内存空间），就必须调用很多的系统调用。如果从实现者（程序员）的角度来看，这势必会加重程序员的负担，良好的程序设计方法是：重视上层的业务逻辑操作，而尽可能避免底层复杂的实现细节。库函数正是为了将程序员从复杂的细节中解脱出来而提出的一种有效方法。它实现对系统调用的封装，将简单的业务逻辑接口呈现给用户，方便用户调用，从这个角度上看，库函数就像是组成汉字的“偏旁”。这样的一种组成方式极大增强了程序设计的灵活性，对于简单的操作，我们可以直接调用系统调用来访问资源，如“人”，对于复杂操作，我们借助于库函数来实现，如“仁”。 Shell 是一个特殊的应用程序，俗称命令行，本质上是一个命令解释器，它下通系统调用，上通各种应用，通常充当着一种“胶水”的角色，来连接各个小功能程序，让不同程序能够以一个清晰的接口协同工作，从而增强各个程序的功能。同时，Shell 是可编程的，它可以执行符合 Shell 语法的文本，这样的文本称为Shell 脚本，通常短短的几行 Shell 脚本就可以实现一个非常大的功能，原因就是这些 Shell 语句通常都对系统调用做了一层封装。为了方便用户和系统交互，一般，一个 Shell对应一个终端，终端是一个硬件设备，呈现给用户的是一个图形化窗口。我们可以通过这个窗口输入或者输出文本。这个文本直接传递给shell进行分析解释，然后执行。 用户态的应用程序可以通过三种方式来访问内核态的资源： 系统调用； 库函数； Shell 脚本。 下图是对上图的一个细分结构，从这个图上可以更进一步对内核所做的事有一个“全景式”的印象。 主要表现为： 向下控制硬件资源； 向内管理操作系统资源：包括进程的调度和管理、内存的管理、文件系统的管理、设备驱动程序的管理以及网络资源的管理； 向上则向应用程序提供系统调用的接口。 从整体上来看，整个操作系统分为两层：用户态和内核态，这种分层的架构极大地提高了资源管理的可扩展性和灵活性，而且方便用户对资源的调用和集中式的管理，带来一定的安全性。 用户态和内核态的切换因为操作系统的资源是有限的，如果访问资源的操作过多，必然会消耗过多的资源，而且如果不对这些操作加以区分，很可能造成资源访问的冲突。所以，为了减少有限资源的访问和使用冲突，Unix/Linux 的设计哲学之一就是：对不同的操作赋予不同的执行等级，就是所谓特权的概念。简单说就是有多大能力做多大的事，与系统相关的一些特别关键的操作必须由最高特权的程序来完成。 Intel 的 X86 架构的 CPU 提供了 0 到 3 四个特权级，数字越小，特权越高，Linux 操作系统中主要采用了 0 和 3 两个特权级，分别对应的就是内核态和用户态。运行于用户态的进程可以执行的操作和访问的资源都会受到极大的限制，而运行在内核态的进程则可以执行任何操作并且在资源的使用上没有限制。很多程序开始时运行于用户态，但在执行的过程中，一些操作需要在内核权限下才能执行，这就涉及到一个从用户态切换到内核态的过程。比如 C 函数库中的内存分配函数 malloc()，它具体是使用 sbrk() 系统调用来分配内存，当 malloc 调用 sbrk() 的时候就涉及一次从用户态到内核态的切换，类似的函数还有 printf()，调用的是 wirte() 系统调用来输出字符串，等等。 用户态到内核态的切换方式： 系统调用； 异常事件： 当 CPU 正在执行运行在用户态的程序时，突然发生某些预先不可知的异常事件，这个时候就会触发从当前用户态执行的进程转向内核态执行相关的异常事件，典型的如缺页异常。 外围设备的中断：当外围设备完成用户的请求操作后，会像 CPU 发出中断信号，此时，CPU 就会暂停执行下一条即将要执行的指令，转而去执行中断信号对应的处理程序，如果先前执行的指令是在用户态下，则自然就发生从用户态到内核态的转换。 注意：系统调用的本质其实也是中断，相对于外围设备的硬中断，这种中断称为软中断，这是操作系统为用户特别开放的一种中断，如 Linux int 80h 中断。所以，从触发方式和效果上来看，这三种切换方式是完全一样的，都相当于是执行了一个中断响应的过程。但是从触发的对象来看，系统调用是进程主动请求切换的，而异常和硬中断则是被动的。 参考文章： 内核态(Kernel Mode)与用户态(User Mode) Linux探秘之用户态与内核态]]></content>
      <categories>
        <category>Linux/Unix</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>用户态</tag>
        <tag>内核态</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析Python的垃圾回收机制]]></title>
    <url>%2F2017%2F07%2F16%2Fpythongc%2F</url>
    <content type="text"><![CDATA[之前写过一篇文章分析了 PHP 的垃圾回收机制，今天看了一下Python的GC，感觉大同小异，总结如下。 引用计数Python 语言默认采用的垃圾收集机制是『引用计数法 Reference Counting』，该算法最早 George E. Collins 在1960的时候首次提出，50 年后的今天，该算法依然被很多编程语言使用，『引用计数法』的原理是：每个对象维护一个 ob_refcnt 字段，用来记录该对象当前被引用的次数，每当新的引用指向该对象时，它的引用计数 ob_refcnt 加1，每当该对象的引用失效时计数 ob_refcnt 减 1，一旦对象的引用计数为 0，该对象立即被回收，对象占用的内存空间将被释放。它的缺点是需要额外的空间维护引用计数，这个问题是其次的，不过最主要的问题是它不能解决对象的“循环引用”，因此，也有很多语言比如 Java 并没有采用该算法做来垃圾的收集机制。 python里每一个东西都是对象，它们的核心就是一个结构体：PyObject 1234typedef struct_object &#123; int ob_refcnt; struct_typeobject *ob_type;&#125;PyObject; 什么是循环引用？A 和 B 相互引用而再没有外部引用 A 与 B 中的任何一个，它们的引用计数虽然都为 1，但显然应该被回收，例子： 123456a = &#123; &#125; #对象A的引用计数为 1 b = &#123; &#125; #对象B的引用计数为 1 a[&apos;b&apos;] = b #B的引用计数增1 b[&apos;a&apos;] = a #A的引用计数增1 del a #A的引用减 1，最后A对象的引用为 1 del b #B的引用减 1, 最后B对象的引用为 1 在这个例子中程序执行完 del 语句后，A、B 对象已经没有任何引用指向这两个对象，但是这两个对象各包含一个对方对象的引用，虽然最后两个对象都无法通过其它变量来引用这两个对象了，这对 GC 来说就是两个非活动对象或者说是垃圾对象，但是他们的引用计数并没有减少到 0。因此如果是使用引用计数法来管理这两对象的话，他们并不会被回收，它会一直驻留在内存中，就会造成了内存泄漏(内存空间在使用完毕后未释放)。为了解决对象的循环引用问题，Python 引入了标记-清除和分代回收两种 GC 机制。 标记清除『标记清除 Mark—Sweep 』算法是一种基于追踪回技术实现的垃圾回收算法。它分为两个阶段： 第一阶段是标记阶段，GC 会把所有的『活动对象』打上标记; 第二阶段是把那些没有标记的对象『非活动对象』进行回收。 那么 GC 又是如何判断哪些是活动对象哪些是非活动对象的呢？ 对象之间通过引用(指针)连在一起，构成一个有向图，对象构成这个有向图的节点，而引用关系构成这个有向图的边。从根对象(root object)出发，沿着有向边遍历对象，可达的(reachable)对象标记为活动对象，不可达的对象就是要被清除的非活动对象。根对象就是全局变量、调用栈、寄存器。 在上图中，我们把小黑圈视为全局变量，也就是把它作为 root object，从小黑圈出发，对象1可直达，那么它将被标记，对象2、3可间接到达也会被标记，而 4 和 5 不可达，那么 1、2、3 就是活动对象，4 和 5 是非活动对象会被 GC 回收。 标记清除算法作为 Python 的辅助垃圾收集技术主要处理的是一些容器对象，比如 list、dict、tuple，instance 等，因为对于字符串、数值对象是不可能造成循环引用问题。Python 使用一个双向链表将这些容器对象组织起来。不过，这种简单粗暴的标记清除算法也有明显的缺点：清除非活动的对象前它必须顺序扫描整个堆内存，哪怕只剩下小部分活动对象也要扫描所有对象。 分代回收分代回收是一种以空间换时间的操作方式，Python 将内存根据对象的存活时间划分为不同的集合，每个集合称为一个代。 12345678910#define NUM_GENERATIONS 3#define GEN_HEAD(n) (&amp;generations[n].head)/* linked lists of container objects */static struct gc_generation generations[NUM_GENERATIONS] = &#123; /* PyGC_Head, threshold, count */ &#123;&#123;&#123;GEN_HEAD(0), GEN_HEAD(0), 0&#125;&#125;, 700, 0&#125;, &#123;&#123;&#123;GEN_HEAD(1), GEN_HEAD(1), 0&#125;&#125;, 10, 0&#125;, &#123;&#123;&#123;GEN_HEAD(2), GEN_HEAD(2), 0&#125;&#125;, 10, 0&#125;,&#125;; Python 将内存分为了3 代，分别为年轻代(第0代)、中年代(第1代)、老年代(第2代)，他们对应的是 3 个链表，它们的垃圾收集频率与对象的存活时间的增大而减小。新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python 垃圾收集机制就会被触发，把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，依此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。同时，分代回收是建立在标记清除技术基础之上。分代回收同样作为 Python 的辅助垃圾收集技术处理那些容器对象。 参考文章： Python垃圾回收机制 Python中的垃圾回收机制]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>垃圾回收</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL存储引擎InnoDB与MyISAM的区别]]></title>
    <url>%2F2017%2F07%2F15%2Fmyisaminnodb%2F</url>
    <content type="text"><![CDATA[存储结构MyISAM：每个MyISAM在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义。数据文件的扩展名为 .MYD (MYData)。索引文件的扩展名是 .MYI (MYIndex)； InnoDB：所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。 存储空间MyISAM：可被压缩，存储空间较小。支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表； InnoDB：需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。 事务支持MyISAM：强调的是性能，每次查询具有原子性，其执行数度比 InnoDB 类型更快，但是不提供事务支持； InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务、回滚和崩溃修复能力的事务安全型表。 CURD操作MyISAM：如果执行大量的 SELECT，MyISAM 是更好的选择。(因为没有支持行级锁)，在增删的时候需要锁定整个表格，效率会低一些。相关的是 InnoDB 支持行级锁，删除插入的时候只需要锁定该行就行，效率较高； InnoDB：如果你的数据执行大量的 INSERT 或 UPDATE，出于性能方面的考虑，应该使用 InnoDB 表。DELETE 从性能上InnoDB 更优，但 DELETE FROM table 时，InnoDB 不会重新建立表，而是一行一行的删除，在 InnoDB 上如果要清空保存有大量数据的表，最好使用truncate table 这个命令。 外键MyISAM：不支持；InnoDB：支持。 锁机制InnoDB 支持数据行锁定，MyISAM 不支持行锁定，只支持锁定整个表。即 MyISAM 同一个表上的读锁和写锁是互斥的， MyISAM 并发读写时如果等待队列中既有读请求又有写请求，默认写请求的优先级高，即使读请求先到，所以 MyISAM 不适合于有大量查询和修改并存的情况，那样查询进程会长时间阻塞。因为 MyISAM 是锁表，所以某项读操作比较耗时会使其他写进程饿死。 索引InnoDB 从5.6.4才开始支持全文索引，而 MyISAM 一直支持。全文索引是指对 char、varchar 和 text 中的每个词（停用词除外）建立倒排序索引。MyISAM的全文索引其实很简单，因为它不支持中文分词，必须由使用者分词后加入空格再写到数据表里，而且少于4个汉字的词会和停用词一样被忽略掉。 其他MyISAM 支持 GIS 数据，InnoDB 不支持。即 MyISAM 支持以下空间数据对象：Point，Line，Polygon，Surface等。 对于AUTO_INCREMENT 类型的字段，InnoDB 中必须包含只有该字段的索引，但是在 MyISAM 表中，可以和其他字段一起建立联合索引。 InnoDB 中不保存表的具体行数，也就是说，执行 select count(*) from table 时，InnoDB 要扫描一遍整个表来计算有多少行，但是 MyISAM 只要简单的读出保存好的行数即可。注意的是，当 count(*) 语句包含 where 条件时，两种表的操作是一样的。 InnoDB 表的行锁也不是绝对的，假如在执行一个 SQL 语句时 MySQL 不能确定要扫描的范围，InnoDB 表同样会锁全表，例如 update table set num=1 where name like &#39;%aaa%&#39;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>InnoDB</tag>
        <tag>MyISAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL中UNIQUE KEY与PRIMARY KEY的区别]]></title>
    <url>%2F2017%2F07%2F15%2Funiqueprimarykey%2F</url>
    <content type="text"><![CDATA[定义了 UNIQUE 约束的字段中不能包含重复值，可以为一个或多个字段定义 UNIQUE 约束。因此，UNIQUE 即可以在字段级也可以在表级定义， 在 UNIQUED 约束的字段上可以包含空值。UNIQUED 可空，可以在一个表里的一个或多个字段定义； PRIMARY KEY 不可空不可重复，在一个表里可以定义联合主键； 一般来说，PRIMARY KEY = UNIQUE KEY + NOT NULL UNIQUE KEY 就是唯一，当你需要限定你的某个表字段每个值都唯一，没有重复值时使用。比如说，如果你有一个person 表，并且表中有个身份证的 column，那么你就可以指定该字段为UNIQUE KEY。 从技术的角度来看，PRIMARY KEY和UNIQUE KEY有很多相似之处。但还是有以下区别： 作为 PRIMARY KEY 的域/域组不能为NULL，而 UNIQUE KEY 可以。 在一个表中只能有一个PRIMARY KEY，而多个 UNIQUE KEY 可以同时存在。 更大的区别在逻辑设计上。PRIMARY KEY 一般在逻辑设计中用作记录标识，这也是设置 PRIMARY KEY 的本来用意，而UNIQUE KEY 只是为了保证域/域组的唯一性。 PRIMARY KEY 的语法：alter table name add constraint key name primary key( columns); UNIQUE KEY 的语法：alter table name add constraint key name unique( columns); 一个表只能有一个主键，但是可以有好多个UNIQUE KEY，而且 UNIQUE KEY 可以为 NULL 值，如员工的电话号码一般就用UNIQUE KEY，因为电话号码肯定是唯一的，但是有的员工可能没有电话。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>UNIQUE KEY</tag>
        <tag>PRIMARY KEY</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python:计算密集型任务和I/O密集型任务]]></title>
    <url>%2F2017%2F07%2F15%2Fjsmjiomj%2F</url>
    <content type="text"><![CDATA[I/O密集型任务 VS 计算密集型任务所谓 I/O 密集型任务，是指磁盘 I/O、网络 I/O 占主要的任务，计算量很小。比如请求网页、读写文件等。当然我们在Python中可以利用 sleep 达到 I/O 密集型任务的目的。 所谓计算密集型任务，是指 CPU 计算占主要的任务，CPU 一直处于满负荷状态。比如在一个很大的列表中查找元素（当然这不合理），复杂的加减乘除等。 多线程 VS 多进程Python中比较常见的并发方式主要有两种：多线程和多进程。 GILGIL 的全称是 Global Interpreter Lock (全局解释器锁)，来源是 python 设计之初的考虑，为了数据安全所做的决定。在Python多线程下，每个线程的执行方式： 获取GIL ； 执行代码直到sleep或者是python虚拟机将其挂起； 释放GIL。 可见，某个线程想要执行，必须先拿到 GIL，我们可以把 GIL 看作是“通行证”，并且在一个 Python 进程中，GIL 只有一个。拿不到通行证的线程，就不允许进入 CPU 执行。 注意：多核多线程不一定比单核多线程更好。原因是单核下多线程，每次释放 GIL，唤醒的那个线程都能获取到 GIL 锁，所以能够无缝执行，但多核下，CPU0 释放 GIL 后，其他 CPU 上的线程都会进行竞争，但 GIL 可能会马上又被 CPU0 拿到，导致其他几个 CPU 上被唤醒后的线程会醒着等待到切换时间后又进入待调度状态，这样会造成线程颠簸，导致效率更低。 多线程多线程即在一个进程中启动多个线程执行任务。一般来说使用多线程可以达到并行的目的，但由于 Python 中使用了全局解释锁 GIL 的概念，导致 Python 中的多线程并不是并行执行，而是“交替执行”。类似于下图：（图片转自网络，侵删） 所以 Python 中的多线程适合 I/O 密集型任务，而不适合计算密集型任务。 Python 提供两组多线程接口，一是 thread 模块 _thread，提供低等级接口。二是 threading 模块，提供更容易使用的基于对象的接口，可以继承 Thread 对象来实现线程，此外其还提供了其它线程相关的对象，例如 Timer，Lock 等。 多进程由于 Python 中 GIL 的原因，对于计算密集型任务，Python 下比较好的并行方式是使用多进程，这样可以非常有效的使用CPU 资源。当然同一时间执行的进程数量取决你电脑的 CPU 核心数。 Python 中的进程模块为 mutliprocess 模块，提供了很多容易使用的基于对象的接口。另外它提供了封装好的管道和队列，可以方便的在进程间传递消息。Python 还提供了进程池 Pool 对象，可以方便的管理和控制线程。 Example这里通过一个实例，说明多线程适合IO密集型任务，多进程适合计算密集型任务。 首先定义一个队列，并定义初始化队列的函数： 1234567891011# 定义全局变量Queue，与Queue.Queue()的不同之处在于，multiprocessing.Queue()支持跨进程使用g_queue = multiprocessing.Queue()def init_queue(): print(&quot;init g_queue start&quot;) while not g_queue.empty(): g_queue.get() for _index in range(10): g_queue.put(_index) print(&quot;init g_queue end&quot;) return 然后定义 I/O 密集型任务和计算密集型任务，分别从队列中获取任务数据： 12345678910111213141516171819202122232425262728293031# 定义一个IO密集型任务：利用time.sleep()def task_io(task_id): print(&quot;IOTask[%s] start&quot; % task_id) while not g_queue.empty(): time.sleep(1) try: data = g_queue.get(block=True, timeout=1) print(&quot;IOTask[%s] get data: %s&quot; % (task_id, data)) except Exception as ex: print(&quot;IOTask[%s] error: %s&quot; % (task_id, str(ex))) print(&quot;IOTask[%s] end&quot; % task_id) returng_search_list = list(range(10000))# 定义一个计算密集型任务：利用一些复杂加减乘除、列表查找等def task_cpu(task_id): print(&quot;CPUTask[%s] start&quot; % task_id) while not g_queue.empty(): count = 0 for i in range(10000): count += pow(3 * 2, 3 * 2) if i in g_search_list else 0 try: data = g_queue.get(block=True, timeout=1) print(&quot;CPUTask[%s] get data: %s&quot; % (task_id, data)) except Exception as excep: print(&quot;CPUTask[%s] error: %s&quot; % (task_id, str(excep))) print(&quot;CPUTask[%s] end&quot; % task_id) return task_id 准备完上述代码之后，进行试验： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758if __name__ == &apos;__main__&apos;: print(&quot;cpu count:&quot;, multiprocessing.cpu_count(), &quot;\n&quot;) print(&quot;========== 直接执行IO密集型任务 ==========&quot;) init_queue() time_0 = time.time() task_io(0) print(&quot;结束：&quot;, time.time() - time_0, &quot;\n&quot;) print(&quot;========== 多线程执行IO密集型任务 ==========&quot;) init_queue() time_0 = time.time() thread_list = [threading.Thread(target=task_io, args=(i,)) for i in range(4)] for t in thread_list: t.start() for t in thread_list: if t.is_alive(): t.join() print(&quot;结束：&quot;, time.time() - time_0, &quot;\n&quot;) print(&quot;========== 多进程执行IO密集型任务 ==========&quot;) init_queue() time_0 = time.time() process_list = [multiprocessing.Process(target=task_io, args=(i,)) for i in range(multiprocessing.cpu_count())] for p in process_list: p.start() for p in process_list: if p.is_alive(): p.join() print(&quot;结束：&quot;, time.time() - time_0, &quot;\n&quot;) print(&quot;========== 直接执行计算密集型任务 ==========&quot;) init_queue() time_0 = time.time() task_cpu(0) print(&quot;结束：&quot;, time.time() - time_0, &quot;\n&quot;) print(&quot;========== 多线程执行计算密集型任务 ==========&quot;) init_queue() time_0 = time.time() thread_list = [threading.Thread(target=task_cpu, args=(i,)) for i in range(4)] for t in thread_list: t.start() for t in thread_list: if t.is_alive(): t.join() print(&quot;结束：&quot;, time.time() - time_0, &quot;\n&quot;) print(&quot;========== 多进程执行计算密集型任务 ==========&quot;) init_queue() time_0 = time.time() process_list = [multiprocessing.Process(target=task_cpu, args=(i,)) for i in range(multiprocessing.cpu_count())] for p in process_list: p.start() for p in process_list: if p.is_alive(): p.join() print(&quot;结束：&quot;, time.time() - time_0, &quot;\n&quot;) 结果如下： 12345678910111213========== 直接执行IO密集型任务 ==========结束： 10.041141033172607 ========== 多线程执行IO密集型任务 ==========结束： 5.073023557662964 ========== 多进程执行IO密集型任务 ==========结束： 1.1165637969970703 ========== 直接执行计算密集型任务 ==========结束： 36.41951608657837 ========== 多线程执行计算密集型任务 ==========结束： 47.48710823059082 ========== 多进程执行计算密集型任务 ==========结束： 0.82016921043396 在执行计算密集型任务时，多进程的方式明显比多线程更好。 参考文章： Python进阶：聊聊IO密集型任务、计算密集型任务，以及多线程、多进程 为什么在python里推荐使用多进程而不是多线程？–转同事的一篇文章 Python编程（二）：Python进程、线程那点事儿]]></content>
      <categories>
        <category>进程/线程</category>
      </categories>
      <tags>
        <tag>计算密集型任务</tag>
        <tag>I/O密集型任务</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[子进程和线程的区别]]></title>
    <url>%2F2017%2F07%2F15%2Fthreadsubprocess%2F</url>
    <content type="text"><![CDATA[相同点 二者都具有 ID ，一组寄存器，状态，优先级以及所要遵循的调度策略； 每个进程都有一个进程控制块，线程也拥有一个线程控制块； 线程和子进程共享父进程中的资源；线程和子进程独立于它们的父进程，竞争使用处理器资源；线程和子进程的创建者可以在线程和子进程上实行某些控制，比如，创建者可以取消、挂起、继续和修改线程和子进程的优先级；线程和子进程可以改变其属性并创建新的资源。 不同点 线程是进程的一部分, 一个没有线程的进程是可以被看作单线程的，如果一个进程内拥有多个进程，进程的执行过程不是一条线（线程）的，而是多条线（线程）共同完成的； 启动一个线程所花费的空间远远小于启动一个进程所花费的空间，而且，线程间彼此切换所需的时间也远远小于进程间切换所需要的时间； 系统在运行的时候会为每个进程分配不同的内存区域，但是不会为线程分配内存（线程所使用的资源是它所属的进程的资源），线程组只能共享资源。对不同进程来说，它们具有独立的数据空间，要进行数据的传递只能通过通信的方式进行，这种方式不仅费时，而且很不方便。而一个线程的数据可以直接为其他线程所用，这不仅快捷，而且方便； 与进程的控制表 PCB 相似，线程也有自己的控制表 TCB，但是 TCB 中所保存的线程状态比 PCB 表中少多了； 进程是系统所有资源分配时候的一个基本单位，拥有一个完整的虚拟空间地址，并不依赖线程而独立存在。 例子进程和线程的区别在于粒度不同，进程之间的变量(或者说是内存)是不能直接互相访问的，而线程可以。线程一定会依附在某一个进程上执行.我举个例子，你在 Windows 下开一个 IE 浏览器，这个 IE 浏览器是一个进程，你用浏览器去打开一个pdf，IE 就去调用 Acrobat 去打开，这时 Acrobat 是一个独立的进程，就是 IE 的子进程。而 IE 自己本身同时用同一个进程开了 2 个网页, 并且同时在跑两个网页上的脚本，这两个网页的执行就是 IE 自己通过两个线程实现的。值得注意的是，线程仍然是 IE 的内容，而子进程 Acrobat 严格来说就不属于 IE ，是另外一个程序。之所以是 IE 的子进程，只是受 IE 调用而启动的而已。子进程与父进程之间可以通过动态数据交换、OLE、管道、邮件槽等进行通信，使用内存映射文件是最便利的方法之一。 参考文章： 子进程和线程的区别]]></content>
      <categories>
        <category>进程/线程</category>
      </categories>
      <tags>
        <tag>线程</tag>
        <tag>子进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP基础知识 - $_SERVER]]></title>
    <url>%2F2017%2F06%2F26%2Fphp-server%2F</url>
    <content type="text"><![CDATA[参数 说明 $_SERVER[‘PHP_SELF’] 当前执行脚本的文件名，与 document root 有关 $_SERVER[‘argv’] 传递给该脚本的参数的数组 $_SERVER[‘argc’] 包含命令行模式下传递给该脚本的参数的数目(如果运行在命令行模式下) $_SERVER[‘ GATEWAY_INTERFACE 服务器使用的 CGI 规范的版本 $_SERVER[‘ SERVER_ADDR’] 当前运行脚本所在的服务器的 IP 地址 $_SERVER[‘ SERVER_NAME’] 当前运行脚本所在的服务器的主机名 $_SERVER[‘SERVER_SOFTWARE’] 服务器标识字符串，在响应请求时的头信息中给出 $_SERVER[‘SERVER_PROTOCOL’] 请求页面时通信协议的名称和版本 $_SERVER[‘REQUEST_METHOD’] 访问页面使用的请求方法。 例如，“GET”、“HEAD”、“POST”、“PUT” $_SERVER[‘REQUEST_TIME’] 请求开始时的时间戳 $_SERVER[‘REQUEST_TIME_FLOAT’] 请求开始时的时间戳，微秒级别的精准度 $_SERVER[‘QUERY_STRING’] 查询字符串，如果有的话，通过它进行页面访问 $_SERVER[‘DOCUMENT_ROOT’] 当前运行脚本所在的文档根目录 $_SERVER[‘HTTP_ACCEPT’] 当前请求头中 Accept: 项的内容 $_SERVER[‘HTTP_ACCEPT_CHARSET’] 当前请求头中 Accept-Charset: 项的内容 $_SERVER[‘HTTP_ACCEPT_ENCODING’] 当前请求头中 Accept-Encoding: 项的内容 $_SERVER[‘HTTP_ACCEPT_LANGUAGE’] 当前请求头中 Accept-Language: 项的内容 $_SERVER[‘HTTP_CONNECTION’] 当前请求头中 Connection: 项的内容 $_SERVER[‘HTTP_HOST’] 当前请求头中 Host: 项的内容 $_SERVER[‘HTTP_REFERER’] 引导用户代理到当前页的前一页的地址 $_SERVER[‘HTTP_USER_AGENT’] 当前请求头中 User-Agent: 项的内容， 该字符串表明了访问该页面的用户代理的信息 $_SERVER[‘HTTPS’] 如果脚本是通过 HTTPS 协议被访问，则被设为一个非空的值 $_SERVER[‘REMOTE_ADDR’] 浏览当前页面的用户的 IP 地址 $_SERVER[‘REMOTE_HOST’] 浏览当前页面的用户的主机名 $_SERVER[‘REMOTE_PORT’] 用户机器上连接到 Web 服务器所使用的端口号 $_SERVER[‘REMOTE_USER’] 经验证的用户 $_SERVER[‘REDIRECT_REMOTE_USER’] 验证的用户，如果请求已在内部重定向 $_SERVER[‘SCRIPT_FILENAME’] 当前执行脚本的绝对路径 $_SERVER[‘SERVER_ADMIN’] 该值指明了 Apache 服务器配置文件中的 SERVER_ADMIN 参数 $_SERVER[‘SERVER_PORT’] Web 服务器使用的端口 $_SERVER[‘SERVER_SIGNATURE’] 包含了服务器版本和虚拟主机名的字符串 $_SERVER[‘PATH_TRANSLATED’] 当前脚本所在文件系统（非文档根目录）的基本路径 $_SERVER[‘SCRIPT_NAME’] 包含当前脚本的路径 $_SERVER[‘REQUEST_URI’] URI 用来指定要访问的页面 $_SERVER[‘PHP_AUTH_DIGEST’] 当作为 Apache 模块运行时，进行 HTTP Digest 认证的过程中，此变量被设置成客户端发送的“Authorization” HTTP 头内容 $_SERVER[‘PHP_AUTH_USER’] 当 PHP 运行在 Apache 或 IIS（PHP 5 是 ISAPI）模块方式下，并且正在使用 HTTP 认证功能，这个变量便是用户输入的用户名 $_SERVER[‘PHP_AUTH_PW’] 当 PHP 运行在 Apache 或 IIS（PHP 5 是 ISAPI）模块方式下，并且正在使用 HTTP 认证功能，这个变量便是用户输入的密码 $_SERVER[‘AUTH_TYPE’] 当 PHP 运行在 Apache 模块方式下，并且正在使用 HTTP 认证功能，这个变量便是认证的类型 $_SERVER[‘ PATH_INFO’] 包含由客户端提供的、跟在真实脚本名称之后并且在查询语句（query string）之前的路径信息 $_SERVER[‘ORIG_PATH_INFO’] 在被 PHP 处理之前，“PATH_INFO” 的原始版本]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>$_SERVER</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从输入 URL 到页面展示到底发生了什么]]></title>
    <url>%2F2017%2F06%2F22%2Furlrequestprocess%2F</url>
    <content type="text"><![CDATA[当我们在浏览器中输入 www.google.com 后，浏览器会给我们展示最终页面，本文就来讨论一下展示页面的过程到底是怎样的。 输入网络地址当我们开始在浏览器中输入网址的时候，浏览器其实就已经在智能的匹配可能得 URL 了，他会从历史记录，书签等地方，找到已经输入的字符串可能对应的 URL，然后给出智能提示，让你可以补全 URL 地址。对于 google 的 Chrome 的浏览器，他甚至会直接从缓存中把网页展示出来，就是说，你还没有按下 Enter，页面就出来了。 DNS解析获取域名IP地址请求一旦发起，浏览器首先要做的事情就是解析这个域名，一般来说，浏览器会首先查看本地硬盘的 hosts 文件，看看其中有没有和这个域名对应的规则，如果有的话就直接使用 hosts 文件里面的 IP 地址。 如果在本地的 hosts 文件没有能够找到对应的 IP 地址，浏览器会发出一个 DNS 请求到本地 DNS 服务器 。本地 DNS 服务器一般都是你的网络接入服务器商提供，比如中国电信，中国移动。 查询你输入的网址的 DNS 请求到达本地 DNS 服务器之后，本地 DNS 服务器会首先查询它的缓存记录，如果缓存中有此条记录，就可以直接返回结果，此过程是递归的方式进行查询。如果没有，本地 DNS 服务器还要向 DNS 根服务器进行查询。 根 DNS 服务器没有记录具体的域名和 IP 地址的对应关系，而是告诉本地 DNS 服务器，你可以到域服务器上去继续查询，并给出域服务器的地址。这种过程是迭代的过程。 本地 DNS 服务器继续向域服务器发出请求，在这个例子中，请求的对象是.com域服务器。.com域服务器收到请求之后，也不会直接返回域名和 IP 地址的对应关系，而是告诉本地 DNS 服务器，你的域名的解析服务器的地址。 最后，本地 DNS 服务器向域名的解析服务器发出请求，这时就能收到一个域名和IP地址对应关系，本地 DNS 服务器不仅要把 IP 地址返回给用户电脑，还要把这个对应关系保存在缓存中，以备下次别的用户查询时，可以直接返回结果，加快网络访问。 DNS缓存DNS 存在着多级缓存，从离浏览器的距离排序的话，有以下几种：浏览器缓存，系统缓存，路由器缓存，IPS服务器缓存，根域名服务器缓存，顶级域名服务器缓存，主域名服务器缓存。 在你的 Chrome 浏览器中输入 chrome://dns/，你可以看到 Chrome 浏览器的 DNS 缓存 系统缓存主要存在 /etc/hosts 中 DNS负载均衡当一个网站有足够多的用户的时候，假如每次请求的资源都位于同一台机器上面，那么这台机器随时可能会宕掉。处理办法就是用 DNS 负载均衡技术，它的原理是在 DNS 服务器中为同一个主机名配置多个 IP 地址，在应答 DNS 查询时，DNS 服务器对每个查询将以 DNS 文件中主机记录的IP地址按顺序返回不同的解析结果，将客户端的访问引导到不同的机器上去，使得不同的客户端访问不同的服务器，从而达到负载均衡的目的。例如可以根据每台机器的负载量，该机器离用户地理位置的距离等等。 建立TCP连接获取域名对应的 IP 地址之后，浏览器会以一个随机端口，进入到网卡，然后是进入到内核的 TCP/IP 协议栈，还有可能要经过 NetFilter 防火墙（属于内核的模块）的过滤，最终到达 Web 程序，最终建立了 TCP/IP 的连接。 因为 HTTP 报文是包裹在 TCP 报文中发送的，在发送 HTTP 请求之前需要客户端与服务器进行 TCP 连接以保证数据的正确稳定传输。TCP/IP协议可以参考 TCP/IP协议入门篇 发送HTTP请求发送 HTTP 请求的过程就是构建 HTTP 请求报文并通过 TCP 协议中发送到服务器指定端口(HTTP 协议80/8080, HTTPS 协议443)。HTTP 请求报文是由三部分组成：请求行, 请求报头和请求正文。 请求行格式如下:Method Request-URL HTTP-Version CRLF eg：GET index.html HTTP/1.1常用的方法有：GET、POST、PUT、DELETE、OPTIONS、HEAD。 请求报头请求报头允许客户端向服务器传递请求的附加信息和客户端自身的信息。 客户端不一定特指浏览器，有时候也可使用 Linux 下的 CURL 命令以及 HTTP 客户端测试工具等。常见的请求报头有：Accept、Accept-Charset、Accept-Encoding、Accept-Language、Content-Type、Authorization、Cookie、User-Agent等。 请求正文当使用POST、PUT等方法时，通常需要客户端向服务器传递数据。这些数据就储存在请求正文中。在请求包头中有一些与请求正文相关的信息，例如：现在的 Web 应用通常采用 Rest 架构，请求的数据格式一般为 JSON。这时就需要设置Content-Type:application/json。 服务器永久重定向响应服务器给浏览器响应一个 301 永久重定向响应，这样浏览器就会访问 “http://www.google.com/” 而非 “http://google.com/”。 为什么服务器一定要重定向而不是直接发送用户想看的网页内容呢？其中一个原因跟搜索引擎排名有关。如果一个页面有两个地址，就像 http://www.yy.com/ 和 http://yy.com/ ，搜索引擎会认为它们是两个网站，结果造成每个搜索链接都减少从而降低排名。而搜索引擎知道 301 永久重定向是什么意思，这样就会把访问带 www 的和不带 www 的地址归到同一个网站排名下。还有就是用不同的地址会造成缓存友好性变差，当一个页面有好几个名字时，它可能会在缓存里出现好几次。 301和302的区别301 和 302 状态码都表示重定向，就是说浏览器在拿到服务器返回的这个状态码后会自动跳转到一个新的 URL 地址，这个地址可以从响应的 Location 首部中获取（用户看到的效果就是他输入的地址 A 瞬间变成了另一个地址 B）——这是它们的共同点。 他们的不同在于。301 表示旧地址 A 的资源已经被永久地移除了（这个资源不可访问了），搜索引擎在抓取新内容的同时也将旧的网址交换为重定向之后的网址； 302 表示旧地址 A 的资源还在（仍然可以访问），这个重定向只是临时地从旧地址 A 跳转到地址 B，搜索引擎会抓取新的内容而保存旧的网址。 重定向原因 网站调整（如改变网页目录结构）； 网页被移到一个新地址； 网页扩展名改变(如应用需要把 .php 改成 .Html 或 .shtml )。 这种情况下，如果不做重定向，则用户收藏夹或搜索引擎数据库中旧地址只能让访问客户得到一个 404 页面错误信息，访问流量白白丧失；再者某些注册了多个域名的网站，也需要通过重定向让访问这些域名的用户自动跳转到主站点等。 跳转的时机当一个网站或者网页 24-48 小时内临时移动到一个新的位置，这时候就要进行 302 跳转，而使用301 跳转的场景就是之前的网站因为某种原因需要移除掉，然后要到新的地址访问，是永久性的。 清晰明确而言：使用 301 跳转的大概场景如下： 域名到期不想续费（或者发现了更适合网站的域名），想换个域名 在搜索引擎的搜索结果中出现了不带 www 的域名，而带 www 的域名却没有收录，这个时候可以用301重定向来告诉搜索引擎我们目标的域名是哪一个 空间服务器不稳定，换空间的时候 浏览器跟踪重定向地址现在浏览器知道了 “http://www.google.com/” 才是要访问的正确地址，所以它会发送另一个http请求。 服务器处理请求数据的接收我们知道，IP 协议的作用是把 TCP 分割好的各种数据包传送给接收方。而要保证确实能传到接收方还需要接收方的 MAC 地址，也就是物理地址。IP 地址和 MAC 地址是一一对应的关系，一个网络设备的 IP 地址可以更换，但是 MAC 地址一般是固定不变的。ARP 协议可以将 IP 地址解析成对应的 MAC 地址。当通信的双方不在同一个局域网时，需要多次中转才能到达最终的目标，在中转的过程中需要通过下一个中转站的MAC地址来搜索下一个中转目标。 在找到对方的 MAC 地址后，就将数据发送到数据链路层传输。接收端的服务器在链路层接收到数据包，再层层向上直到应用层。这过程中包括在运输层通过TCP协议讲分段的数据包重新组成原来的HTTP请求报文。 处理请求一些大一点的网站会将你的请求转到反向代理服务器中，因为当网站访问量非常大，网站越来越慢，一台服务器已经不够用了。于是将同一个应用部署在多台服务器上，将大量用户的请求分配给多台机器处理。此时，客户端不是直接通过 HTTP 协议访问某网站应用服务器，而是先请求到 Nginx，Nginx 再请求应用服务器，然后将结果返回给客户端，这里 Nginx 的作用是反向代理服务器。同时也带来了一个好处，其中一台服务器万一挂了，只要还有其他服务器正常运行，就不会影响用户使用。 用户将 HTTP 请求发送给 Nginx 服务器； Nginx 会根据用户访问的 URL 和后缀对请求进行判断，如果请求的是静态资源（HTML页面）会直接返回，如果请求的是动态资源，执行下一步； Nginx 会通过 fastcgi_pass 将用户的请求发送给 PHP-FPM； fastcgi_pass 将动态资源交给 PHP-FPM 后，PHP-FPM 会将资源转给 PHP 脚本解析服务器的 Wrapper； Wrapper 收到 PHP-FPM 转过来的请求后，Wrapper 会生成一个新的线程调用 PHP 动态程序解析服务器； PHP 会将查询到的结果返回给 Nginx ； Nginx 构造一个响应报文将结果返回给用户。 返回HTTP响应HTTP 响应与 HTTP 请求相似，HTTP 响应也由3个部分构成，分别是：状态行、响应头、响应正文 状态行状态行由协议版本、数字形式的状态代码、及相应的状态描述，各元素之间以空格分隔。 格式: HTTP-Version Status-Code Reason-Phrase CRLF 例如: HTTP/1.1 200 OK 响应头 响应正文服务器返回给浏览器的文本信息，通常 HTML、CSS、JS, 图片等文件就放在这一部分。 页面渲染在浏览器没有完整接受全部 HTML 文档时，它就已经开始显示这个页面了，浏览器是如何把页面呈现在屏幕上的呢？不同浏览器可能解析的过程不太一样，这里我们只介绍 WebKit 的渲染过程，下图对应的就是 WebKit 渲染的过程，这个过程包括： 解析 HTML 以构建 DOM 树 -&gt; 构建 Render 树 -&gt; 布局 Render 树 -&gt; 绘制 Render 树 浏览器在解析 HTML 文件时，会”自上而下“加载，并在加载过程中进行解析渲染。在解析过程中，如果遇到请求外部资源时，如图片、外链的 CSS、iconfont 等，请求过程是异步的，并不会影响 HTML 文档进行加载。 解析过程中，浏览器首先会解析 HTML 文件构建 DOM 树，然后解析 CSS 文件构建渲染树，等到渲染树构建完成后，浏览器开始布局渲染树并将其绘制到屏幕上。这个过程比较复杂，涉及到两个概念：reflow(回流)和 repain(重绘)。 DOM 节点中的各个元素都是以盒模型的形式存在，这些都需要浏览器去计算其位置和大小等，这个过程称为relow；当盒模型的位置、大小以及其他属性，如颜色、字体等确定下来之后，浏览器便开始绘制内容，这个过程称为repain。 页面在首次加载时必然会经历 reflow 和 repain。reflow 和 repain 过程是非常消耗性能的，尤其是在移动设备上，它会破坏用户体验，有时会造成页面卡顿。所以我们应该尽可能少的减少 reflow 和 repain。 当文档加载过程中遇到 JS 文件，HTML 文档会挂起渲染（加载解析渲染同步）的线程，不仅要等待文档中 JS 文件加载完毕，还要等待解析执行完毕，才可以恢复 HTML 文档的渲染线程。因为 JS 有可能会修改DOM，最为经典的document.write，这意味着，在 JS 执行完成前，后续所有资源的下载可能是没有必要的，这是 JS 阻塞后续资源下载的根本原因。所以我明平时的代码中， JS 是放在 HTML 文档末尾的。 JS 的解析是由浏览器中的 JS 解析引擎完成的，比如谷歌的是V8。 JS 是单线程运行，也就是说，在同一个时间内只能做一件事，所有的任务都需要排队，前一个任务结束，后一个任务才能开始。但是又存在某些任务比较耗时，如IO读写等，所以需要一种机制可以先执行排在后面的任务，这就是：同步任务(synchronous)和异步任务(asynchronous)。 JS 的执行机制就可以看做是一个主线程加上一个任务队列(task queue)。同步任务就是放在主线程上执行的任务，异步任务是放在任务队列中的任务。所有的同步任务在主线程上执行，形成一个执行栈;异步任务有了运行结果就会在任务队列中放置一个事件；脚本运行时先依次运行执行栈，然后会从任务队列里提取事件，运行任务队列中的任务，这个过程是不断重复的，所以又叫做事件循环(Event loop)。 在浏览器还没接收到完整的 HTML 文件时，它就开始渲染页面了，在遇到外部链入的脚本标签或样式标签或图片时，会再次发送 HTTP 请求重复上述的步骤。在收到 CSS 文件后会对已经渲染的页面重新渲染，加入它们应有的样式，图片文件加载完立刻显示在相应位置。在这一过程中可能会触发页面的重绘或重排。 参考文章： 从输入URL到浏览器显示页面发生了什么 从输入 URL 到页面展示到底发生了什么 百度面试题：从输入url到显示网页，后台发生了什么？ 从输入URL到页面加载发生了什么? HTTP 响应头信息]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
        <tag>网络编程</tag>
        <tag>TCP/IP</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP初级算法系列 - 2]]></title>
    <url>%2F2017%2F06%2F21%2Fbdphp2%2F</url>
    <content type="text"><![CDATA[算法功能：返回数组中最小的 k 个数。 算法实现1： 123456789101112131415161718192021222324252627class BDphp3&#123; public function bd_php3($tmp_array, $k) &#123; $count = 0; $length = count($tmp_array); if ($length &lt; $k) &#123; return false; &#125; for ($i = 0; $i &lt; $length; $i++) &#123; $min = $i; for ($j = $i + 1; $j &lt; $length; $j++) &#123; if ($tmp_array[$j] &lt; $tmp_array[$min]) &#123; $min = $j; &#125; &#125; $tmp = $tmp_array[$i]; $tmp_array[$i] = $tmp_array[$min]; $tmp_array[$min] = $tmp; $count++; if ($count &gt; $k) &#123; break; &#125; &#125; return array_slice($tmp_array, 0, $k); &#125;&#125; 算法实现2： 1234567891011121314151617181920212223242526class BDphp3&#123; private function replaceMaxValue($arr, $otherNum) &#123; $max = $arr[0]; for ($i = 1; $i &lt; count($arr); $i++) &#123; if ($arr[$i] &gt; $max) &#123; $max = &amp;$arr[$i]; &#125; &#125; if ($otherNum &lt; $max) &#123; $max = $otherNum; &#125; return $arr; &#125; public function getTop($arr, $topNum) &#123; $topK = array_slice($arr, 0, $topNum); $other = array_slice($arr, $topNum); foreach ($other as $value) &#123; $topK = $this-&gt;replaceMaxValue($topK, $value); &#125; return $topK; &#125;&#125; 算法说明：第一种方式中改进了直接插入排序算法，只是将数组排序 k 次，然后返回数组的前 k 个数。第二种方式将数组分割成两部分，前 k 个数值为第一部分，其余数值为第二部分，对于第二部分的每个数值，如果存在小于第一部分最大值的数值，将第一部分的最大值替换，最后返回第一部分。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP初级算法系列 - 1]]></title>
    <url>%2F2017%2F06%2F21%2Fbdphp1%2F</url>
    <content type="text"><![CDATA[给定一个数组： 1$arr = array(&apos;b&apos; =&gt; &apos;a&apos;, &apos;c&apos; =&gt; &apos;a&apos;, &apos;e&apos; =&gt; &apos;b&apos;, &apos;d&apos; =&gt; &apos;b&apos;, &apos;f&apos; =&gt; &apos;c&apos;, &apos;g&apos; =&gt; &apos;e&apos;, &apos;h&apos; =&gt; &apos;f&apos;); 给出可以完成以下格式转换的算法。 1234567891011121314151617181920212223242526Array( [a] =&gt; Array ( [b] =&gt; Array ( [e] =&gt; Array ( [0] =&gt; g ) [0] =&gt; d ) [c] =&gt; Array ( [f] =&gt; Array ( [0] =&gt; h ) ) )) 算法实现： 1234567891011121314151617181920212223242526272829class BDphp1&#123; private function php1($tmp_array, $start) &#123; $result = array(); if ($ret = array_keys($tmp_array, $start)) &#123; foreach ($ret as $v) &#123; if ($rev = $this-&gt;php1($tmp_array, $v)) &#123; $result[$v] = $rev; &#125; else &#123; $result[] = $v; &#125; &#125; &#125; return $result; &#125; public function bd_php1($tmp_array, $start = &apos;&apos;) &#123; $tmp = array(); if ($start == null) &#123; $start = array_shift(array_values($tmp_array)); &#125; $ret = $this-&gt;php1($tmp_array, $start); $tmp[$start] = $ret; return $tmp; &#125;&#125; 算法说明：主要使用 array_shift(),array_values(),array_keys() 函数和递归思想。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[抽象类和接口的区别]]></title>
    <url>%2F2017%2F06%2F10%2Fabstractinterface%2F</url>
    <content type="text"><![CDATA[抽象类与接口相似，都是一种比较特殊的类。抽象类是一种特殊的类，而接口也是一种特殊的抽象类。它们通常配合面向对象的多态性一起使用。虽然声明和使用都比较容易，但它们的作用在理解上会困难一点。 抽象类在OOP语言中，一个类可以有一个或多个子类，而每个类都有至少一个公有方法作为外部代码访问它的接口。而抽象方法就是为了方便继承而引入的。本节中先来介绍一下抽象类和抽象方法的声明，然后再说明其用途。在声明抽象类之前，我们先了解一下什么是抽象方法。抽象方法就是没有方法体的方法，所谓没有方法体是指在方法声明时没有花括号及其中的内容，而是在声明方法时直接在方法名后加上分号结束。另外在声明抽象方法时，还要使用关键字 abstract 来修饰。声明抽象方法的格式如下所示： abstract function fun1(); //不能有花括号，就更不能有方法体中的内容了abstract function fun2(); //直接在方法名的括号后面加上分号结束，还要使用 abstract 修饰 只要在声明类时有一个方法时抽象方法，那么这个类就是抽象类，抽象类也要使用 abstract 关键字来修饰。在抽象类中可以有不是抽象的成员方法和成员属性，但访问权限不能使用private关键字修饰为私有的。下面的例子在Person类中声明了两个抽象方法 say() 和 eat() ，则 Person 类就是一个抽象类，需要使用 abstract 标识。代码如下所示： 123456789101112131415161718&lt;?php abstract class Person&#123; protected $name; protected $country; function __construct($name=&quot;&quot;,$country=&quot;china&quot;)&#123; $this -&gt;name = $name; $this -&gt;country = $country; &#125; abstract function say(); abstract function eat(); function run()&#123; echo &quot;使用两条腿走路&lt;br&gt;&quot;; &#125; &#125;?&gt; 在上例中声明了一个抽象类 Person ，在这个类中定义了两个成员属性、一个构造方法和两个抽象方法，还有一个是非抽象的方法。抽象类就像是一个“半成品”的类，在抽象类中有没有被实现的抽象方法，所以抽象类是不能被实例化的，即创建不了对象，也就不能直接使用它。既然抽象类是一个“半成品”的类，那么使用抽象类有什么作用呢？使用抽象类就包含了继承关系，它是为它的子类定义公共接口，将它的操作（可能是部分，也可能是全部）交给子类去实现。就是将抽象类作为子类重载的模板使用，定义抽象类就相当于定义了一种规范，这种规范要求子类去遵守。当子类继承抽象类以后，就必须把抽象类中的抽象方法按照子类自己的需要去实现。子类必须把父类中的抽象方法全部都实现，否则子类中还存在抽象方法，所以还是抽象类，也不能实例化对象。在下例中声明了两个类，分别实现上例中声明的抽象类 Person 。代码如下所示： 1234567891011121314151617181920212223242526272829303132&lt;?php class ChineseMan extends Person&#123; function say()&#123; echo $this -&gt;name.&quot;是&quot;.$this-&gt;country.&quot;人，讲汉语&lt;br&gt;&quot;; &#125; function eat()&#123; echo $this -&gt;name.&quot;使用筷子吃饭&lt;br&gt;&quot;; &#125; &#125; class Americans extends Person&#123; function say()&#123; echo $this -&gt;name.&quot;是&quot;.$this-&gt;country.&quot;人，讲英语&lt;br&gt;&quot;; &#125; function eat()&#123; echo $this -&gt;name.&quot;使用刀子和叉子吃饭&lt;br&gt;&quot;; &#125; &#125; $chianeseMan = new ChineseMan(&quot;高洛峰&quot;，&quot;中国&quot;); $americans = new Americans (&quot;alex&quot;,&quot;美国&quot;); $chineseMan -&gt;say(); $chineseMan -&gt;eat(); $americans -&gt;say(); $americans -&gt;eat();?&gt; 在上例中声明了两个类去继承抽象类 Person ，并将 Person 类中的抽象方法按各自的需求分别实现，这样两个子类就都可以创建对象了。抽象类 Person 就可以看成是一个模板，类中的抽象方法自己不去实现，只是规范了子类中必须要有父类中声明的抽象方法，而且要按照自己的特点实现抽象方法的内容。 接口因为 PHP 只支持单继承，也就是说每个类只能继承一个父类。当声明的新类继承抽象类实现模板以后，它就不能再有其他父类了。为了解决这个问题， PHP 引入了接口。接口是一种特殊的抽象类，而抽象类又是一种特殊的类。如果抽象类中的所有方法都是抽象方法，我们就可以换另外一种声明方式，使用“接口”技术。接口中声明的方法必须都是抽象方法，另外不能再接口中声明变量，只能使用 const 关键字声明为常量的成员属性，而且接口中所有成员都必须有 public 的访问权限。类的声明是使用 class 关键字标识的，而接口的声明则是使用 interface 关键字标识的。声明接口的格式如下所示： 123456&lt;?phpinterface 接口名称&#123; //使用interface关键字声明接口常量成员 //接口中的成员属性只能是常量，不能是变量抽象方法 //接口中的所有方法必须是抽象方法，不能有非抽象的方法存在&#125;?&gt; 接口中的所有方法都要求是抽象方法，所以就不需要在方法前使用 abstract 关键字标识了。而且在接口中也不需要显式地使用 public 访问权限进行修饰，因为默认权限就是 public 的，也只能是公有的。另外接口和抽象类一样也不能实例化对象，它是一种更严格的规范，也需要通过子类来实现。但可以直接使用接口名称在接口外面去获取常量成员的值。一个接口的声明例子，代码如下所示： 1234567&lt;?phpinterface one&#123;const CONSTANT = &apos;CONSTANT value&apos;;function fun1();function fun2();&#125;?&gt; 也可以使用 extends 关键字让一个接口去继承另一个接口，实现接口之间的扩展。在下面的例子中声明一个 Two 接口继承了上例中的 One 接口。代码如下所示： 123456&lt;?phpinterface Two extends one&#123;function fun3();function fun4();&#125;?&gt; 如果需要使用接口中的成员，则需要通过子类去实现接口中的全部抽象方法，然后创建子类的对象去调用在子类中实现后的方法。但通过类去继承接口时需要使用 implements 关键字来实现，而并不是使用 extends 关键字完成。如果需要使用抽象类去实现接口中的部分方法，也需要使用 implements 关键字实现。在下面的例子中声明一个抽象类 Three 去实现 One 接口中的部分方法，但要想实例化对象，这个抽象类还需要有子类把它所有的抽象方法都实现才行。声明一个 Four 类去实现 One 接口中全部方法。代码如下所示： 1234567891011121314151617181920212223242526&lt;?phpinterface one&#123;const CONSTANT = &apos;CONSTANT value&apos;;function fun1();function fun2();&#125; //声明一个抽象类去实现接口One中的第二个方法abstract class Three implements One&#123; //只实现接口中的一个抽象方法function fun2()&#123;//具体的实现内容由子类自己决定&#125;&#125; //声明一个类实现接口One中的全部抽象方法class Four implements One&#123;function fun1()&#123;//具体的实现内容由子类自己决定&#125; function fun2()&#123;//具体的实现内容由子类自己决定&#125;&#125; ?&gt; PHP 是单继承的，一个类只能有一个父类，但是一个类可以实现多个接口。将要实现的多个接口之间使用逗号分隔开，而且在子类中药将所有接口中的抽象方法全部实现才可以创建对象。就相当于一个类要遵守多个规范，就像我们不仅要遵守国家的法律，如果是在学校，还需要遵守学校的校规一样。实现多个接口的格式如下所示： 123class 类名 implements 接口一，接口二，……接口n&#123;实现所有接口中的抽象方法&#125; 实现多个接口是使用 implements 关键字，同时还可以使用 extends 关键字继承一个类。即在继承类的同时实现多个接口，但一定更要先使用extends继承一个类，再去使用 inplements 实现多个接口。使用格式如下所示： 123class 类名 extends 父类名 implements 接口一，接口二，……接口n&#123;实现所有接口中的抽象方法&#125; 除了上述的一些应用外，还有很多地方可以使用使用接口，例如对于一些已经开发好的系统，在结构上进行较大的调整已经不太现实，这时可以通过自定义一些接口并追加相应的实现来完成功能结构的扩展。 区别 对接口的使用方式是通过关键字 implements 来实现的，而对于抽象类的操作是使用类继承的关键字 extends 实现的，使用时要特别注意。 接口没有数据成员，但是抽象类有数据成员，抽象类可以实现数据的封装。 接口没有构造函数，抽象类可以有构造函数。 接口中的方法都是 public 类型，而抽象类中的方法可以使用 private 、 protected 或 public 来修饰。 一个类可以同时实现多个接口，但是只能实现一个抽象类。 如何选择 如果要创建一个模型，这个模型将由一些紧密相关的对象采用，就可以使用抽象类。如果要创建将由一些不相关对象采用的功能，就使用接口。 如果必须从多个来源继承行为，就使用接口。 如果知道所有类都会共享一个公共的行为实现，就使用抽象类，并在其中实现该行为。 参考文章： PHP的抽象类和接口 PHP的抽象类、接口的区别和选择 PHP高级——抽象类与接口的区别]]></content>
      <categories>
        <category>基本概念</category>
      </categories>
      <tags>
        <tag>抽象类</tag>
        <tag>接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析FCGI与PHP-FPM]]></title>
    <url>%2F2017%2F06%2F07%2Fcgifcgiphpfpm%2F</url>
    <content type="text"><![CDATA[基本概念CGI通用网关接口（Common Gateway Interface/CGI）是一种重要的互联网技术，可以让一个客户端，从网页浏览器向执行在网络服务器上的程序请求数据。CGI 描述了服务器和请求处理程序之间传输数据的一种标准。 FCGI快速通用网关接口（Fast Common Gateway Interface/FastCGI）是一种让交互程序与 Web 服务器通信的协议。多数流行的 HTTP server 都支持 FastCGI，包括 Apache、Nginx 和 lighttpd 等，同时，FastCGI 也被许多脚本语言所支持，其中就有 PHP。FastCGI 是早期通用网关接口（CGI）的增强版本。 FastCGI 致力于减少网页服务器与 CGI 程序之间互动的开销，从而使服务器可以同时处理更多的网页请求。 CGI 和 FastCGI 是一种通信协议规范，不是一个实体。 在平时讨论中我们应该用词规范。有些人说 CGI ，其实指的是实现 CGI 协议的程序，我们称之为 CGI 程序；有些人说 FCGI ，其实指的是实现 FCGI 协议的程序，我们称之为 FCGI 程序。 CGI 程序web server（比如说 Nginx）只是内容的分发者。比如，如果请求 /index.html，那么 web server 会去文件系统中找到这个文件，发送给浏览器，这里分发的是静态数据。如果现在请求的是 /index.php，根据配置文件，Nginx 知道这个不是静态文件，需要去找 PHP 解析器来处理，那么他会把这个请求简单处理后交给 PHP 解析器。Nginx 会传哪些数据给 PHP 解析器呢？比如：URL、查询字符串、POST 数据、HTTP header等。CGI 就是规定要传哪些数据、以什么样的格式传递给后方处理这个请求的协议。 当 web server 收到 /index.php 这个请求后，会启动对应的 CGI 程序，这里就是 PHP 的解析器。接下来 PHP 解析器会解析 php.ini 文件，初始化执行环境，然后处理请求，再以规定 CGI 规定的格式返回处理后的结果，退出进程。web server 再把结果返回给浏览器。 CGI 程序运行在独立的进程中，并对每个 Web 请求建立一个进程（每个请求都会解析 php.ini、初始化执行环境、处理请求），这种方法非常容易实现，但效率很差，难以扩展。面对大量请求，进程的大量建立和消亡使操作系统性能大大下降。此外，由于地址空间无法共享，也限制了资源重用。 FCGI 程序FCGI 协议是 CGI 协议的增强版本，是为了提高 CGI 程序的性能。那么CGI 程序的性能问题在哪呢？ 就是”PHP解析器解析php.ini文件，初始化执行环境”这个工作。标准的CGI 程序对每个请求都会执行这些步骤，所以处理每个时间的时间会比较长。 那么 FCGI 程序是怎么做的呢？首先，FCGI 程序会先启一个 master，解析配置文件，初始化执行环境，然后再启动多个worker。当请求过来时，master 会传递给一个 worker，然后立即可以接受下一个请求。这样就避免了重复的劳动，效率自然是高。而且当 worker 不够用时，master 可以根据配置预先启动几个 worker 等着；当然空闲 worker 太多时，也会停掉一些，这样就提高了性能，也节约了资源。这就是 FCGI 程序的对进程的管理。 PHP-CGI 与 PHP-FPMPHP-CGI 和 PHP-FPM 都是实现 FastCGI 协议 的程序，并且提供了进程管理的功能 PHP-FPM 是一个 PHP 进程管理器，包含 master 进程和 worker 进程两种进程：master 进程只有一个，负责监听端口，接收来自 Web Server 的请求，而 worker 进程则一般有多个 (具体数量根据实际需要配置)，每个进程内部都嵌入了一个 PHP 解释器，是 PHP 代码真正执行的地方。 PHP-CGI 的问题在于 ： PHP-CGI 变更 php.ini 配置后需重启 PHP-CGI 才能让新的 php-ini 生效，不可以平滑重启 直接杀死 PHP-CGI 进程，PHP 就不能运行了。(PHP-FPM和Spawn-FCGI就没有这个问题，守护进程会平滑从新生成新的子进程。） 针对 PHP-CGI 的不足，PHP-FPM（PHP-Fast CGI Process Manager） 应运而生。PHP-FPM 的管理对象是 PHP-CGI。使用 PHP-FPM 来控制 PHP-CGI 的 FastCGI 进程。 FCGI运行模式分析FastCGI 的工作原理是： Web Server 启动时载入 FastCGI 进程管理器（PHP-FPM）； FastCGI 进程管理器自身初始化，启动多个 CGI 解释器进程 (在任务管理器中可见多个 php-cgi.exe )并等待来自 Web Server 的连接； 当客户端请求到达 Web Server 时，FastCGI 进程管理器选择并连接到一个 CGI 解释器。Web server 将 CGI 环境变量和标准输入发送到 FastCGI 子进程 php-cgi.exe； FastCGI 子进程完成处理后将标准输出和错误信息从同一连接返回 Web Server。当 FastCGI 子进程关闭连接时，请求便告处理完成。FastCGI 子进程接着等待并处理来自 FastCGI 进程管理器（运行在 WebServer中）的下一个连接。 在正常的CGI模式中，php-cgi.exe 在此便退出了。 如果使用正常的 CGI 协议，你可以想象请求处理有多慢。每一个 Web 请求 PHP 都必须重新解析 php.ini、重新载入全部 dll 扩展并重初始化全部数据结构。使用 FastCGI，所有这些 都只在进程启动时发生一次。一个额外的好处是，持续数据库连接可以工作。 FastCGI 的主要优点是把动态语言和 HTTP Server 分离开来，所以 Nginx 与 PHP/PHP-FPM 经常被部署在不同的服务器上，以分担前端 Nginx 服务器的压力，使 Nginx 专一处理静 态请求和转发动态请求，而 PHP/PHP-FPM 服务器专一解析PHP动态请求。 给张图 图中表示了 Nginx、FCGI、Wrapper、PHP 之间的关系。其中 fastcgi_pass 指定了 FastCGI 服务器监听端口与地址，Wrapper 是调用其他程序的一段辅助程序，整个的执行流程是： 用户将 HTTP 请求发送给 Nginx 服务器； Nginx 会根据用户访问的 URL 和后缀对请求进行判断，如果请求的是静态资源（HTML页面）会直接返回，如果请求的是动态资源，执行下一步； Nginx 会将请求交给 fastcgi 客户端，通过 fastcgi_pass 将用户的请求发送给 PHP-FPM； fastcgi_pass 将动态资源交给 PHP-FPM 后，PHP-FPM 会将资源转给 PHP 脚本解析服务器的 Wrapper； Wrapper 收到 PHP-FPM 转过来的请求后，Wrapper 会生成一个新的线程调用 PHP 动态程序解析服务器； PHP 会将查询到的结果返回给 Nginx ； Nginx 构造一个响应报文将结果返回给用户。 参考文章： PHP-FastCGI详解 搞不清FastCgi与PHP-fpm之间是个什么样的关系 关于CGI 和 PHP-FPM需要弄清的 理解CGI、FCGI、php-cgi、php-fpm的概念 Nginx–&gt;进阶–&gt;原理–&gt;Nginx+php+fastcgi的原理与关系 Nginx+Php-fpm运行原理详解]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>CGI</tag>
        <tag>FCGI</tag>
        <tag>PHP-FPM</tag>
        <tag>PHP-CGI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跨服务器Session共享]]></title>
    <url>%2F2017%2F06%2F07%2Fsharesession%2F</url>
    <content type="text"><![CDATA[默认情况下，PHP 的 session 文件是保存在磁盘文件中。在php.ini配置文件中的配置项如下： 12session.save_handler = filessession.save_path = &quot;N;/path&quot; 第一个配置项是指定使用 files(文件形式) 存储session数据。 第二个参数指定保存的路径。N 表示生成多少级目录(不放到一个目录下，分散到多个磁盘目录中去) 所以我们可以知道，session 的数据默认是保存在磁盘文件中。 问题背景伴随网站业务规模和访问量的逐步发展，原本由单台服务器、单个域名的迷你网站架构已经无法满足发展需要。 此时我们可能会购买更多服务器，并且启用多个二级子域名以频道化的方式，根据业务功能将网站分布部署在独立的服务器上；或通过负载均衡技术（如：DNS轮询、Radware、F5、LVS等）让多个频道共享一组服务器。 OK，头脑中我们已经构思了这样的解决方案，不过进入深入开发后新的技术问题又随之而来： 我们把网站程序分布部署到多台服务器上，而且独立为几个二级域名，由于 session 受实现原理的局限（PHP中session默认以文件的形式保存在本地服务器的硬盘），使得我们的网站用户不得不经常在几个频道间来回输入用户名、密码登入，导致用户体验大打折扣；另外，原本程序可以直接从用户 session 变量中读取的资料（如：昵称、积分、登入时间等），因为无法跨服务器同步更新 session 变量，迫使开发人员必须实时读写数据库，从而增加了数据库的负担。 于是，解决网站跨服务器之间的Session共享方案需求变得迫切起来。 解决方案基于 Cookie 的 Session 共享这个方案我们可能比较陌生，但它在大型网站中还是比较普遍被使用。原理是将全站用户的 session 信息加密、序列化后以cookie的方式，统一种植在根域名下（如：.host.com），利用浏览器访问该根域名下的所有二级域名站点时，会传递与之域名对应的所有 cookie 内容的特性，从而实现用户的 cookie 化 session 在多服务间的共享访问。 这个方案的优点无需额外的服务器资源；缺点是由于受 HTTP 协议头信心长度的限制，仅能够存储小部分的用户信息，同时cookie 化的 session 内容需要进行安全加解密（如：采用DES、RSA等进行明文加解密；再由MD5、SHA-1等算法进行防伪认证），另外它也会占用一定的带宽资源，因为浏览器会在请求当前域名下任何资源时将本地 cookie 附加在 HTTP 头中传递到服务器。 基于数据库的 Session 共享采用一台 MySQL 服务器做共享服务器，把所有的 session 的数据保存到 MySQL 服务器上，所有 web服务器都来这台MySQL 服务器来获取 session 数据，并且建议使用内存表Heap，提高session操作的读写效率。这个方案的实用性比较强，相信大家普遍在使用，它的缺点在于 session 的并发读写能力取决于 MySQL 数据库的性能，同时需要自己实现 session 淘汰逻辑，以便定时从数据表中更新、删除 session 记录，当并发过高时容易出现表锁，虽然我们可以选择行级锁的表引擎，但不得不否认使用数据库存储 session 还是有些杀鸡用牛刀的架势。 基于 NFS 的 Session 共享这种方法和使用数据库类似，采用一台公共的 NFS 服务器做共享服务器，所有的 web服务器都把 session 数据写到共享存储介质上，也都要来这台服务器获取 session 数据，通过这样的方式来实现 session 数据的共享。这个方案实现最为简单，无需做过多的二次开发，仅需将共享目录服务器 mount 到各频道服务器的本地 session 目录即可，缺点是 NFS 依托于复杂的安全机制和文件系统，因此并发效率不高，尤其对于 session 这类高并发读写的小文件，会由于共享目录服务器的 io-wait过高，最终拖累前端 web 应用程序的执行效率。 基于缓存数据库的 Session 共享这种方式可能是目前互联网中比较流行的一种用法。所有 web服务器都把 session 写入到 MemCache/Redis ，也都从MemCache/Redis 来获取。MemCache/Redis 本身就是一个分布式缓存，便于扩展。网络开销较小，几乎没有IO。性能也更好。缺点，受制于 MemCache/Redis 的容量，如果用户量突然增多， cache 由于容量的限制会将一些数据挤出缓存，另外 MemCache/Redis 故障或重启 session 会完全丢失掉。 利用组播实现 Session 共享通过组播的方式进行集群间的共享，比如 tomcat 目前就具备这样的功能，优点是 web 容器自身支持，配置简单，适合小型网站。缺点是当一台机器的上的 session 变更后会将变更的数据以组播的形式分发给集群间的所有节点，对网络和所有的web 容器都是存在开销。集群越大浪费越严重。不能做到线性的扩展。 参考文章： 如何在多台web服务器上共享session？ 多服务器之间Session共享 如何解决 cluster 中应用中 session 共享问题？都有那些方案，各有什么优缺点？]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
        <tag>session共享</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-top]]></title>
    <url>%2F2017%2F06%2F07%2Flinuxtop%2F</url>
    <content type="text"><![CDATA[简介top 命令是 Linux 下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。 top 显示系统当前的进程和其他状况，是一个动态显示过程，即可以通过用户按键来不断刷新当前状态。如果在前台执行该命令，它将独占前台，直到用户终止该程序为止。比较准确的说，top命令提供了实时的对系统处理器的状态监视。它将显示系统中CPU最“敏感”的任务列表。该命令可以按CPU使用、内存使用、执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定。 参数含义 统计信息区前五行是系统整体的统计信息。 第一行是任务队列信息，同 uptime 命令的执行结果。其内容如下： 参数 意义 09:51:29 当前时间 up 423 days 已经运行天数 15:28 系统运行时间，格式为时:分 5 users 当前登录用户数 load average: 0.09, 0.04, 0.05 系统负载，即任务队列的平均长度。三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。 第二行是进程信息。其内容如下： 参数 意义 143 total 进程总数 2 running 正在运行的进程数 138 sleeping 睡眠的进程数 0 stopped 停止的进程数 3 zombie 僵尸进程数 第三行是CPU信息。其内容如下： 参数 意义 1.0 us 用户空间占用CPU百分比 0.3 sy 内核空间占用CPU百分比 0.0 ni 用户进程空间内改变过优先级的进程占用CPU百分比 97.7 id 空闲CPU百分比 1.0 wa 等待输入输出的CPU时间百分比 0.0 hi 硬件CPU中断占用百分比 0.0 si 软中断占用百分比 0.0 st 虚拟机占用百分比 最后两行是内存信息。其内容如下： 参数 意义 KiB Mem： 1016656 total 物理内存总量 936036 used 使用的物理内存总量 80620 free 空闲内存总量 77144 buffers 用作内核缓存的内存量 KiB Swap： 0 total 交换区总量 0 used 使用的交换区总量 0 free 空闲交换区总量 178128 cached Mem 缓冲的交换区总量，内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖，该数值即为这些内容已存在于内存中的交换区的大小,相应的内存再次被换出时可不必再对交换区写入。 进程信息区统计信息区域的下方显示了各个进程的详细信息。首先来认识一下各列的含义。 序号 列名 含义 1 PID 进程id 2 PPID 父进程id 3 RUSER 真正的用户名 4 UID 进程所有者的用户id 5 USER 进程所有者的用户名 6 GROUP 进程所有者的组名 7 TTY 启动进程的终端名。不是从终端启动的进程则显示为 ? 8 PR 优先级 9 NI nice值。负值表示高优先级，正值表示低优先级 10 P 最后使用的CPU，仅在多CPU环境下有意义 11 %CPU 上次更新到现在的CPU时间占用百分比 12 TIME 进程使用的CPU时间总计，单位秒 13 TIME+ 进程使用的CPU时间总计，单位1/100秒 14 %MEM 进程使用的物理内存百分比 15 VIRT 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES 16 SWAP 进程使用的虚拟内存中，被换出的大小，单位kb。 17 RES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA 18 CODE 可执行代码占用的物理内存大小，单位kb 19 DATA 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb 20 SHR 共享内存大小，单位kb 21 nFLT 页面错误次数 22 nDRT 最后一次写入到现在，被修改过的页面数。 23 S 进程状态(D=不可中断的睡眠状态,R=运行,S=睡眠,T=跟踪/停止,Z=僵尸进程) 24 COMMAND 命令名/命令行 25 WCHAN 若该进程在睡眠，则显示睡眠中的系统函数名 26 Flags 任务标志，参考 sched.h 默认情况下仅显示比较重要的 PID、USER、PR、NI、VIRT、RES、SHR、S、%CPU、%MEM、TIME+、COMMAND 列。 可以通过下面的快捷键来更改显示内容。 更改显示内容通过 f 键可以选择显示的内容。按 f 键之后会显示列的列表，按 a-z 即可显示或隐藏对应的列，最后按回车键确定。 按 o 键可以改变列的显示顺序。按小写的 a-z 可以将相应的列向右移动，而大写的 A-Z 可以将相应的列向左移动。最后按回车键确定。 按大写的 F 或 O 键，然后按 a-z 可以将进程按照相应的列进行排序。而大写的 R 键可以将当前的排序倒转。 命令使用top 使用格式 top [-] [d] [q] [c] [S] [s] [i] [n] [b] 参数说明 参数 说明 d 改变显示的更新速度，或是在交互式指令列( interactive command)按 s q 没有任何延迟的显示速度，如果使用者是有 superuser 的权限，则 top 将会以最高的优先序执行 c 切换显示模式，共有两种模式，一是只显示执行档的名称，另一种是显示完整的路径与名称S : 累积模式，会将己完成或消失的子行程 ( dead child process ) 的 CPU time 累积起来 s 安全模式，将交互式指令取消, 避免潜在的危机 i 不显示任何闲置 (idle) 或无用 (zombie) 的行程 n 更新的次数，完成后将会退出 top b 批次档模式，搭配 “n” 参数一起使用，可以用来将 top 的结果输出到档案内 交互命令 命令 说明 h / ? 显示帮助画面，给出一些简短的命令总结说明 k 终止一个进程。系统将提示用户输入需要终止的进程PID，以及需要发送给该进程什么样的信号。一般的终止进程可以使用15信号；如果不能正常结束那就使用信号9强制结束该进程。默认值是信号15。在安全模式中此命令被屏蔽 i 忽略闲置和僵死进程。这是一个开关式命令 q 退出程序 r 重新安排一个进程的优先级别。系统提示用户输入需要改变的进程PID以及需要设置的进程优先级值。输入一个正值将使优先级降低，反之则可以使该进程拥有更高的优先权。默认值是10 S 切换到累计模式 s 改变两次刷新之间的延迟时间。系统将提示用户输入新的时间，单位为s。如果有小数，就换算成m s。输入0值则系统将不断刷新，默认值是5 s。需要注意的是如果设置太小的时间，很可能会引起不断刷新，从而根本来不及看清显示的情况，而且系统负载也会大大增加 f / F 从当前显示中添加或者删除项目 o / O 改变显示项目的顺序 l 切换显示平均负载和启动时间信息 m 切换显示内存信息 t 切换显示进程和CPU状态信息 c 切换显示命令名称和完整命令行 M 根据驻留内存大小进行排序 P 根据CPU使用百分比大小进行排序 T 根据时间/累计时间进行排序 W 将当前设置写入~/.toprc文件中。这是写top配置文件的推荐方法 常用操作 命令 说明 top 每隔5秒显式所有进程的资源占用情况 top -d 2 每隔2秒显式所有进程的资源占用情况 top -c 每隔5秒显式进程的资源占用情况，并显示进程的命令行参数(默认只有进程名) top -p 12345 -p 6789 每隔5秒显示pid是12345和pid是6789的两个进程的资源占用情况 top -d 2 -c -p 123456 每隔2秒显示pid是12345的进程的资源使用情况，并显示该进程启动的命令行参数 Load Average平均负载 (load average) 是指系统的运行队列的平均利用率，也可以认为是可运行进程的平均数。通过系统命令”w” 或”top”可以查看当前 load average 情况。 第一位：表示最近1分钟平均负载；第二位：表示最近5分钟平均负载；第三位：表示最近15分钟平均负载。 假设我们的系统是单CPU单内核的，把它比喻成是一条单向马路，把CPU任务比作汽车。当车不多的时候，load &lt; 1；当车占满整个马路的时候 load = 1；当马路都站满了，而且马路外还堆满了汽车的时候，load &gt; 1。 我们经常会发现服务器Load &gt; 1但是运行仍然不错，那是因为服务器是多核处理器。假设我们服务器CPU是2核，那么将意味我们拥有2条马路，我们的Load = 2时，所有马路都跑满车辆。 0.7 &lt; load &lt; 1：此时是不错的状态，如果进来更多的汽车，你的马路仍然可以应付； load = 1：你的马路即将拥堵，而且没有更多的资源额外的任务，赶紧看看发生了什么吧； load &gt; 5：非常严重拥堵，我们的马路非常繁忙，每辆车都无法很快的运行。 通常我们先看15分钟load，如果load很高，再看1分钟和5分钟负载，查看是否有下降趋势。1分钟负载值 &gt; 1，那么我们不用担心，但是如果15分钟负载都超过1，我们要赶紧看看发生了什么事情。所以我们要根据实际情况查看这三个值。 参考文章： Linux系统中的load average linux 平均负载 load average 的含义 linux的top命令参数详解 Linux top命令]]></content>
      <categories>
        <category>Linux/Unix</category>
      </categories>
      <tags>
        <tag>命令</tag>
        <tag>Linux</tag>
        <tag>top</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transitions-Python状态机库]]></title>
    <url>%2F2017%2F05%2F26%2Ftransitionstranslate%2F</url>
    <content type="text"><![CDATA[由于项目需要，使用到了Python的状态机库 Transitions ，但GitHub上是英文文档，使用过后觉得还不错，遂手工翻译一下，加深理解，水平有限，如有翻译不当之处请Email (kevin920902@gmail.com) 我。 TransitionsPython 实现的轻量级、面向对象状态机，兼容Python 2.7+ 和 Python 3.0+。 安装方法方法一 1pip install transitions 方法二 直接在 GitHub 上clone仓库到本地，然后 1python setup.py install 快速入门俗话说，接触一个新的工具库，100页的API文档、一千字的描述文档或更多的解释说明都不如一个简单的Demo，虽然这个说法我们无法验证真伪，但下面的例子会让你快速了解 Transitions 的基本用法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from transitions import Machineimport randomclass NarcolepticSuperhero(object): # Define some states. Most of the time, narcoleptic superheroes are just like # everyone else. Except for... states = [&apos;asleep&apos;, &apos;hanging out&apos;, &apos;hungry&apos;, &apos;sweaty&apos;, &apos;saving the world&apos;] def __init__(self, name): # No anonymous superheroes on my watch! Every narcoleptic superhero gets # a name. Any name at all. SleepyMan. SlumberGirl. You get the idea. self.name = name # What have we accomplished today? self.kittens_rescued = 0 # Initialize the state machine self.machine = Machine(model=self, states=NarcolepticSuperhero.states, initial=&apos;asleep&apos;) # Add some transitions. We could also define these using a static list of # dictionaries, as we did with states above, and then pass the list to # the Machine initializer as the transitions= argument. # At some point, every superhero must rise and shine. self.machine.add_transition(trigger=&apos;wake_up&apos;, source=&apos;asleep&apos;, dest=&apos;hanging out&apos;) # Superheroes need to keep in shape. self.machine.add_transition(&apos;work_out&apos;, &apos;hanging out&apos;, &apos;hungry&apos;) # Those calories won&apos;t replenish themselves! self.machine.add_transition(&apos;eat&apos;, &apos;hungry&apos;, &apos;hanging out&apos;) # Superheroes are always on call. ALWAYS. But they&apos;re not always # dressed in work-appropriate clothing. self.machine.add_transition(&apos;distress_call&apos;, &apos;*&apos;, &apos;saving the world&apos;, before=&apos;change_into_super_secret_costume&apos;) # When they get off work, they&apos;re all sweaty and disgusting. But before # they do anything else, they have to meticulously log their latest # escapades. Because the legal department says so. self.machine.add_transition(&apos;complete_mission&apos;, &apos;saving the world&apos;, &apos;sweaty&apos;, after=&apos;update_journal&apos;) # Sweat is a disorder that can be remedied with water. # Unless you&apos;ve had a particularly long day, in which case... bed time! self.machine.add_transition(&apos;clean_up&apos;, &apos;sweaty&apos;, &apos;asleep&apos;, conditions=[&apos;is_exhausted&apos;]) self.machine.add_transition(&apos;clean_up&apos;, &apos;sweaty&apos;, &apos;hanging out&apos;) # Our NarcolepticSuperhero can fall asleep at pretty much any time. self.machine.add_transition(&apos;nap&apos;, &apos;*&apos;, &apos;asleep&apos;) def update_journal(self): &quot;&quot;&quot; Dear Diary, today I saved Mr. Whiskers. Again. &quot;&quot;&quot; self.kittens_rescued += 1 def is_exhausted(self): &quot;&quot;&quot; Basically a coin toss. &quot;&quot;&quot; return random.random() &lt; 0.5 def change_into_super_secret_costume(self): print(&quot;Beauty, eh?&quot;) 现在我们已经建立了一个 NarcolepticSuperhero 状态机，来看看具体怎么使用吧 123456789101112131415161718192021222324252627282930313233343536373839404142&gt;&gt;&gt; batman = NarcolepticSuperhero(&quot;Batman&quot;)&gt;&gt;&gt; batman.state&apos;asleep&apos;&gt;&gt;&gt; batman.wake_up()&gt;&gt;&gt; batman.state&apos;hanging out&apos;&gt;&gt;&gt; batman.nap()&gt;&gt;&gt; batman.state&apos;asleep&apos;&gt;&gt;&gt; batman.clean_up()MachineError: &quot;Can&apos;t trigger event clean_up from state asleep!&quot;&gt;&gt;&gt; batman.wake_up()&gt;&gt;&gt; batman.work_out()&gt;&gt;&gt; batman.state&apos;hungry&apos;# Batman still hasn&apos;t done anything useful...&gt;&gt;&gt; batman.kittens_rescued0# We now take you live to the scene of a horrific kitten entreement...&gt;&gt;&gt; batman.distress_call()&apos;Beauty, eh?&apos;&gt;&gt;&gt; batman.state&apos;saving the world&apos;# Back to the crib.&gt;&gt;&gt; batman.complete_mission()&gt;&gt;&gt; batman.state&apos;sweaty&apos;&gt;&gt;&gt; batman.clean_up()&gt;&gt;&gt; batman.state&apos;asleep&apos; # Too tired to shower!# Another productive day, Alfred.&gt;&gt;&gt; batman.kittens_rescued1 详细文档基本初始化方式使状态机正常运行非常简单。 假设你有一个对象 lump ( Matter 类的一个实例) ，并且你想要管理它的状态： 1234class Matter(object): passlump = Matter() 我们将 lump 绑定到状态机上来初始化一个最小状态机 123456from transitions import Machinemachine = Machine(model=lump, states=[&apos;solid&apos;, &apos;liquid&apos;, &apos;gas&apos;, &apos;plasma&apos;], initial=&apos;solid&apos;)# Lump now has state!lump.state&gt;&gt;&gt; &apos;solid&apos; 之所以称之为最小状态机，因为这种状态机在技术上是可操作的，状态机初试状态为 solid ，但因为我们没有增加任何的转移动作(transitions：这应该是个动词，我在这翻译为转移动作)， 所以它实际上什么也没做。 下面我们来定义更多的状态和转移动作 1234567891011121314151617181920212223242526# The statesstates=[&apos;solid&apos;, &apos;liquid&apos;, &apos;gas&apos;, &apos;plasma&apos;]# And some transitions between states. We&apos;re lazy, so we&apos;ll leave out# the inverse phase transitions (freezing, condensation, etc.).transitions = [ &#123; &apos;trigger&apos;: &apos;melt&apos;, &apos;source&apos;: &apos;solid&apos;, &apos;dest&apos;: &apos;liquid&apos; &#125;, &#123; &apos;trigger&apos;: &apos;evaporate&apos;, &apos;source&apos;: &apos;liquid&apos;, &apos;dest&apos;: &apos;gas&apos; &#125;, &#123; &apos;trigger&apos;: &apos;sublimate&apos;, &apos;source&apos;: &apos;solid&apos;, &apos;dest&apos;: &apos;gas&apos; &#125;, &#123; &apos;trigger&apos;: &apos;ionize&apos;, &apos;source&apos;: &apos;gas&apos;, &apos;dest&apos;: &apos;plasma&apos; &#125;]# Initializemachine = Machine(lump, states=states, transitions=transitions, initial=&apos;liquid&apos;)# Now lump maintains state...lump.state&gt;&gt;&gt; &apos;liquid&apos;# And that state can change...lump.evaporate()lump.state&gt;&gt;&gt; &apos;gas&apos;lump.trigger(&apos;ionize&apos;)lump.state&gt;&gt;&gt; &apos;plasma&apos; 注意：我们为 Matter 实例 lump 绑定了 evaporate() ionize() 等方法，每个方法触发相应的状态转换，你不必在任何地方明确定义这些方法，每个转换的名称通过绑定到模型上来传递给状态机初始化函数(lump)。evaporate() ionize()另等方法都是静态来触发状态转移，如果我们想实现动态转移，可以使用trigger 方法。 状态集毫无疑问，状态机的核心是状态集，上述介绍中我们通过传递给 Machine 初始化函数字符串列表来定义有效的模型状态，但是在 Machine 内部，状态以 State 对象的形式存储。 我们可以通过多种方式初始化和跟新状态，比如： 传递一个给定状态名的字符串给 Machine 初始化函数； 直接初始化每个新的 State 对象； 传递具有初始化参数的字典。 以下代码片段说明了实现相同目标的几种方法 123456789101112131415161718# Create a list of 3 states to pass to the Machine# initializer. We can mix types; in this case, we# pass one State, one string, and one dict.states = [ State(name=&apos;solid&apos;), &apos;liquid&apos;, &#123; &apos;name&apos;: &apos;gas&apos;&#125; ]machine = Machine(lump, states)# This alternative example illustrates more explicit# addition of states and state callbacks, but the net# result is identical to the above.machine = Machine(lump)solid = State(&apos;solid&apos;)liquid = State(&apos;liquid&apos;)gas = State(&apos;gas&apos;)machine.add_states([solid, liquid, gas]) 回调方法我们可以给一个状态添加一系列的”进入“和”退出“的回调函数，还可以在初始化期间指定回调，或者稍后再添加回调。 为方便起见，每当新的状态被添加到 Machine 中时，方法 on_enter_«state name» 和 on_exit_«state name» 都是在 Machine 上（而不是 Model 上）上动态创建的，并且允许我们动态地添加新的进入和退出回调函数。 1234567891011121314151617181920212223242526272829# Our old Matter class, now with a couple of new methods we# can trigger when entering or exit states.class Matter(object): def say_hello(self): print(&quot;hello, new state!&quot;) def say_goodbye(self): print(&quot;goodbye, old state!&quot;)lump = Matter()# Same states as above, but now we give StateA an exit callbackstates = [ State(name=&apos;solid&apos;, on_exit=[&apos;say_goodbye&apos;]), &apos;liquid&apos;, &#123; &apos;name&apos;: &apos;gas&apos; &#125; ]machine = Machine(lump, states=states)machine.add_transition(&apos;sublimate&apos;, &apos;solid&apos;, &apos;gas&apos;)# Callbacks can also be added after initialization using# the dynamically added on_enter_ and on_exit_ methods.# Note that the initial call to add the callback is made# on the Machine and not on the model.machine.on_enter_gas(&apos;say_hello&apos;)# Test out the callbacks...machine.set_state(&apos;solid&apos;)lump.sublimate()&gt;&gt;&gt; &apos;goodbye, old state!&apos;&gt;&gt;&gt; &apos;hello, new state!&apos; 注意：首次初始化状态机时， on_enter_«state name» 回调函数是不会触发的。例如，我们定义了一个回调函数 on_enter_A() ，并给状态机初始化状态为 initial=&#39;A&#39; ，on_enter_A() 会在下次进入状态A的时候出发，首次并不会触发。如果我们想在初始化的时候就触发 on_enter_A() 方法，可以创建一个虚拟的初始状态，然后调用 to_A() 方法，而不是 __init__ 方法。 除了在初始化状态机传递回调或动态添加回调，为增加代码的清晰度，还可以在模型类本身中定义回调。例如 1234567class Matter(object): def say_hello(self): print(&quot;hello, new state!&quot;) def say_goodbye(self): print(&quot;goodbye, old state!&quot;) def on_enter_A(self): print(&quot;We&apos;ve just entered state A!&quot;)lump = Matter()machine = Machine(lump, states=[&apos;A&apos;, &apos;B&apos;, &apos;C&apos;]) 现在，任何时候 lump 转移到状态A，类Matter 中的 on_enter_A() 方法总是会触发。 检查状态我们可以随时通过以下方式检查模型的当前状态 检查 .state 属性； 调用 is_«state name»() 方法。 如果要检索当前状态的实际状态对象，可以通过 Machine 实例的 get_state() 方法来实现 12345678lump.state&gt;&gt;&gt; &apos;solid&apos;lump.is_gas()&gt;&gt;&gt; Falselump.is_solid()&gt;&gt;&gt; Truemachine.get_state(lump.state).name&gt;&gt;&gt; &apos;solid&apos; 状态转换上面的一些例子已经说明了状态转移的使用，但是在这里我们将更详细地探讨它们。 与状态一样，每个状态转换在内部用 Transitions 对象来表示，我们可以传递一个字典或字典列表来快速初始化状态转换集合。 如上面我们已经写过的 1234567transitions = [ &#123; &apos;trigger&apos;: &apos;melt&apos;, &apos;source&apos;: &apos;solid&apos;, &apos;dest&apos;: &apos;liquid&apos; &#125;, &#123; &apos;trigger&apos;: &apos;evaporate&apos;, &apos;source&apos;: &apos;liquid&apos;, &apos;dest&apos;: &apos;gas&apos; &#125;, &#123; &apos;trigger&apos;: &apos;sublimate&apos;, &apos;source&apos;: &apos;solid&apos;, &apos;dest&apos;: &apos;gas&apos; &#125;, &#123; &apos;trigger&apos;: &apos;ionize&apos;, &apos;source&apos;: &apos;gas&apos;, &apos;dest&apos;: &apos;plasma&apos; &#125;]machine = Machine(model=Matter(), states=states, transitions=transitions) 使用字典定义转换有利于代码的清晰，但显得比较笨重。比较简单的方法是使用列表定义转换，只需确保每个列表中的元素与 Transition 初始化中的位置参数的顺序相同(比如 trigger, source, destination 等) 我们可以这样定义转换 123456transitions = [ [&apos;melt&apos;, &apos;solid&apos;, &apos;liquid&apos;], [&apos;evaporate&apos;, &apos;liquid&apos;, &apos;gas&apos;], [&apos;sublimate&apos;, &apos;solid&apos;, &apos;gas&apos;], [&apos;ionize&apos;, &apos;gas&apos;, &apos;plasma&apos;]] 另外，我们也可以在状态机初始化之后添加转换 12machine = Machine(model=lump, states=states, initial=&apos;solid&apos;)machine.add_transition(&apos;melt&apos;, source=&apos;solid&apos;, dest=&apos;liquid&apos;) trigger 参数定义了绑定到模型上的触发方法的名称，当回调这个方法的时候，状态机会执行相应的状态转换。 123&gt;&gt;&gt; lump.melt()&gt;&gt;&gt; lump.state&apos;liquid&apos; 当我们调用非法的转换时，Machine 会抛出异常 1234&gt;&gt;&gt; lump.to_gas()&gt;&gt;&gt; # This won&apos;t work because only objects in a solid state can melt&gt;&gt;&gt; lump.melt()transitions.core.MachineError: &quot;Can&apos;t trigger event melt from state gas!&quot; 这种做法通常是可取的，因为它有助于提醒代码中的问题。但是在某些情况下，我们希望忽略这些非法转换，这时可以设置 ignore_invalid_triggers=True (可以全部忽略，也可忽略特定一条) 12345678&gt;&gt;&gt; # Globally suppress invalid trigger exceptions&gt;&gt;&gt; m = Machine(lump, states, initial=&apos;solid&apos;, ignore_invalid_triggers=True)&gt;&gt;&gt; # ...or suppress for only one group of states&gt;&gt;&gt; states = [&apos;new_state1&apos;, &apos;new_state2&apos;]&gt;&gt;&gt; m.add_states(states, ignore_invalid_triggers=True)&gt;&gt;&gt; # ...or even just for a single state. Here, exceptions will only be suppressed when the current state is A.&gt;&gt;&gt; states = [State(&apos;A&apos;, ignore_invalid_triggers=True), &apos;B&apos;, &apos;C&apos;]&gt;&gt;&gt; m = Machine(lump, states) 如果在做状态转换之前，希望知道在当前状态下，哪些是合法的转换，可以使用 get_triggers() 方法 123456789m.get_triggers(&apos;solid&apos;)&gt;&gt;&gt; [&apos;melt&apos;, &apos;sublimate&apos;]m.get_triggers(&apos;liquid&apos;)&gt;&gt;&gt; [&apos;evaporate&apos;]m.get_triggers(&apos;plasma&apos;)&gt;&gt;&gt; []# you can also query several states at oncem.get_triggers(&apos;solid&apos;, &apos;liquid&apos;, &apos;gas&apos;, &apos;plasma&apos;)&gt;&gt;&gt; [&apos;melt&apos;, &apos;evaporate&apos;, &apos;sublimate&apos;, &apos;ionize&apos;] 自动转换所有状态除了显式添加的任何转换之外，每当将状态添加到 Machine 实例时，都会自动创建一个 to_«state»() 方法，该方法无论状态机当前处于哪种状态，都会转换到目标状态。 123456lump.to_liquid()lump.state&gt;&gt;&gt; &apos;liquid&apos;lump.to_solid()lump.state&gt;&gt;&gt; &apos;solid&apos; 我们可以在状态机初始化的时候设置 auto_transitions=False 禁用这个功能。 多状态转换给定的触发器可以附加到多个转换，其中一些可以在相同的状态下潜在地开始或结束。比如 1234machine.add_transition(&apos;transmogrify&apos;, [&apos;solid&apos;, &apos;liquid&apos;, &apos;gas&apos;], &apos;plasma&apos;)machine.add_transition(&apos;transmogrify&apos;, &apos;plasma&apos;, &apos;solid&apos;)# This next transition will never executemachine.add_transition(&apos;transmogrify&apos;, &apos;plasma&apos;, &apos;gas&apos;) 在这种情况下，如果当前状态是 plasma 调用 transmogrify() ，会转换到 solid 状态，然后继续转换到 plasma 状态。注意：只有第一个匹配的转换将执行，因此，上述最后一行中定义的转换将不会做任何事情。 我们还可以通过使用 * 通配符来引发触发器从所有状态转换到特定状态 1machine.add_transition(&apos;to_liquid&apos;, &apos;*&apos;, &apos;liquid&apos;) 一个反身触发（触发器具有与源和目标相同的状态）可以使用“=”来表示目的状态，比如通过 touch 触发，solid 的目标状态为自身。如果我们希望同一个反身触发器应用到多个状态中，使用“=”是比较方便的。 1machine.add_transition(&apos;touch&apos;, [&apos;liquid&apos;, &apos;gas&apos;, &apos;plasma&apos;], &apos;=&apos;, after=&apos;change_shape&apos;) 通过上面的语句， &#39;liquid&#39;, &#39;gas&#39;, &#39;plasma&#39; 三个状态通过 touch 触发器，可以到达的目的状态分别是 &#39;liquid&#39;, &#39;gas&#39;, &#39;plasma&#39; 。 有序转换比如我们现在有这样一个需求，状态的转换是遵循自定义的序列的，如给定状态 [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]，我们可能需要这样的状态转换 A → B, B → C, C → A （没有其他的非法转换）。为了实现这个功能，Transitions 在 Machine 类中提供了 add_ordered_transitions() 方法 12345678910111213states = [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;] # See the &quot;alternative initialization&quot; section for an explanation of the 1st argument to initmachine = Machine(states=states, initial=&apos;A&apos;)machine.add_ordered_transitions()machine.next_state()print(machine.state)&gt;&gt;&gt; &apos;B&apos;# We can also define a different order of transitionsmachine = Machine(states=states, initial=&apos;A&apos;)machine.add_ordered_transitions([&apos;A&apos;, &apos;C&apos;, &apos;B&apos;])machine.next_state()print(machine.state)&gt;&gt;&gt; &apos;C&apos; 排队转换Transitions中的默认行为是立即处理事件。也就是说，调用绑定在 after 上的回调函数之前， on_enter 方法中的事件会预先处理。 123456789101112131415161718192021222324def go_to_C(): global machine machine.to_C()def after_advance(): print(&quot;I am in state B now!&quot;)def entering_C(): print(&quot;I am in state C now!&quot;)states = [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;]machine = Machine(states=states)# we want a message when state transition to B has been completedmachine.add_transition(&apos;advance&apos;, &apos;A&apos;, &apos;B&apos;, after=after_advance)# call transition from state B to state Cmachine.on_enter_B(go_to_C)# we also want a message when entering state Cmachine.on_enter_C(entering_C)machine.advance()&gt;&gt;&gt; &apos;I am in state C now!&apos;&gt;&gt;&gt; &apos;I am in state B now!&apos; # what? 上述状态机的执行顺序为： 1prepare -&gt; before -&gt; on_enter_B -&gt; on_enter_C -&gt; after. 如果启用排队处理，则在触发下一个转换之前，转换将完成： 12345machine = Machine(states=states, queued=True)...machine.advance()&gt;&gt;&gt; &apos;I am in state B now!&apos;&gt;&gt;&gt; &apos;I am in state C now!&apos; # That&apos;s better! 执行顺序为： 1prepare -&gt; before -&gt; on_enter_B -&gt; queue(to_C) -&gt; after -&gt; on_enter_C. 注意：当使用排队处理事件时，触发器的调用始终返回True，所以在排队执行过程中，无法确定一个包含排队回调函数的转换最终是否能成功，即使是在处理一个事件的时候。 12345678machine.add_transition(&apos;jump&apos;, &apos;A&apos;, &apos;C&apos;, conditions=&apos;will_fail&apos;)...# queued=Falsemachine.jump()&gt;&gt;&gt; False# queued=Truemachine.jump()&gt;&gt;&gt; True 条件转换在某些情况下，我们可能需要在某些条件成立的时候去执行特定的转换，这个时候，conditions 就派上用场了。 1234567# Our Matter class, now with a bunch of methods that return booleans.class Matter(object): def is_flammable(self): return False def is_really_hot(self): return Truemachine.add_transition(&apos;heat&apos;, &apos;solid&apos;, &apos;gas&apos;, conditions=&apos;is_flammable&apos;)machine.add_transition(&apos;heat&apos;, &apos;solid&apos;, &apos;liquid&apos;, conditions=[&apos;is_really_hot&apos;]) 在上面的示例中，如果模型初始化的状态为 solid ，当条件 is_flammable 成立时，调用 heat() 会转换到 gas 状态。同样，如果条件 is_really_hot 成立，调用 heat() 会转换到 liquid 状态。 我们还可能在某些条件不成立的条件下，转换到另一状态，Transitions 为我们提供了 unless 方法。 1machine.add_transition(&apos;heat&apos;, &apos;solid&apos;, &apos;gas&apos;, unless=[&apos;is_flammable&apos;, &apos;is_really_hot&apos;]) 这时，如果 is_flammable(), is_really_hot() 返回的都是 False ，模型调用 heat() 会从 solid 转换到 gas 状态。 注意：条件检查方法会被动接收传递给触发函数的参数或数据对象。比如： 12lump.heat(temp=74)# equivalent to lump.trigger(&apos;heat&apos;, temp=74) 这样会将 temp=74 的可选参数传递给 is_flammable() 方法（以 EventData 实例的方式），参数的传递我们在后面会介绍。 回调方法我们可以将回调方法绑定到状态集和转换集上，每一个转换都包含 before 和 after 属性，这也是我们比较常用的，这两个属性包含了一系列在状态转换前后可以调用的方法。 123456789101112131415class Matter(object): def make_hissing_noises(self): print(&quot;HISSSSSSSSSSSSSSSS&quot;) def disappear(self): print(&quot;where&apos;d all the liquid go?&quot;)transitions = [ &#123; &apos;trigger&apos;: &apos;melt&apos;, &apos;source&apos;: &apos;solid&apos;, &apos;dest&apos;: &apos;liquid&apos;, &apos;before&apos;: &apos;make_hissing_noises&apos;&#125;, &#123; &apos;trigger&apos;: &apos;evaporate&apos;, &apos;source&apos;: &apos;liquid&apos;, &apos;dest&apos;: &apos;gas&apos;, &apos;after&apos;: &apos;disappear&apos; &#125;]lump = Matter()machine = Machine(lump, states, transitions=transitions, initial=&apos;solid&apos;)lump.melt()&gt;&gt;&gt; &quot;HISSSSSSSSSSSSSSSS&quot;lump.evaporate()&gt;&gt;&gt; &quot;where&apos;d all the liquid go?&quot; 在状态转换一开始，在其他转换开始执行之前，我们也可以使用 prepare 回调函数： 123456789101112131415161718192021class Matter(object): heat = False attempts = 0 def count_attempts(self): self.attempts += 1 def is_really_hot(self): return self.heat def heat_up(self): self.heat = random.random() &lt; 0.25 def stats(self): print(&apos;It took you %i attempts to melt the lump!&apos; %self.attempts)states=[&apos;solid&apos;, &apos;liquid&apos;, &apos;gas&apos;, &apos;plasma&apos;]transitions = [ &#123; &apos;trigger&apos;: &apos;melt&apos;, &apos;source&apos;: &apos;solid&apos;, &apos;dest&apos;: &apos;liquid&apos;, &apos;prepare&apos;: [&apos;heat_up&apos;, &apos;count_attempts&apos;], &apos;conditions&apos;: &apos;is_really_hot&apos;, &apos;after&apos;: &apos;stats&apos;&#125;,]lump = Matter()machine = Machine(lump, states, transitions=transitions, initial=&apos;solid&apos;)lump.melt()lump.melt()lump.melt()lump.melt()&gt;&gt;&gt; &quot;It took you 4 attempts to melt the lump!&quot; 注意：除非当前状态是定义在转换集上有效的状态，否则，prepare 回调是不会起作用的。 如果在每一个转换执行之前或之后都需要进行一些默认操作，我们可以在状态机初始化的时候显示使用 before_state_change 和 after_state_change 属性。 1234567891011class Matter(object): def make_hissing_noises(self): print(&quot;HISSSSSSSSSSSSSSSS&quot;) def disappear(self): print(&quot;where&apos;d all the liquid go?&quot;)states=[&apos;solid&apos;, &apos;liquid&apos;, &apos;gas&apos;, &apos;plasma&apos;]lump = Matter()m = Machine(lump, states, before_state_change=&apos;make_hissing_noises&apos;, after_state_change=&apos;disappear&apos;)lump.to_gas()&gt;&gt;&gt; &quot;HISSSSSSSSSSSSSSSS&quot;&gt;&gt;&gt; &quot;where&apos;d all the liquid go?&quot; 还有一些可以独立使用的关键字： prepare_event ：通过 prepare_event 关键字传递给状态机的回调方法，在处理可能发生的状态转换（包括私有的 prepare 回调方法）之前，仅执行一次； finalize_event ：通过 finalize_event 关键字传递给状态机的回调方法，不管转换是否成功，都会被执行； send_event ：如果在状态转换过程中出现了错误，这个错误会绑定到 event_data 上，我们可以使用 send_event=True 来检索错误 。 123456789101112131415161718192021from transitions import Machineclass Matter(object): def raise_error(self, event): raise ValueError(&quot;Oh no&quot;) def prepare(self, event): print(&quot;I am ready!&quot;) def finalize(self, event): print(&quot;Result: &quot;, type(event.error), event.error)states=[&apos;solid&apos;, &apos;liquid&apos;, &apos;gas&apos;, &apos;plasma&apos;]lump = Matter()m = Machine(lump, states, prepare_event=&apos;prepare&apos;, before_state_change=&apos;raise_error&apos;, finalize_event=&apos;finalize&apos;, send_event=True)try: lump.to_gas()except ValueError: passprint(lump.state)&gt;&gt;&gt; I am ready!&gt;&gt;&gt; Result: &lt;class &apos;ValueError&apos;&gt; Oh no&gt;&gt;&gt; initial 执行命令以下是转换集上回调方法可以执行的命令： 回调 状态 备注 machine.prepare_event 源状态 在私有转换执行之前仅执行一次 transition.prepare 源状态 转换一开始就执行 transition.conditions 源状态 可使转换失败或停止 transition.unless 源状态 可使转换失败或停止 machine.before_state_change 源状态 声明在模型上的默认回调方法 transition.before 源状态 state.on_exit 源状态 声明在源状态上的回调方法 &lt;STATE CHANGE&gt; state.on_enter 目的状态 声明在目的状态上的回调方法 transition.after 目的状态 machine.after_state_change 目的状态 声明在模型上的默认回调方法 machine.finalize_event 源状态/目的状态 即使出现错误或异常，回调方法也会执行 传递数据在大多数情况下，我们需要传递给注册在状态机中的回调方法一些参数，来反应当前模型的状态和进行一些必要计算。Transitions 提供了两种方法。 第一种方法（默认方法）： 我们可以直接传递参数给触发器方法。 123456789101112131415161718192021class Matter(object): def __init__(self): self.set_environment() def set_environment(self, temp=0, pressure=101.325): self.temp = temp self.pressure = pressure def print_temperature(self): print(&quot;Current temperature is %d degrees celsius.&quot; % self.temp) def print_pressure(self): print(&quot;Current pressure is %.2f kPa.&quot; % self.pressure)lump = Matter()machine = Machine(lump, [&apos;solid&apos;, &apos;liquid&apos;], initial=&apos;solid&apos;)machine.add_transition(&apos;melt&apos;, &apos;solid&apos;, &apos;liquid&apos;, before=&apos;set_environment&apos;)lump.melt(45) # positional arg;# equivalent to lump.trigger(&apos;melt&apos;, 45)lump.print_temperature()&gt;&gt;&gt; &apos;Current temperature is 45 degrees celsius.&apos;machine.set_state(&apos;solid&apos;) # reset state so we can melt againlump.melt(pressure=300.23) # keyword args also worklump.print_pressure()&gt;&gt;&gt; &apos;Current pressure is 300.23 kPa.&apos; 我们通过这种方法传递任何参数给触发器方法。 但是这种方式有一种局限性：状态转换触发的每一个回调方法都必须处理所有的参数。如果我们的不同回调方法需要不同的参数，这样就会导致一些不必要的麻烦。 为了解决这个问题，Transitions 提供了另一种方法，上文中我们提到过，参数可以以EventData 实例的方式传递。在Machine 初始化的时候设置 send_event=True ，这样，所有传递给非触发器的参数都以EventData 实例的方式传递给回调方法，为了方便我们随时访问与事件相关的源状态、模型、转换、触发等参数，EventData 实例会维护这些参数的内部引用。 12345678910111213141516171819202122class Matter(object): def __init__(self): self.temp = 0 self.pressure = 101.325 # Note that the sole argument is now the EventData instance. # This object stores positional arguments passed to the trigger method in the # .args property, and stores keywords arguments in the .kwargs dictionary. def set_environment(self, event): self.temp = event.kwargs.get(&apos;temp&apos;, 0) self.pressure = event.kwargs.get(&apos;pressure&apos;, 101.325) def print_pressure(self): print(&quot;Current pressure is %.2f kPa.&quot; % self.pressure)lump = Matter()machine = Machine(lump, [&apos;solid&apos;, &apos;liquid&apos;], send_event=True, initial=&apos;solid&apos;)machine.add_transition(&apos;melt&apos;, &apos;solid&apos;, &apos;liquid&apos;, before=&apos;set_environment&apos;)lump.melt(temp=45, pressure=1853.68) # keyword argslump.print_pressure()&gt;&gt;&gt; &apos;Current pressure is 1853.68 kPa.&apos; 其他初始化方式在以上所有的示例中，我们将新的 Machine 实例绑定到一个单独的模型上（Matter 类的实例 lump），虽然这种方式可以使我们的类和回调方法比较整洁，但是我们必须能跟踪到状态机上调用的方法，并且知道模型调用的这些方法绑定了哪个状态机（比如： lump.on_enter_StateA() vs machine.add_transition()） 庆幸的是，Transitions 为我们提供了两种不同的初始化方式。 第一种方式，我们可以创建一个独立的状态机，完全不需要其他模型，在初始化的时候忽略模型参数。 1234machine = Machine(states=states, transitions=transitions, initial=&apos;solid&apos;)machine.melt()machine.state&gt;&gt;&gt; &apos;liquid&apos; 如果我们使用这种方式初始化状态机，我们就可以直接绑定所有的触发事件和回调方法到 Machine 实例上。 这种方法有利于在一个地方整合所有的状态机功能，但如果考虑到状态逻辑应该包含在模型本身中，而不是单独出来，这种方式恐怕也不是最好的。 一种替代的办法是让 Machine 要绑定的模型继承 Machine 类，只要重写 Machine 的 __init__() 方法即可： 123456789101112131415class Matter(Machine): def say_hello(self): print(&quot;hello, new state!&quot;) def say_goodbye(self): print(&quot;goodbye, old state!&quot;) def __init__(self): states = [&apos;solid&apos;, &apos;liquid&apos;, &apos;gas&apos;] Machine.__init__(self, states=states, initial=&apos;solid&apos;) self.add_transition(&apos;melt&apos;, &apos;solid&apos;, &apos;liquid&apos;)lump = Matter()lump.state&gt;&gt;&gt; &apos;solid&apos;lump.melt()lump.state&gt;&gt;&gt; &apos;liquid&apos; 现在我们已经整合所有的状态机功能到 lump 模型中，这比将功能独立出来让人感觉更舒服。 如果我们绑定多个模型到 Machine 中，状态机也是可以处理的。如果想要添加模型和 Machine 实例本身，我们可以在初始化的时候使用 self 关键字（ Machine(model=[&#39;self&#39;, model1, ...])），也可以创建一个单独的状态机，然后通过 machine.add_model 动态注册模型到状态机中，如果模型不再使用的情况下，应该调用 machine.remove_model 来回收模型。 12345678910111213141516171819class Matter(): passlump1 = Matter()lump2 = Matter()machine = Machine(states=states, transitions=transitions, initial=&apos;solid&apos;, add_self=False)machine.add_model(lump1)machine.add_model(lump2, initial=&apos;liquid&apos;)lump1.state&gt;&gt;&gt; &apos;solid&apos;lump2.state&gt;&gt;&gt; &apos;liquid&apos;machine.remove_model([lump1, lump2])del lump1 # lump1 is garbage collecteddel lump2 # lump2 is garbage collected 如果在状态机初始化的时候没有提供初始状态，在添加模型的时候就必须提供这个初始状态。 12345machine = Machine(states=states, transitions=transitions, add_self=False)machine.add_model(Matter())&gt;&gt;&gt; &quot;MachineError: No initial state configured for machine, must specify when adding model.&quot;machine.add_model(Matter(), initial=&apos;liquid&apos;) 日志Transitions 提供一些基本的日志功能，使用 Python 标准的 logging 模块将一些如状态转换、转换触发、条件检查等信息作为 INFO 级别的信息记录下来，我们可以在脚本中轻松地将日志记录配置为标准输出： 12345678# Set up loggingimport loggingfrom transitions import loggerlogger.setLevel(logging.INFO)# Business as usualmachine = Machine(states=states, transitions=transitions, initial=&apos;solid&apos;)... 存储/恢复状态机实例如果想要存储或加载状态机，必须使用Python3.3 或较早版本的 dill 。 123456789101112131415161718import dill as pickle # only required for Python 3.3 and earlierm = Machine(states=[&apos;A&apos;, &apos;B&apos;, &apos;C&apos;], initial=&apos;A&apos;)m.to_B()m.state &gt;&gt;&gt; B# store the machinedump = pickle.dumps(m)# load the Machine instance againm2 = pickle.loads(dump)m2.state&gt;&gt;&gt; Bm2.states.keys()&gt;&gt;&gt; [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;] 以上介绍了 Transitions 的基本使用方法，手工翻译难免有些晦涩难懂之处，敬请谅解。除了这些基本用法之外，Transitions 还提供了一些扩展方法，比如图表可视化机器的当前状态、用于嵌套和重用的分层状态机、并行执行的线程安全锁等，这里暂时就不介绍了，先欠着吧，等以后深入使用 Transitions 库的时候再去学习。 Transitions 英文完整版介绍请移步 Transitions]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Transitions</tag>
        <tag>状态机库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重构改善既有代码的设计-代码的坏味道]]></title>
    <url>%2F2017%2F05%2F20%2Fbadtasteofcode%2F</url>
    <content type="text"><![CDATA[《重构，改善既有代码的设计》一书中，将重构时机用味道来形容，“代码的坏味道”顾名思义就是我们的程序中存在一些不合理、难以理解、难以扩展的设计，本文参考书中的“坏味道条款”列举程序设计中可能遇到的可重构代码。 Alternative Class with Different Interfaces说明：异曲同工的类。如果两个函数做同一件事儿，却有着不同的签名。 拟可重构方法：Rename Method(函数重命名)，Move Method(搬移函数) Comments说明：过多的注释。并不是说不该写注释，但是过多无谓的注释，不如几行简洁的代码，毕竟代码说明了一切。 拟可重构方法：Extract Method(提炼函数)，Introduce Assertion(引入断言) Data Class说明：纯稚的类。某个类拥有一些字段，已经用于访问这些字段的函数。 拟可重构方法：Move Method(搬移函数)，Encapsulate Filed(封装字段)，Encapsulate Collection(封装集合) Data Clumps说明：数据泥团。比如多个类中有相同的字段，几个函数签名中有相同的参数。 拟可重构方法：Extract Class(提炼类)，Introduce Parameter Object(引入参数对象)，Preserve Whole Object(保持对象完整) Divergent Change说明：发散式变化。比如我们在一个类中，如果换一种数据库，需要修改这三个函数，如果出现一种新的解决办法，需要修改那四个函数。 拟可重构方法：Extract Class(提炼类) Duplicated Code说明：重复代码。有重复就会有重构。 拟可重构方法：Extract Method(提炼函数)，Extract Class(提炼类)，Pull Up Method(函数上移)，Form Template Method(塑造模板函数) Feature Envy说明：依赖情结。某个函数对另一个类的依赖高于其所处的类。 拟可重构方法：Move Method(搬移函数)，Move Filed(搬移字段)，Extract Method(提炼函数) Inappropriate Intimacy说明：狎昵关系。如果某一个类过渡访问另一个类的private字段，这就需要将这种关系拆散。 拟可重构方法：Move Method(搬移函数)，Move Filed(搬移字段)，Change Bidirectional Association to Unidirectional(将双向关联改为单向关联)，Replace Inheritance with Delegation(以委托取代继承)，Hide Delegation(隐藏委托关系) Incomplete Library说明：不完美的库类。类库中提供的函数不足以我们的程序设计。 拟可重构方法：Introduce Foreign Method(引入外加函数)，Introduce Local Extension(引入本地扩展) Large Class说明：过大的类。 拟可重构方法：Extract Class(提炼类)，Extract Subclass(提炼子类)，Extract Interface(提炼接口)，Replace Data Value with Object(以对象取代数值) Lazy Class说明：冗赘类。如果某一个类的功能过于简单或者根本没有必要，我们去除这个类可以使代码更加简洁明了。 拟可重构方法：Inline Class(将类内联化)，Collapse Hierarchy(折叠继承关系) Long Method说明：过长的函数。程序越长越难理解，并且不易于维护。 拟可重构方法：Move Method(搬移函数)，Replace Temp with Query(以查询取代临时变量)，Replace Method with Method Object(以函数对象取代函数)，Decompose Conditional(分解条件表达式) Long Parameter List说明：过长参数列。太长的参数列难以理解，可能造成前后不一致，不易使用。 拟可重构方法：Replace Parameter with Method(以函数取代参数)，Introduce Parameter Object(引入参数对象)，Preserve Whole Object(保持对象完整) Message Chains说明：过度耦合的消息链。对象A请求对象B，对象B请求对象C……无穷无尽。 拟可重构方法：Hide Delegate(隐藏委托关系) Middle Man说明：中间人。滥用委托的结果，某个类接口有一半的函数都委托给其他类。 拟可重构方法：Remove Middle Man(移除中间人)，Inline Method(内联函数)，Replace Delegation with Inheritance(以继承取代委托) Parallel Inheritance Hierarchies说明：平行继承体系。为某一个类添加一个子类时，必须也为另一个类增加相应的子类。 拟可重构方法：Move Method(搬移函数)，Move Filed(搬移字段) Primitive Obsession说明：基本类型偏执 。一个很大的类中可能含有许多基本类型数据，或者一个函数参数列表由数个基本类型构成。完全可以用对象来代替这些基本类型。比如结合数值和币种的money类。 拟可重构方法：Replace Data Value with Object(以对象取代数据值)，Extract Class(提炼类)，Introduce Parameter Object(引入参数对象)，Replace Array with Object(以对象取代数组)，Replace Type Code with Class(以类取代类型码)，Replace Type Code with Subclass(以子类取代类型码)，Replace Type Code with State/Strategy(以State/Strategy取代类型码) Refuse Bequest说明：被拒绝的遗赠。子类不想继承父类的函数和数据。 拟可重构方法：Replace Interface with Delegation(以委托取代继承) Shotgun Surgery说明：散弹式修改。我们希望软件发生变化时，只修改程序中的一个地方，而不是各个角落。 拟可重构方法：Move Method(搬移函数)，Move Filed(搬移字段)，Inline Class(将类内联化) Speculative Generality说明：夸夸其谈未来性。我们在项目设计的时候总想着以后会有什么新的需要，所以企图以各种各式各样的钩子和特殊情况处理一些给必要的事情，这就导致了系统更加难以理解和维护。 拟可重构方法：Collapse Hierarchy(折叠继承体系)，Inline Class(将类内联化)，Remove Parameter(移除参数)，Rename Method(重命名函数) Switch Statements说明：switch惊悚现身。switch语句的问题就在于重复，有重复就可以重构。 拟可重构方法：Replace Conditional with Polymorphism(以多态取代条件表达式)，Replace Type Code with Subclass(以子类取代类型码)，Replace Type Code with State/Strategy(以State/Strategy取代类型码)，Replace Parameter with Explicit Methods(以明确函数取代参数)，Introduce Null Object(引入Null对象) Temporary Field说明：令人迷惑的暂时字段。某个类的实例变量只是服务于某种特殊情况，这会给阅读者造成理解障碍。 拟可重构方法：Extract Class(提炼类)，Introduce Null Object(引入Null对象) 参考文章： 《重构—改善既有代码的设计》]]></content>
      <categories>
        <category>重构</category>
      </categories>
      <tags>
        <tag>重构</tag>
        <tag>代码的坏味道</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Laravel学习笔记-Laravel5.3用户登录实现及注册源码初探]]></title>
    <url>%2F2017%2F05%2F05%2Flaraveluserlogin%2F</url>
    <content type="text"><![CDATA[简介在底层代码中，Laravel 的认证组件由 guards 和 providers 组成，Guard 定义了用户在每个请求中如何实现认证，例如，Laravel 通过 session guard来维护 Session 存储的状态、Cookie 以及 token guard，token guard 是认证用户发送请求时带的API token。 Provider 定义了如何从持久化存储中获取用户信息，Laravel 底层支持通过 Eloquent 和数据库查询构建器两种方式来获取用户，如果需要的话，你还可以定义额外的 Provider。 相对于 Laravel5.2 而言，Laravel5.3 在底层代码中做了很多修改，方法更加简洁。虽然代码改了很多，但是原理都是一样的，我们只需要重写不同的方法而已。 默认认证首先我们使用Laravel 5.3提供的开箱即用的认证： 1php artisan make:auth 该Artisan命令会生成用户认证所需的路由、视图以及HomeController 认证的路由也一并生成好了，查看路由文件routes/web.php，会发现该文件已经被更新 123Auth::routes();Route::get(&apos;/home&apos;, &apos;HomeController@index&apos;); 其中Auth::routes()定义了登录注册及找回密码路由，/home 为用户认证成功后跳转的路由。 验证接下来我们先实现前台用户登录，也就是Laravel自带的Users用户表登录。通过生成的默认登录认证，已经写好了所有代码，剩下要做的就是使用迁移命令创建用户认证相关表： 1php artisan migrate 执行命令后会生成 users 表和 password_resets 表，分别为用户表和密码重置表。然后我们就可以在浏览器中输入http://myapp.com/register 来注册新用户 我们创建一个用户，注册成功后直接跳转 /home，并且刚注册的用户名也已经显示出来了 发送邮件重置密码Laravel 已经为我们提供了邮件重置密码的方式，只需要简单的设置即可。 在config/mail.php 中配置邮件发送方 1234&apos;from&apos; =&gt; [ &apos;address&apos; =&gt; &apos;liuyangplus@163.com&apos;, &apos;name&apos; =&gt; &apos;Laravel Reset Password&apos;, ], 在根目录的.env 文件中配置 123456MAIL_DRIVER=smtpMAIL_HOST=smtp.163.comMAIL_PORT=587MAIL_USERNAME=youremail@163.comMAIL_PASSWORD=youremailpasswordMAIL_ENCRYPTION=ssl 注意：如果想使用其他邮箱，最好将&#39;from&#39;中的address和MAIL_HOST MAIL_USERNAME 邮箱类型一致。 以上就是发送邮件重置密码成功的邮件，其实我们也可以通过添加邮件发送类来自定义邮件的主题、内容等，这部分内容以后再添加。 用户认证源码初探我们知道 Laravel 提供了开箱即用的用户认证过程，下面为了深入了解具体的用户认证实现，我们将深入 Laravel 的源码，探究用户验证是怎么处理的。 在实现用户认证过程中，我们首先执行了如下命令 1php artisan make:auth 该命令会在项目里添加以下文件（目录） 12345678910app/Http/Controller/HomeController.PHP resources/views/auth/ resources/views/auth/login.blade.php resources/views/auth/passwords/ resources/views/auth/passwords/email.blade.php resources/views/auth/passwords/reset.blade.php resources/views/auth/register.blade.php resources/views/home.blade.php resources/views/layouts/ resources/views/layouts/app.blade.php 除了一个 HomeController 是处理用户登陆之后的逻辑，其他都是一些视图，用于显示相应的页面。 在 routes/web.php 里添加了以下内容 123Auth::routes();Route::get(&apos;/home&apos;, &apos;HomeController@index&apos;); Auth::routes() 是登陆、注册需要的一些路由；下面是定义一个 /home 路由，交给 HomeController@index 处理。 那么，就从路由开始我们的探究之旅吧。 路由 Auth::routes()定义在 vendor/laravel/framework/src/Illuminate/Support/Facades/Auth.php 1234public static function routes() &#123; static::$app-&gt;make(&apos;router&apos;)-&gt;auth(); &#125; 这里由 IOC 容器解析了一个 Illuminate\Routing\Router 类的实例，再调用里面的 auth() 方法。 我们再来看看 auth() 方法，定义在 vendor/laravel/framework/src/Illuminate/Routing/Router.php 1234567891011121314151617public function auth() &#123; // Authentication Routes... $this-&gt;get(&apos;login&apos;, &apos;Auth\LoginController@showLoginForm&apos;)-&gt;name(&apos;login&apos;); $this-&gt;post(&apos;login&apos;, &apos;Auth\LoginController@login&apos;); $this-&gt;post(&apos;logout&apos;, &apos;Auth\LoginController@logout&apos;); // Registration Routes... $this-&gt;get(&apos;register&apos;, &apos;Auth\RegisterController@showRegistrationForm&apos;); $this-&gt;post(&apos;register&apos;, &apos;Auth\RegisterController@register&apos;); // Password Reset Routes... $this-&gt;get(&apos;password/reset&apos;, &apos;Auth\ForgotPasswordController@showLinkRequestForm&apos;); $this-&gt;post(&apos;password/email&apos;, &apos;Auth\ForgotPasswordController@sendResetLinkEmail&apos;); $this-&gt;get(&apos;password/reset/&#123;token&#125;&apos;, &apos;Auth\ResetPasswordController@showResetForm&apos;); $this-&gt;post(&apos;password/reset&apos;, &apos;Auth\ResetPasswordController@reset&apos;); &#125; 这里定义了 登陆 、 注销 、注册 和密码重置的路由。我们先看注册部分。 注册App\Http\Controllers\Auth\RegisterController 负责注册的逻辑，这里 use 了Illuminate\Foundation\Auth\RegistersUsers 这个 trait ，包含注册时通用的一些逻辑。 路由 get(&#39;/register&#39;) 所绑定的方法 Auth\RegisterController@showRegistrationForm 就定义在这个 trait 里 1234public function showRegistrationForm() &#123; return view(&apos;auth.register&apos;); &#125; 很简单，返回一个 auth.register 视图。 auth.register 视图获取用户的输入： name，email，password，然后 POST 提交到/register。 再来看看路由 post(&#39;/register&#39;) 所绑定的方法 Auth\RegisterController@register。 同样， register 方法定义在 Illuminate\Foundation\Auth\RegistersUsers 里 12345678public function register(Request $request) &#123; $this-&gt;validator($request-&gt;all())-&gt;validate(); $this-&gt;guard()-&gt;login($this-&gt;create($request-&gt;all())); return redirect($this-&gt;redirectPath()); &#125; 首先使用请求传入的表单调用 validator() ，返回一个验证对象，再调用 validate() 验证表单内容的合法性。 validator() 定义在 App\Http\Controllers\Auth\RegisterController 里 12345678protected function validator(array $data) &#123; return Validator::make($data, [ &apos;name&apos; =&gt; &apos;required|max:255&apos;, &apos;email&apos; =&gt; &apos;required|email|max:255|unique:users&apos;, &apos;password&apos; =&gt; &apos;required|min:6|confirmed&apos;, ]); &#125; 在这里给出了输入表单的验证规则，如果我们的用户注册需要的表单与这几个字段不一致（例如需要添加一个手机号），就在这里修改。 返回的 Validator 对象会在 register() 方法里验证。 再回到 register() 方法， 往下走 $this-&gt;guard()-&gt;login($this-&gt;create($request-&gt;all()));。 $this-&gt;guard() 这里会调用 Illuminate\Foundation\Auth\RegistersUsers 里的 guard() 1234protected function guard() &#123; return Auth::guard(); &#125; 这里无参数调用 Auth::guard() 返回一个默认的 guard，看一下 config/auth.php 12345678910111213141516&apos;guards&apos; =&gt; [ &apos;web&apos; =&gt; [ &apos;driver&apos; =&gt; &apos;session&apos;, &apos;provider&apos; =&gt; &apos;users&apos;, ], &apos;admin&apos; =&gt; [ &apos;driver&apos; =&gt; &apos;session&apos;, &apos;provider&apos; =&gt; &apos;admins&apos;, ], &apos;api&apos; =&gt; [ &apos;driver&apos; =&gt; &apos;token&apos;, &apos;provider&apos; =&gt; &apos;users&apos;, ], ], 默认的 guard 是 web ； web 这个 guard 采用 session 驱动， 数据提供者是 users ；users 数据提供者使用 eloquent 驱动， 使用 App\User::class 模型。 接下来调用 guard 的 login($this-&gt;create($request-&gt;all()))。 首先是 $this-&gt;create() ，这个方法定义在 App\Http\Controllers\Auth\RegisterController 里 12345678protected function create(array $data) &#123; return User::create([ &apos;name&apos; =&gt; $data[&apos;name&apos;], &apos;email&apos; =&gt; $data[&apos;email&apos;], &apos;password&apos; =&gt; bcrypt($data[&apos;password&apos;]), ]); &#125; 使用 User 模型对输入的内容新增一条记录，并返回这个模型的对象。 同样，如果需要修改注册时使用的字段，也是改写这个方法。 生成的 User 对象交给 guard 的 login() 方法，做一系列登录的操作，具体怎么做的，还是放到登陆验证里再详细说明。 最后， return redirect($this-&gt;redirectPath()); 完成了注册、登陆的操作，最后跳转到我们在 App\Http\Controllers\Auth\RegisterController 里设置的 protected $redirectTo = &#39;/home&#39;; 目标URL。 可以看一下 $this-&gt;redirectPath() 方法怎么写的，在 Illuminate\Foundation\Auth\RedirectsUsers 这个 trait 里 1234public function redirectPath() &#123; return property_exists($this, &apos;redirectTo&apos;) ? $this-&gt;redirectTo : &apos;/home&apos;; &#125; 如果定义了 $redirectTo 这个属性，就按照这个属性返回；如果没有，返回 &#39;/home&#39;。 这里把这个方法写成 trait 是因为这个方法还会在 App\Http\Controllers\Auth\LoginController 登陆控制器里使用，所以就把 redirectPath() 这个方法提出来做成一个 trait，严格遵守 DRY 原则。 参考文章： Laravel 5.3 用户验证源码探究 （一） 路由与注册 Laravel 5.3 多用户表登录实现]]></content>
      <categories>
        <category>Laravel</category>
      </categories>
      <tags>
        <tag>Laravel</tag>
        <tag>多用户登录</tag>
        <tag>验证原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Laravel学习笔记-Laravel5.3后台用户认证]]></title>
    <url>%2F2017%2F05%2F05%2Flaraveladminlogin%2F</url>
    <content type="text"><![CDATA[简介Laravel 5.3 的 Auth 认证在 5.2 的基础上又有一些改变，本文说明如何在 Laravel 5.3 下做不同用户表的登录认证。 Auth 认证原理简述Laravel 的认证是使用 guard 与 provider 配合完成， guard 负责认证的业务逻辑，认证信息的服务端保存等； provider 负责提供认证信息的持久化数据提供。请求提交给 guard， guard 从 provider 里取出数据（类似用户名、密码等），验证输入数据与服务器端存储的数据是否吻合。如果提交的数据正确，再做 session 等业务的处理（如有需要）。 认证脚手架首先我们导入 Laravel 的自带的认证脚手架 1php artisan make:auth 执行数据库迁移 1php artisan migrate 修改 Auth 认证的配置文件 config/auth.php，在 gurads 处，添加 admin guard 用于后台管理员认证 12345678910111213141516&apos;guards&apos; =&gt; [ &apos;web&apos; =&gt; [ &apos;driver&apos; =&gt; &apos;session&apos;, &apos;provider&apos; =&gt; &apos;users&apos;, ], &apos;admin&apos; =&gt; [ &apos;driver&apos; =&gt; &apos;session&apos;, &apos;provider&apos; =&gt; &apos;admins&apos;, ], &apos;api&apos; =&gt; [ &apos;driver&apos; =&gt; &apos;token&apos;, &apos;provider&apos; =&gt; &apos;users&apos;, ], ], 在 providers 处添加 admins provider，使用 Admin 模型 1234567891011&apos;providers&apos; =&gt; [ &apos;users&apos; =&gt; [ &apos;driver&apos; =&gt; &apos;eloquent&apos;, &apos;model&apos; =&gt; App\Models\User::class, ], &apos;admins&apos; =&gt; [ &apos;driver&apos; =&gt; &apos;eloquent&apos;, &apos;model&apos; =&gt; App\Models\Admin::class, ], ], 注意：本文将所有的 Model 都存放在 App\Models\ 目录下。 创建后台管理员模型我们再创建一个 Admin 模型，用于后台管理员登录验证 1php artisan make:model Admin -m -m 参数会同时生成数据库迁移文件 xxxx_create_admins_table 修改 app/Admin.php 模型文件 1234567891011121314151617181920212223242526272829&lt;?phpnamespace App;use Illuminate\Notifications\Notifiable;use Illuminate\Foundation\Auth\User as Authenticatable;class Admin extends Authenticatable&#123; use Notifiable; /** * The attributes that are mass assignable. * * @var array */ protected $fillable = [ &apos;name&apos;, &apos;password&apos;, ]; /** * The attributes that should be hidden for arrays. * * @var array */ protected $hidden = [ &apos;password&apos;, &apos;remember_token&apos;, ];&#125; 编辑 xxxx_create_admins_table 文件，后台管理员模型结构与前台用户差不多，去掉 email 字段，name 字段设为 unique()，后台用户登录使用用户名、密码的形式，如果需要 email ，也可以不去掉。 12345678910111213141516171819202122232425262728293031323334&lt;?phpuse Illuminate\Support\Facades\Schema;use Illuminate\Database\Schema\Blueprint;use Illuminate\Database\Migrations\Migration;class CreateAdminsTable extends Migration&#123; /** * Run the migrations. * * @return void */ public function up() &#123; Schema::create(&apos;admins&apos;, function (Blueprint $table) &#123; $table-&gt;increments(&apos;id&apos;); $table-&gt;string(&apos;name&apos;)-&gt;unique(); $table-&gt;string(&apos;password&apos;); $table-&gt;rememberToken(); $table-&gt;timestamps(); &#125;); &#125; /** * Reverse the migrations. * * @return void */ public function down() &#123; Schema::dropIfExists(&apos;admins&apos;); &#125;&#125; 管理员模型填充数据定义一个数据模型工厂，在 database/factories/ModelFactory.php 中添加如下代码 123456789$factory-&gt;define(App\Admin::class, function (Faker\Generator $faker) &#123; static $password; return [ &apos;name&apos; =&gt; $faker-&gt;firstName, &apos;password&apos; =&gt; $password ?: $password = bcrypt(&apos;secret&apos;), &apos;remember_token&apos; =&gt; str_random(10), ];&#125;); 使用 Faker 随机填充用户名 在 database/seeds 目录下生成 AdminsTableSeeder.php 文件 1php artisan make:seeder AdminsTableSeeder 编辑 database/seeds/AdminsTableSeeder.php 文件的 run 方法，添加3个管理员用户，密码为 123456 123456public function run() &#123; factory(&apos;App\Admin&apos;, 3)-&gt;create([ &apos;password&apos; =&gt; bcrypt(&apos;123456&apos;) ]); &#125; 在 database/seeds/DatabaseSeeder.php 的 run 方法里调用 AdminsTableSeeder 类 1234public function run() &#123; $this-&gt;call(AdminsTableSeeder::class); &#125; 执行数据库迁移命令 1php artisan migrate --seed 数据库里会创建 admins 表，并且生成了3条数据： id name password remember_token created_at updated_at 1 John $2y$10$AYD4MoW… 9p7bycJ5Wn 2017-05-05 15:12:37 2017-05-05 15:12:37 2 Ransom $2y$10$AYD4MoW… Ct8W5nmTsg 2017-05-05 15:12:37 2017-05-05 15:12:37 3 Dulce $2y$10$AYD4MoW… I8RJpxwVrk 2017-05-05 15:12:37 2017-05-05 15:12:37 创建后台页面创建控制器12php artisan make:controller Admin/LoginController php artisan make:controller Admin/IndexController 其中， Admin/LoginController 负责登录逻辑； Admin/IndexController 管理登录后的首页。 编辑 Admin/LoginController.php 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;?phpnamespace App\Http\Controllers\Admin;use App\Http\Controllers\Controller;use Illuminate\Foundation\Auth\AuthenticatesUsers;class LoginController extends Controller&#123; /* |-------------------------------------------------------------------------- | Login Controller |-------------------------------------------------------------------------- | | This controller handles authenticating users for the application and | redirecting them to your home screen. The controller uses a trait | to conveniently provide its functionality to your applications. | */ use AuthenticatesUsers; /** * Where to redirect users after login / registration. * * @var string */ protected $redirectTo = &apos;/admin&apos;; /** * Create a new controller instance. * * @return void */ public function __construct() &#123; $this-&gt;middleware(&apos;guest.admin&apos;, [&apos;except&apos; =&gt; &apos;logout&apos;]); &#125; /** * 显示后台登录模板 */ public function showLoginForm() &#123; return view(&apos;admin.login&apos;); &#125; /** * 使用 admin guard */ protected function guard() &#123; return auth()-&gt;guard(&apos;admin&apos;); &#125;&#125; 编辑 Admin/IndexController.php 12345678910111213141516171819&lt;?phpnamespace App\Http\Controllers\Admin;use Illuminate\Http\Request;use App\Http\Requests;use App\Http\Controllers\Controller;class IndexController extends Controller&#123; /** * 显示后台管理模板首页 */ public function index() &#123; return view(&apos;admin.index&apos;); &#125;&#125; 后台显示模板复制 views/layouts/app.blade.php 成 views/layouts/admin.blade.php。 编辑后台管理布局模板 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt; &lt;!-- CSRF Token --&gt; &lt;meta name=&quot;csrf-token&quot; content=&quot;&#123;&#123; csrf_token() &#125;&#125;&quot;&gt; &lt;title&gt;&#123;&#123; config(&apos;app.name&apos;, &apos;Laravel&apos;) &#125;&#125; - Admin&lt;/title&gt; &lt;!-- Styles --&gt; &lt;link href=&quot;/css/app.css&quot; rel=&quot;stylesheet&quot;&gt; &lt;!-- Scripts --&gt; &lt;script&gt; window.Laravel = &lt;?php echo json_encode([ &apos;csrfToken&apos; =&gt; csrf_token(), ]); ?&gt; &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;nav class=&quot;navbar navbar-default navbar-static-top&quot;&gt; &lt;div class=&quot;container&quot;&gt; &lt;div class=&quot;navbar-header&quot;&gt; &lt;!-- Collapsed Hamburger --&gt; &lt;button type=&quot;button&quot; class=&quot;navbar-toggle collapsed&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#app-navbar-collapse&quot;&gt; &lt;span class=&quot;sr-only&quot;&gt;Toggle Navigation&lt;/span&gt; &lt;span class=&quot;icon-bar&quot;&gt;&lt;/span&gt; &lt;span class=&quot;icon-bar&quot;&gt;&lt;/span&gt; &lt;span class=&quot;icon-bar&quot;&gt;&lt;/span&gt; &lt;/button&gt; &lt;!-- Branding Image --&gt; &lt;a class=&quot;navbar-brand&quot; href=&quot;&#123;&#123; url(&apos;/&apos;) &#125;&#125;&quot;&gt; &#123;&#123; config(&apos;app.name&apos;, &apos;Laravel&apos;) &#125;&#125; &lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;collapse navbar-collapse&quot; id=&quot;app-navbar-collapse&quot;&gt; &lt;!-- Left Side Of Navbar --&gt; &lt;ul class=&quot;nav navbar-nav&quot;&gt; &amp;nbsp; &lt;/ul&gt; &lt;!-- Right Side Of Navbar --&gt; &lt;ul class=&quot;nav navbar-nav navbar-right&quot;&gt; &lt;!-- Authentication Links --&gt; @if (auth()-&gt;guard(&apos;admin&apos;)-&gt;guest()) &lt;li&gt;&lt;a href=&quot;&#123;&#123; url(&apos;/admin/login&apos;) &#125;&#125;&quot;&gt;Login&lt;/a&gt;&lt;/li&gt; @else &lt;li class=&quot;dropdown&quot;&gt; &lt;a href=&quot;#&quot; class=&quot;dropdown-toggle&quot; data-toggle=&quot;dropdown&quot; role=&quot;button&quot; aria-expanded=&quot;false&quot;&gt; &#123;&#123; auth()-&gt;guard(&apos;admin&apos;)-&gt;user()-&gt;name &#125;&#125; &lt;span class=&quot;caret&quot;&gt;&lt;/span&gt; &lt;/a&gt; &lt;ul class=&quot;dropdown-menu&quot; role=&quot;menu&quot;&gt; &lt;li&gt; &lt;a href=&quot;&#123;&#123; url(&apos;/admin/logout&apos;) &#125;&#125;&quot; onclick=&quot;event.preventDefault(); document.getElementById(&apos;logout-form&apos;).submit();&quot;&gt; Logout &lt;/a&gt; &lt;form id=&quot;logout-form&quot; action=&quot;&#123;&#123; url(&apos;/admin/logout&apos;) &#125;&#125;&quot; method=&quot;POST&quot; style=&quot;display: none;&quot;&gt; &#123;&#123; csrf_field() &#125;&#125; &lt;/form&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; @endif &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/nav&gt; @yield(&apos;content&apos;) &lt;!-- Scripts --&gt; &lt;script src=&quot;/js/app.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 复制 views/auth/login.blade.php 成 views/admin/login.blade.php。 编辑该模板，更改布局文件为 layouts.admin， 把表单的提交 url 改为 admin/login，email 字段改成 name 字段，去掉找回密码的部分 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@extends(&apos;layouts.admin&apos;)@section(&apos;content&apos;)&lt;div class=&quot;container&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-md-8 col-md-offset-2&quot;&gt; &lt;div class=&quot;panel panel-default&quot;&gt; &lt;div class=&quot;panel-heading&quot;&gt;Admin Login&lt;/div&gt; &lt;div class=&quot;panel-body&quot;&gt; &lt;form class=&quot;form-horizontal&quot; role=&quot;form&quot; method=&quot;POST&quot; action=&quot;&#123;&#123; url(&apos;/admin/login&apos;) &#125;&#125;&quot;&gt; &#123;&#123; csrf_field() &#125;&#125; &lt;div class=&quot;form-group&#123;&#123; $errors-&gt;has(&apos;name&apos;) ? &apos; has-error&apos; : &apos;&apos; &#125;&#125;&quot;&gt; &lt;label for=&quot;name&quot; class=&quot;col-md-4 control-label&quot;&gt;Name&lt;/label&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;input id=&quot;name&quot; type=&quot;text&quot; class=&quot;form-control&quot; name=&quot;name&quot; value=&quot;&#123;&#123; old(&apos;name&apos;) &#125;&#125;&quot; required autofocus&gt; @if ($errors-&gt;has(&apos;name&apos;)) &lt;span class=&quot;help-block&quot;&gt; &lt;strong&gt;&#123;&#123; $errors-&gt;first(&apos;name&apos;) &#125;&#125;&lt;/strong&gt; &lt;/span&gt; @endif &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;form-group&#123;&#123; $errors-&gt;has(&apos;password&apos;) ? &apos; has-error&apos; : &apos;&apos; &#125;&#125;&quot;&gt; &lt;label for=&quot;password&quot; class=&quot;col-md-4 control-label&quot;&gt;Password&lt;/label&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;input id=&quot;password&quot; type=&quot;password&quot; class=&quot;form-control&quot; name=&quot;password&quot; required&gt; @if ($errors-&gt;has(&apos;password&apos;)) &lt;span class=&quot;help-block&quot;&gt; &lt;strong&gt;&#123;&#123; $errors-&gt;first(&apos;password&apos;) &#125;&#125;&lt;/strong&gt; &lt;/span&gt; @endif &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;div class=&quot;col-md-6 col-md-offset-4&quot;&gt; &lt;div class=&quot;checkbox&quot;&gt; &lt;label&gt; &lt;input type=&quot;checkbox&quot; name=&quot;remember&quot;&gt; Remember Me &lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;div class=&quot;col-md-8 col-md-offset-4&quot;&gt; &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary&quot;&gt; Login &lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;@endsection 复制 views/home.blade.php 成 views/admins/index.blade.php。 编辑该模板 1234567891011121314151617@extends(&apos;layouts.admin&apos;)@section(&apos;content&apos;)&lt;div class=&quot;container&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-md-8 col-md-offset-2&quot;&gt; &lt;div class=&quot;panel panel-default&quot;&gt; &lt;div class=&quot;panel-heading&quot;&gt;Dashboard&lt;/div&gt; &lt;div class=&quot;panel-body&quot;&gt; You are logged in admin dashboard! &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;@endsection 添加后台路由编辑 routes/web.php， 添加以下内容 123456789Route::group([&apos;prefix&apos; =&gt; &apos;admin&apos;], function () &#123; Route::group([&apos;middleware&apos; =&gt; &apos;auth.admin&apos;], function () &#123; Route::get(&apos;/&apos;, &apos;Admin\IndexController@index&apos;); &#125;); Route::get(&apos;login&apos;, &apos;Admin\LoginController@showLoginForm&apos;)-&gt;name(&apos;admin.login&apos;); Route::post(&apos;login&apos;, &apos;Admin\LoginController@login&apos;); Route::post(&apos;logout&apos;, &apos;Admin\LoginController@logout&apos;);&#125;); 后台管理认证中间件创建后台管理认证中间件 1php artisan make:middleware AuthAdmin 编辑 AuthAdmin 12345678910111213141516171819202122232425262728&lt;?phpnamespace App\Http\Middleware;use Closure;class AuthAdmin&#123; /** * Handle an incoming request. * * @param \Illuminate\Http\Request $request * @param \Closure $next * @return mixed */ public function handle($request, Closure $next) &#123; if (auth()-&gt;guard(&apos;admin&apos;)-&gt;guest()) &#123; if ($request-&gt;ajax() || $request-&gt;wantsJson()) &#123; return response(&apos;Unauthorized.&apos;, 401); &#125; else &#123; return redirect()-&gt;guest(&apos;admin/login&apos;); &#125; &#125; return $next($request); &#125;&#125; 创建后台管理登录跳转中间件，用于有些操作在登录之后的跳转 1php artisan make:middleware GuestAdmin 编辑该中间件的 handle 方法 12345678public function handle($request, Closure $next) &#123; if (auth()-&gt;guard(&apos;admin&apos;)-&gt;check()) &#123; return redirect(&apos;/admin&apos;); &#125; return $next($request); &#125; 在 app/Http/Kernel.php 中注册以上中间件 123456protected $routeMiddleware = [ ...... &apos;auth.admin&apos; =&gt; \App\Http\Middleware\AuthAdmin::class, &apos;guest.admin&apos; =&gt; \App\Http\Middleware\GuestAdmin::class, ......]; 处理注销经过上面的步骤，已经实现了前后台分离登录，但是不管是在前台注销，还是在后台注销，都销毁了所有的 session，导致前后台注销连在一起。所以我们还要对注销的方法处理一下。 原来的 logout 方法是这样写的，在 Illuminate\Foundation\Auth\AuthenticatesUsers 里 12345678910public function logout(Request $request) &#123; $this-&gt;guard()-&gt;logout(); $request-&gt;session()-&gt;flush(); $request-&gt;session()-&gt;regenerate(); return redirect(&apos;/&apos;); &#125; 注意这一句 1$request-&gt;session()-&gt;flush(); 将所有的 session 全部清除，这里不分前台、后台，所以要对这里进行改造。 因为前台、后台注销都要修改，所以我们新建一个 trait，前后台都可以使用。 新建一个文件 app/Extensions/AuthenticatesLogout.php 这里需要新建 Extensions 文件夹，在Laravel5.3中是没有这个文件夹的 12345678910111213141516171819&lt;?phpnamespace App\Extensions;use Illuminate\Http\Request;trait AuthenticatesLogout&#123; public function logout(Request $request) &#123; $this-&gt;guard()-&gt;logout(); $request-&gt;session()-&gt;forget($this-&gt;guard()-&gt;getName()); $request-&gt;session()-&gt;regenerate(); return redirect(&apos;/&apos;); &#125;&#125; 我们将 1$request-&gt;session()-&gt;flush(); 改成 1$request-&gt;session()-&gt;forget($this-&gt;guard()-&gt;getName()); 只是删除掉当前 guard 所创建的 session，这样就达到了分别注销的目的。 修改 Auth/LoginController.php 和 Admin/LoginController.php，将 1use AuthenticatesUsers; 改掉，在文件的前面别忘了加上 use 语句 12345678910use App\Extensions\AuthenticatesLogout;...class LoginController extends Controller&#123; use AuthenticatesUsers, AuthenticatesLogout &#123; AuthenticatesLogout::logout insteadof AuthenticatesUsers; &#125;... 到这里，就完成了整个不同用户表登录认证的过程。 参考文章： Laravel 5.3 不同用户表登录认证]]></content>
      <categories>
        <category>Laravel</category>
      </categories>
      <tags>
        <tag>Laravel</tag>
        <tag>后台用户认证</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Laravel学习笔记-深入理解控制反转(IoC)和依赖注入(DI)]]></title>
    <url>%2F2017%2F04%2F27%2Flaraveliocanddi%2F</url>
    <content type="text"><![CDATA[容器，字面上理解就是装东西的东西。常见的变量、对象属性等都可以算是容器。一个容器能够装什么，全部取决于你对该容器的定义。当然，有这样一种容器，它存放的不是文本、数值，而是对象、对象的描述（类、接口）或者是提供对象的回调，通过这种容器，我们得以实现许多高级的功能，其中最常提到的，就是 “解耦” 、“依赖注入(DI)”。本文就从这里开始。 IoC 容器 - Laravel 的核心Laravel 的核心就是一个 IoC 容器，根据文档，称其为“服务容器”，顾名思义，该容器提供了整个框架中需要的一系列服务。作为初学者，很多人会在这一个概念上犯难，因此，我打算从一些基础的内容开始讲解，通过理解面向对象开发中依赖的产生和解决方法，来逐渐揭开“依赖注入”的面纱，逐渐理解这一神奇的设计理念。 本文一大半内容都是通过举例来让读者去理解什么是 IoC（控制反转） 和 DI（依赖注入），通过理解这些概念，来更加深入。更多关于 Laravel 服务容器的用法建议阅读文档即可。 IoC 容器诞生的故事讲解 IoC 容器有很多的文章，我之前也写过。但现在我打算利用当下的灵感重新来过，那么开始吧。 超人和超能力，依赖的产生面向对象编程，有以下几样东西无时不刻的接触：接口、类还有对象。这其中，接口是类的原型，一个类必须要遵守其实现的接口；对象则是一个类实例化后的产物，我们称其为一个实例。当然这样说肯定不利于理解，我们就实际的写点中看不中用的代码辅助学习。 怪物横行的世界，总归需要点超级人物来摆平。 我们把一个“超人”作为一个类： 1class Superman &#123;&#125; 我们可以想象，一个超人诞生的时候肯定拥有至少一个超能力，这个超能力也可以抽象为一个对象，为这个对象定义一个描述他的类吧。一个超能力肯定有多种属性、（操作）方法，这个尽情的想象，但是目前我们先大致定义一个只有属性的“超能力”，至于能干啥，我们以后再丰富： 1234567891011121314151617class Power &#123; /** * 能力值 */ protected $ability; /** * 能力范围或距离 */ protected $range; public function __construct($ability, $range) &#123; $this-&gt;ability = $ability; $this-&gt;range = $range; &#125;&#125; 这时候我们回过头，修改一下之前的“超人”类，让一个“超人”创建的时候被赋予一个超能力： 123456789class Superman&#123; protected $power; public function __construct() &#123; $this-&gt;power = new Power(999, 100); &#125;&#125; 这样的话，当我们创建一个“超人”实例的时候，同时也创建了一个“超能力”的实例，但是，我们看到了一点，“超人”和“超能力”之间不可避免的产生了一个依赖。 所谓“依赖”，就是 “我若依赖你，我就不能离开你”。 在一个贯彻面向对象编程的项目中，这样的依赖随处可见。少量的依赖并不会有太过直观的影响，我们随着这个例子逐渐铺开，让大家慢慢意识到，当依赖达到一个量级时，是怎样一番噩梦般的体验。当然，我也会自然而然的讲述如何解决问题。 一堆乱麻，可怕的依赖之前的例子中，超能力类实例化后是一个具体的超能力，但是我们知道，超人的超能力是多元化的，每种超能力的方法、属性都有不小的差异，没法通过一种类描述完全。我们现在进行修改，我们假设超人可以有以下多种超能力： 飞行，属性有：飞行速度、持续飞行时间 蛮力，属性有：力量值 能量弹，属性有：伤害值、射击距离、同时射击个数 我们创建了如下类： 1234567891011121314151617181920class Flight&#123; protected $speed; protected $holdtime; public function __construct($speed, $holdtime) &#123;&#125;&#125;class Force&#123; protected $force; public function __construct($force) &#123;&#125;&#125;class Shot&#123; protected $atk; protected $range; protected $limit; public function __construct($atk, $range, $limit) &#123;&#125;&#125; 为了省事儿我没有详细写出 __construct() 这个构造函数的全部，只写了需要传递的参数。 好了，这下我们的超人有点“忙”了。在超人初始化的时候，我们会根据需要来实例化其拥有的超能力，大致如下： 1234567891011121314151617class Superman&#123; protected $power; public function __construct() &#123; $this-&gt;power = new Fight(9, 100); // $this-&gt;power = new Force(45); // $this-&gt;power = new Shot(99, 50, 2); /* $this-&gt;power = array( new Force(45), new Shot(99, 50, 2) ); */ &#125;&#125; 我们需要自己手动的在构造函数内（或者其他方法里）实例化一系列需要的类，这样并不好。可以想象，假如需求变更（不同的怪物横行地球），需要更多的有针对性的新的超能力，或者需要变更超能力的方法，我们必须 重新改造 超人。换句话说就是，改变超能力的同时，我还得重新制造个超人。效率太低了！新超人还没创造完成世界早已被毁灭。 这时，灵机一动的人想到：为什么不可以这样呢？超人的能力可以被随时更换，只需要添加或者更新一个芯片或者其他装置啥的（想到钢铁侠没）。这样的话就不要整个重新来过了。 对，就是这样的。 我们不应该手动在 “超人” 类中固化了他的 “超能力” 初始化的行为，而转由外部负责，由外部创造超能力模组、装置或者芯片等（我们后面统一称为 “模组”），植入超人体内的某一个接口，这个接口是一个既定的，只要这个 “模组” 满足这个接口的装置都可以被超人所利用，可以提升、增加超人的某一种能力。这种由外部负责其依赖需求的行为，我们可以称其为 “控制反转（IoC）”。 工厂模式，转移依赖当然，实现控制反转的方法有几种。在这之前，不如我们先了解一些好玩的东西。 我们可以想到，组件、工具（或者超人的模组），是一种可被生产的玩意儿，生产的地方当然是 “工厂（Factory）”，于是有人就提出了这样一种模式： 工厂模式。 工厂模式，顾名思义，就是一个类所依赖的外部事物的实例，都可以被一个或多个 “工厂” 创建的这样一种开发模式，就是 “工厂模式”。 我们为了给超人制造超能力模组，我们创建了一个工厂，它可以制造各种各样的模组，且仅需要通过一个方法： 1234567891011121314class SuperModuleFactory&#123; public function makeModule($moduleName, $options) &#123; switch ($moduleName) &#123; case &apos;Fight&apos;: return new Fight($options[0], $options[1]); case &apos;Force&apos;: return new Force($options[0]); case &apos;Shot&apos;: return new Shot($options[0], $options[1], $options[2]); &#125; &#125;&#125; 这时候，超人 创建之初就可以使用这个工厂： 123456789101112131415161718192021class Superman&#123; protected $power; public function __construct() &#123; // 初始化工厂 $factory = new SuperModuleFactory; // 通过工厂提供的方法制造需要的模块 $this-&gt;power = $factory-&gt;makeModule(&apos;Fight&apos;, [9, 100]); // $this-&gt;power = $factory-&gt;makeModule(&apos;Force&apos;, [45]); // $this-&gt;power = $factory-&gt;makeModule(&apos;Shot&apos;, [99, 50, 2]); /* $this-&gt;power = array( $factory-&gt;makeModule(&apos;Force&apos;, [45]), $factory-&gt;makeModule(&apos;Shot&apos;, [99, 50, 2]) ); */ &#125;&#125; 可以看得出，我们不再需要在超人初始化之初，去初始化许多第三方类，只需初始化一个工厂类，即可满足需求。但这样似乎和以前区别不大，只是没有那么多 new 关键字。其实我们稍微改造一下这个类，你就明白，工厂类的真正意义和价值了。 123456789101112131415161718192021class Superman&#123; protected $power; public function __construct(array $modules) &#123; // 初始化工厂 $factory = new SuperModuleFactory; // 通过工厂提供的方法制造需要的模块 foreach ($modules as $moduleName =&gt; $moduleOptions) &#123; $this-&gt;power[] = $factory-&gt;makeModule($moduleName, $moduleOptions); &#125; &#125;&#125;// 创建超人$superman = new Superman([ &apos;Fight&apos; =&gt; [9, 100], &apos;Shot&apos; =&gt; [99, 50, 2]]); 现在修改的结果令人满意。现在，“超人” 的创建不再依赖任何一个 “超能力” 的类，我们如若修改了或者增加了新的超能力，只需要针对修改 SuperModuleFactory 即可。扩充超能力的同时不再需要重新编辑超人的类文件，使得我们变得很轻松。但是，这才刚刚开始。 IoC 容器的重要组成 - 依赖注入由 “超人” 对 “超能力” 的依赖变成 “超人” 对 “超能力模组工厂” 的依赖后，对付小怪兽们变得更加得心应手。但这也正如你所看到的，依赖并未解除，只是由原来对多个外部的依赖变成了对一个 “工厂” 的依赖。假如工厂出了点麻烦，问题变得就很棘手。 其实大多数情况下，工厂模式已经足够了。工厂模式的缺点就是：接口未知（即没有一个很好的契约模型，关于这个我马上会有解释）、产生对象类型单一。总之就是，还是不够灵活。虽然如此，工厂模式依旧十分优秀，并且适用于绝大多数情况。不过我们为了讲解后面的依赖注入 ，这里就先夸大一下工厂模式的缺陷咯。 我们知道，超人依赖的模组，我们要求有统一的接口，这样才能和超人身上的注入接口对接，最终起到提升超能力的效果。 事实上，我之前说谎了，不仅仅只有一堆小怪兽，还有更多的大怪兽。嘿嘿。额，这时候似乎工厂的生产能力显得有些不足 —— 由于工厂模式下，所有的模组都已经在工厂类中安排好了，如果有新的、高级的模组加入，我们必须修改工厂类（好比增加新的生产线）： 123456789101112131415161718class SuperModuleFactory&#123; public function makeModule($moduleName, $options) &#123; switch ($moduleName) &#123; case &apos;Fight&apos;: return new Fight($options[0], $options[1]); case &apos;Force&apos;: return new Force($options[0]); case &apos;Shot&apos;: return new Shot($options[0], $options[1], $options[2]); // case &apos;more&apos;: ....... // case &apos;and more&apos;: ....... // case &apos;and more&apos;: ....... // case &apos;oh no! its too many!&apos;: ....... &#125; &#125;&#125; 看到没。。。噩梦般的感受！ 其实灵感就差一步！你可能会想到更为灵活的办法！对，下一步就是我们今天的主要配角 —— DI （依赖注入） 由于对超能力模组的需求不断增大，我们需要集合整个世界的高智商人才，一起解决问题，不应该仅仅只有几个工厂垄断负责。不过高智商人才们都非常自负，认为自己的想法是对的，创造出的超能力模组没有统一的接口，自然而然无法被正常使用。这时我们需要提出一种契约，这样无论是谁创造出的模组，都符合这样的接口，自然就可被正常使用。 12345678910interface SuperModuleInterface&#123; /** * 超能力激活方法 * * 任何一个超能力都得有该方法，并拥有一个参数 *@param array $target 针对目标，可以是一个或多个，自己或他人 */ public function activate(array $target);&#125; 上文中，我们定下了一个接口 （超能力模组的规范、契约），所有被创造的模组必须遵守该规范，才能被生产。 其实，这就是 php 中接口（ interface ）的用处和意义！很多人觉得，为什么 php 需要接口这种东西？难道不是 java 、 C# 之类的语言才有的吗？这么说，只要是一个正常的面向对象编程语言（虽然 php 可以面向过程），都应该具备这一特性。因为一个 对象（object） 本身是由他的模板或者原型 —— 类 （class） ，经过实例化后产生的一个具体事物，而有时候，实现统一种方法且不同功能（或特性）的时候，会存在很多的类（class），这时候就需要有一个契约，让大家编写出可以被随时替换却不会产生影响的接口。这种由编程语言本身提出的硬性规范，会增加更多优秀的特性。 虽然有些绕，但通过我们接下来的实例，大家会慢慢领会接口带来的好处。 这时候，那些提出更好的超能力模组的高智商人才，遵循这个接口，创建了下述（模组）类： 123456789101112131415161718192021/** * X-超能量 */class XPower implements SuperModuleInterface&#123; public function activate(array $target) &#123; // 这只是个例子。。具体自行脑补 &#125;&#125;/** * 终极炸弹 */class UltraBomb implements SuperModuleInterface&#123; public function activate(array $target) &#123; // 这只是个例子。。具体自行脑补 &#125;&#125; 同时，为了防止有些 “砖家” 自作聪明，或者一些叛徒恶意捣蛋，不遵守契约胡乱制造模组，影响超人，我们对超人初始化的方法进行改造： 123456789class Superman&#123; protected $module; public function __construct(SuperModuleInterface $module) &#123; $this-&gt;module = $module; &#125;&#125; 改造完毕！现在，当我们初始化 “超人” 类的时候，提供的模组实例必须是一个SuperModuleInterface 接口的实现。否则就会提示错误。 正是由于超人的创造变得容易，一个超人也就不需要太多的超能力，我们可以创造多个超人，并分别注入需要的超能力模组即可。这样的话，虽然一个超人只有一个超能力，但超人更容易变多，我们也不怕怪兽啦！ 现在有人疑惑了，你要讲的依赖注入呢？ 其实，上面讲的内容，正是依赖注入。 什么叫做依赖注入？本文从开头到现在提到的一系列依赖，只要不是由内部生产（比如初始化、构造函数 __construct 中通过工厂方法、自行手动 new 的），而是由外部以参数或其他形式注入的，都属于依赖注入（DI） 。是不是豁然开朗？事实上，就是这么简单。下面就是一个典型的依赖注入： 1234// 超能力模组$superModule = new XPower;// 初始化一个超人，并注入一个超能力模组依赖$superMan = new Superman($superModule); 关于依赖注入这个本文的主要配角，也就这么多需要讲的。理解了依赖注入，我们就可以继续深入问题。 更为先进的工厂 - IoC容器刚刚列了一段代码： 12$superModule = new XPower;$superMan = new Superman($superModule); 读者应该看出来了，手动的创建了一个超能力模组、手动的创建超人并注入了刚刚创建超能力模组。呵呵，手动。 现代社会，应该是高效率的生产，干净的车间，完美的自动化装配。 一群怪兽来了，如此低效率产出超人是不现实，我们需要自动化 —— 最多一条指令，千军万马来相见。我们需要一种高级的生产车间，我们只需要向生产车间提交一个脚本，工厂便能够通过指令自动化生产。这种更为高级的工厂，就是工厂模式的升华 —— IoC 容器。 12345678910111213141516171819202122232425262728293031323334353637class Container&#123; protected $binds; protected $instances; public function bind($abstract, $concrete) &#123; // Todo: 向 container 添加一种对象的的生产方式 // $abstract: 第一个参数 $abstract, 一般为一个字符串(有时候也会是一个接口), // 当你需要 make 这个类的对象的时候, 传入这个字符串(或者接口), 这样make 就知道制造什么样的对象了 // $concrete: 第二个参数 $concrete, 一般为一个 Closure 或者 一个单例对象, 用于说明制造这个对象的方式 if ($concrete instanceof Closure) &#123; $this-&gt;binds[$abstract] = $concrete; &#125; else &#123; $this-&gt;instances[$abstract] = $concrete; &#125; &#125; public function make($abstract, $parameters = []) &#123; // Todo: 生产一种对象 // $abstract: 在bind方法中已经介绍过 // $parameters: 生产这种对象所需要的参数 if (isset($this-&gt;instances[$abstract])) &#123; return $this-&gt;instances[$abstract]; &#125; array_unshift($parameters, $this); return call_user_func_array($this-&gt;binds[$abstract], $parameters); &#125;&#125; 这时候，一个十分粗糙的容器就诞生了。现在的确很简陋，但不妨碍我们进一步提升他。先着眼现在，看看这个容器如何使用吧： 123456789101112131415161718192021222324// 创建一个容器（后面称作超级工厂）$container = new Container;// 向该超级工厂添加超人的生产脚本$container-&gt;bind(&apos;superman&apos;, function($container, $moduleName) &#123; return new Superman($container-&gt;make($moduleName));&#125;);// 向该超级工厂添加超能力模组的生产脚本$container-&gt;bind(&apos;xpower&apos;, function($container) &#123; return new XPower;&#125;);// 同上$container-&gt;bind(&apos;ultrabomb&apos;, function($container) &#123; return new UltraBomb;&#125;);// ****************** 华丽丽的分割线 **********************// 开始启动生产$superman_1 = $container-&gt;make(&apos;superman&apos;, [&apos;xpower&apos;]);$superman_2 = $container-&gt;make(&apos;superman&apos;, [&apos;ultrabomb&apos;]);$superman_3 = $container-&gt;make(&apos;superman&apos;, [&apos;xpower&apos;]);// ...... 看到没？通过最初的 绑定（bind） 操作，我们向 超级工厂 注册了一些生产脚本，这些生产脚本在生产指令下达之时便会执行。发现没有？我们彻底的解除了 超人 与 超能力模组 的依赖关系，更重要的是，容器类也丝毫没有和他们产生任何依赖！我们通过注册、绑定的方式向容器中添加一段可以被执行的回调（可以是匿名函数、非匿名函数、类的方法）作为生产一个类的实例的 脚本 ，只有在真正的 生产（make） 操作被调用执行时，才会触发。 这样一种方式，使得我们更容易在创建一个实例的同时解决其依赖关系，并且更加灵活。当有新的需求，只需另外绑定一个“生产脚本”即可。 实际上，真正的 IoC 容器更为高级。我们现在的例子中，还是需要手动提供超人所需要的模组参数，但真正的 IoC 容器会根据类的依赖需求，自动在注册、绑定的一堆实例中搜寻符合的依赖需求，并自动注入到构造函数参数中去。Laravel 框架的服务容器正是这么做的。 不过我告诉大家，这种自动搜寻依赖需求的功能，是通过反射（Reflection）实现的，恰好的，php 完美的支持反射机制！关于反射，php 官方文档有详细的资料，并且中文翻译基本覆盖，足够学习和研究： 现在，到目前为止，我们已经不再惧怕怪兽们了。高智商人才集思广益，井井有条，根据接口契约创造规范的超能力模组。超人开始批量产出。最终，人人都是超人，你也可以是哦！ 重新审视 Laravel 的核心现在，我们开始慢慢解读 Laravel 的核心。其实，Laravel 的核心就是一个 IoC 容器，也恰好是我之前所说的高级的 IoC 容器。 可以说，Laravel 的核心本身十分轻量，并没有什么很神奇很实质性的应用功能。很多人用到的各种功能模块比如 Route（路由）、Eloquent ORM（数据库 ORM 组件）、Request（请求）以及 Response（响应）等等等等，实际上都是与核心无关的类模块提供的，这些类从注册到实例化，最终被你所使用，其实都是 Laravel 的服务容器负责的。 我们以大家最常见的 Route 类作为例子。大家可能经常见到路由定义是这样的： 123Route::get(&apos;/&apos;, function() &#123; // ......&#125;); 实际上， Route 类被定义在这个命名空间：Illuminate\Routing\Router，文件 vendor/laravel/framework/src/Illuminate/Routing/Router.php。 我们通过打开发现，这个类的这一系列方法，如 get，post，any 等都不是静态（static）方法，这是怎么一回事儿？不要急，我们继续。 服务提供者我们在前文介绍 IoC 容器的部分中，提到了，一个类需要绑定、注册至容器中，才能被“制造”。 对，一个类要被容器所能够提取，必须要先注册至这个容器。既然 Laravel 称这个容器叫做服务容器，那么我们需要某个服务，就得先注册、绑定这个服务到容器，那么提供服务并绑定服务至容器的东西，就是服务提供者（Service Provider）。 虽然，绑定一个类到容器不一定非要通过服务提供者。 但是，我们知道，有时候我们的类、模块会有需要其他类和组件的情况，为了保证初始化阶段不会出现所需要的模块和组件没有注册的情况，Laravel 将注册和初始化行为进行拆分，注册的时候就只能注册，初始化的时候就是初始化。拆分后的产物就是现在的服务提供者。 服务提供者主要分为两个部分，register（注册） 和boot（引导、初始化），具体参考文档。register 负责进行向容器注册“脚本”，但要注意注册部分不要有对未知事物的依赖，如果有，就要移步至 boot 部分。 门面（Facade）我们现在解答之前关于Route 的方法为何能以静态方法访问的问题。实际上这个问题文档上有写，简单说来就是模拟一个类，提供一个静态魔术方法__callStatic，并将该静态方法映射到真正的方法上。 我们使用的 Route 类实际上是 Illuminate\Support\Facades\Route 通过 class_alias() 函数创造的别名而已，这个类被定义在文件 vendor/laravel/framework/src/Illuminate/Support/Facades/Route.php 。 12345678910111213141516171819&lt;?php namespace Illuminate\Support\Facades; /** * @see \Illuminate\Routing\Router */ class Route extends Facade &#123; /** * Get the registered name of the component. * * @return string */ protected static function getFacadeAccessor() &#123; return &apos;router&apos;; &#125;&#125; 看到这个类之后，并没有找到之前 Route 调用的 get 方法，此时我们再看里面的这行注释， @see\Illuminate\Routing\Router ，他提示我们去找这个位置，那我们就去找一下，我们又发现了一个 Router 类，而这个 Router 类中，是有 get 方法的，看起来这里似乎就是 Route 的真实身份了。 123456789101112131415161718namespace Illuminate\Routing;class Router implements RegistrarContract &#123; ... /** * Register a new GET route with the router. * * @param string $uri * @param \Closure|array|string|null $action * @return \Illuminate\Routing\Route */ public function get($uri, $action = null) &#123; return $this-&gt;addRoute([&apos;GET&apos;, &apos;HEAD&apos;], $uri, $action); &#125; ...&#125; 那 Laravel 是如何为 Illuminate\Support\Facades\Route::class 这个类找到他的真实身份的呢？ 首先， class Route extends Facade， Route 继承自 Facade 类，Route 类又调用了静态的 get 方法，我们在 Route 类，或者是他的父类 Facade 中都是无法找到这个 get 方法的。 但是在 Facade 类中，我们可以发现有一个 __callStatic() 魔术方法 ，这个方法的作用就是：如果你想要调用的静态方法在类的定义中并没有声明，那么就会执行 __callStatic() 。在我们当前的情景中，静态方法 get 并没有被声明，那么当然，我们的类就会转而调用 __callStatic()。 1234567891011121314151617181920212223public static function __callStatic($method, $args)&#123; $instance = static::getFacadeRoot(); if (! $instance) &#123; throw new RuntimeException(&apos;A facade root has not been set.&apos;); &#125; switch (count($args)) &#123; case 0: return $instance-&gt;$method(); case 1: return $instance-&gt;$method($args[0]); case 2: return $instance-&gt;$method($args[0], $args[1]); case 3: return $instance-&gt;$method($args[0], $args[1], $args[2]); case 4: return $instance-&gt;$method($args[0], $args[1], $args[2], $args[3]); default: return call_user_func_array([$instance, $method], $args); &#125;&#125; 然后我们看 __callStatic() 的执行过程。首先看 getFacadeRoot() 是如何执行的： 1234public static function getFacadeRoot() &#123; return static::resolveFacadeInstance(static::getFacadeAccessor());&#125; 在最初，定义 Route 类时，我们只实现了一个方法 getFacadeAccessor() ，这时我们当初定义的字符串，就会在此处用到了，所以上面这个函数，实际上返回的内容就是 static::resolveFacadeInstance(&quot;router&quot;); 我们继续看 resolveFacadeInstance 这个函数的执行过程： 123456789101112131415protected static function resolveFacadeInstance($name)&#123; //判断是否为对象，当然不是了，$name 是字符串 if (is_object($name)) &#123; return $name; &#125; //判断 resolvedInstance 这个数组中是否存了 $name 相关的信息，当然也没有，因为我们假设程序是第一次执行这里 if (isset(static::$resolvedInstance[$name])) &#123; return static::$resolvedInstance[$name]; &#125; // 返回 static::$app[$name]，同时把得到的结果保存到上面验证的数组中 return static::$resolvedInstance[$name] = static::$app[$name];&#125; 所以我们的程序执行了最后的一个 return， 返回了 static::$app[‘router’] 这个值。 $app 就是前面说过的 Laravel Application 类的实例化对象，这个类是一个 IOC Container，实例化过程发生在 Laravel 最开始的时候。 在 Facade 初始化的时候，也让自己有了一个 static::$app 这个就是 Application 类的实例化对象。 而 $app 其实并没有 ‘router’ 这个属性，那为什么可以这样调用呢？ 是因为 Application 继承了 Container， 而 Container 又继承了 ArrayAccess 这个类。正是由于 ArrayAccess 的存在，以及 Container 实现了 ArrayAccess 的下面这个方法： 1234public function offsetGet($key)&#123; return $this-&gt;make($key);&#125; 所以，当我们使用 $app[&#39;router&#39;] 时，实际上是执行了 $app-&gt;make(‘router’)，到这里已经比较明显了，这里就是从 $app 这个 IOC Container 中， make 了一个 router 的实例。 已经绕的有点远了，不过还好，我们终于要回去了。到最初的 __callStatic() 中的： $instance = static::getFacadeRoot(); 也就相当于 $instance = $app-&gt;make(‘router’) __callStatic() 继续往下执行，想一下我们最初的那条代码 123Route::get(&apos;/&apos;, function() &#123; // ......&#125;); 有2个参数，所以会执行到 case 2 这条语句： 1return $instance-&gt;$method($args[0], $args[1]); 到这里，我们的 $instance 就是我们的 IOC Container make 出的具有实际功能的实例，这个实例将会执行这个实例的类所声明过的 get 方法。 参考文章： Laravel 服务容器实例教程 —— 深入理解控制反转（IoC）和依赖注入（DI） Laravel 从 1 行代码开始，带你系统性的理解 Laravel Service Container 的核心概念]]></content>
      <categories>
        <category>Laravel</category>
      </categories>
      <tags>
        <tag>Laravel</tag>
        <tag>控制反转(IoC)</tag>
        <tag>依赖注入(DI)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache2.4配置虚拟主机]]></title>
    <url>%2F2017%2F04%2F18%2Fapachevirtualhost%2F</url>
    <content type="text"><![CDATA[在开发项目的时候，每个项目对应的工程目录是不同的，想直接用一个自定义主机名来分别对应，这样就不用每次敲目录名了。Apache的虚拟主机可以实现这种功能。 配置虚拟主机有三种方式： 基于IP地址 基于主机名 基于端口号 本文主要介绍 基于主机名 和 基于端口号 这两种方式。 基于主机名我们安装好Apache后，如果成功安装，在浏览器中中输入 localhost 即可显示出欢迎界面，Apache 默认的监听端口是80端口，这里不用输入80端口号就可以。 首先修改 C:\Windows\System32\drivers\etc 目录下的 hosts 文件，如果修改时保存失败，第一种方式是可以修改 hosts 文件的安全属性，右键-&gt;属性-&gt;安全，赋予相应用户的权限即可；第二种方式是，使用管理员身份打开记事本，然后在记事本中打开 hosts 文件，进行如下修改： 1234567891011121314151617181920212223# Copyright (c) 1993-2009 Microsoft Corp.## This is a sample HOSTS file used by Microsoft TCP/IP for Windows.## This file contains the mappings of IP addresses to host names. Each# entry should be kept on an individual line. The IP address should# be placed in the first column followed by the corresponding host name.# The IP address and the host name should be separated by at least one# space.## Additionally, comments (such as these) may be inserted on individual# lines or following the machine name denoted by a &apos;#&apos; symbol.## For example:## 102.54.94.97 rhino.acme.com # source server# 38.25.63.10 x.acme.com # x client host# localhost name resolution is handled within DNS itself.# 127.0.0.1 localhost# ::1 localhost192.168.1.105 windows10.microdone.cn127.0.0.1 myapp.com // 自定义的主机名，名称不能与网上的域名冲突 hosts 文件配置好后，打开 \Apache24\conf 目录下的 httpd.conf 文件，修改如下： 1234567# Virtual hosts# Include conf/extra/httpd-vhosts.conf将 # 去掉# Virtual hostsInclude conf/extra/httpd-vhosts.conf 然后打开 \Apache24\conf\extra 目录下的 httpd-vhosts.conf 文件，修改如下： 123456789101112&lt;VirtualHost *:80&gt; ServerAdmin YourName DocumentRoot &quot;$&#123;SRVROOT&#125;/htdocs/laravel/public&quot; ServerName myapp.com ErrorLog &quot;$&#123;SRVROOT&#125;/htdocs/laravel/myapp.com-error.log&quot; CustomLog &quot;$&#123;SRVROOT&#125;/htdocs/laravel/myapp.com-access.log&quot; common &lt;Directory &quot;$&#123;SRVROOT&#125;/htdocs/laravel/public&quot;&gt; Options Indexes FollowSymLinks MultiViews AllowOverride None Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt; DocumentRoot 对应虚拟主机主目录；ServerName 对应主机名；ErrorLog 对应错误日志存放路径；CustomLog 对应访问日志存放路径；其中的&lt; Directory &gt;&lt; Directory /&gt;对应相应地设置信息。 最后，在浏览器中输入 mapp.com/index.php 就可以访问不同目录了。 基于端口号打开 \Apache24\conf 目录下的 httpd.conf 文件，修改如下： 1234567891011## Listen: Allows you to bind Apache to specific IP addresses and/or# ports, instead of the default. See also the &lt;VirtualHost&gt;# directive.## Change this to Listen on specific IP addresses as shown below to # prevent Apache from glomming onto all bound IP addresses.##Listen 12.34.56.78:80Listen 80Listen 8090 // 添加的apache监听的另一端口号，确保未被占用 然后打开 \Apache24\conf\extra 目录下的 httpd-vhosts.conf 文件，修改如下： 123456789101112&lt;VirtualHost *:8090&gt; ServerAdmin YourName DocumentRoot &quot;$&#123;SRVROOT&#125;/htdocs/laravel/public&quot; ServerName myapp.com ErrorLog &quot;$&#123;SRVROOT&#125;/htdocs/laravel/myapp.com-error.log&quot; CustomLog &quot;$&#123;SRVROOT&#125;/htdocs/laravel/myapp.com-access.log&quot; common &lt;Directory &quot;$&#123;SRVROOT&#125;/htdocs/laravel/public&quot;&gt; Options Indexes FollowSymLinks MultiViews AllowOverride None Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt; 以上通过两种方式实现了通过访问不同的主机名来访问不同目录，基于IP地址的方式请访问 Apache 配置虚拟主机三种方式]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>虚拟主机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP新的垃圾回收机制]]></title>
    <url>%2F2017%2F04%2F15%2Fphpnewgc%2F</url>
    <content type="text"><![CDATA[概述在5.2及更早版本的PHP中，没有专门的垃圾回收器GC（Garbage Collection），引擎在判断一个变量空间是否能够被释放的时候是依据这个变量的zval的refcount的值，如果refcount为0，那么变量的空间可以被释放，否则就不释放，这是一种非常简单的GC实现。 然而在这种简单的GC实现方案中，出现了意想不到的变量内存泄漏情况（Bug），引擎将无法回收这些内存，于是在PHP 5.3中出现了新的GC，新的GC有专门的机制负责清理垃圾数据，防止内存泄漏。 手册中相关内容：垃圾回收机制 在介绍这个新的GC之前，读者必须先了解PHP中变量的内部存储相关知识，请先阅读 PHP内核探索：变量的内部存储（值和类型） PHP内核探索：变量的内部存储（引用和计数） 什么算垃圾首先我们需要定义一下“垃圾”的概念，新的GC负责清理的垃圾是指变量的容器zval还存在，但是又没有任何变量名指向此zval。因此GC判断是否为垃圾的一个重要标准是有没有变量名指向变量容器zval。假设我们有一段PHP代码，使用了一个临时变量$tmp存储了一个字符串，在处理完字符串之后，就不需要这个$tmp变量了，$tmp变量对于我们来说可以算是一个“垃圾”了，但是对于GC来说，$tmp其实并不是一个垃圾，$tmp变量对我们没有意义，但是这个变量实际还存在，$tmp符号依然指向它所对应的zval，GC会认为PHP代码中可能还会使用到此变量，所以不会将其定义为垃圾。 那么如果我们在PHP代码中使用完$tmp后，调用unset删除这个变量，那么$tmp是不是就成为一个垃圾了呢。很可惜，GC仍然不认为$tmp是一个垃圾，因为$tmp在unset之后，refcount减少1变成了0(这里假设没有别的变量和$tmp指向相同的zval)，这个时候GC会直接将$tmp对应的zval的内存空间释放，$tmp和其对应的zval就根本不存在了。此时的$tmp也不是新的GC所要对付的那种“垃圾”。那么新的GC究竟要对付什么样的垃圾呢，下面我们将生产一个这样的垃圾。 顽固垃圾的产生过程如果读者已经阅读了变量内部存储相关的内容，想必对refcount和is_ref这些变量内部的信息有了一定的了解。这里我们将结合手册中的一个例子来介绍垃圾的产生过程： 12345&lt;?php$a = array(&apos;one&apos;);$a[] = &amp;$a;xdebug_debug_zval(&apos;a&apos;);?&gt; 以上例程的输出类似于： 1234a: (refcount=2, is_ref=1)=array ( 0 =&gt; (refcount=1, is_ref=0)=&apos;one&apos;, 1 =&gt; (refcount=2, is_ref=1)=...) 图示： 能看到数组变量 (a) 同时也是这个数组的第二个元素(1) 指向的变量容器中“refcount”为 2。上面的输出结果中的”…”说明发生了递归操作，显然在这种情况下意味着”…”指向原始数组。 跟刚刚一样，对一个变量调用unset，将删除这个符号，且它指向的变量容器中的引用次数也减1。所以，如果我们在执行完上面的代码后，对变量a调用unset，那么变量 a 和数组元素 “1” 所指向的变量容器的引用次数减1，从”2”变成”1”。下例可以说明： 1234(refcount=1, is_ref=1)=array ( 0 =&gt; (refcount=1, is_ref=0)=&apos;one&apos;, 1 =&gt; (refcount=1, is_ref=1)=...) 图示： 那么问题也就产生了，a已经不在符号表中了，用户无法再访问此变量，但是a之前指向的zval的refcount变为1而不是0，因此不能被回收，这样产生了内存泄露，这样，这么一个zval就成为了一个真是意义的垃圾了，新的GC要做的工作就是清理这种垃圾。 新的GC算法在较新的PHP手册中有简单的介绍新的GC使用的垃圾清理算法，这个算法名为 计数系统中的同步周期回收(Concurrent Cycle Collection in Reference Counted Systems) ，这里不详细介绍此算法，根据手册中的内容来先简单的介绍一下思路： 首先我们有几个基本的准则： 如果一个zval的refcount增加，那么此zval还在使用，不属于垃圾； 如果一个zval的refcount减少到0， 那么zval可以被释放掉，不属于垃圾； 如果一个zval的refcount减少之后大于0，那么此zval还不能被释放，此zval可能成为一个垃圾。 只有在准则3下，GC才会把zval收集起来，然后通过新的算法来判断此zval是否为垃圾。那么如何判断这么一个变量是否为真正的垃圾呢？ 简单的说，就是对此zval中的每个元素进行一次refcount减1操作，操作完成之后，如果zval的refcount=0，那么这个zval就是一个垃圾。这个原理看起来很简单，但是又不是那么容易理解，起初笔者也无法理解其含义，直到挖掘了源代码之后才算是了解。如果你现在不理解没有关系，后面会详细介绍，这里先把这算法的几个步骤描叙一下，首先引用手册中的一张图： A：为了避免每次变量的refcount减少的时候都调用GC的算法进行垃圾判断，此算法会先把所有前面准则3情况下的zval节点放入一个节点(root)缓冲区(root buffer)，并且将这些zval节点标记成紫色，同时算法必须确保每一个zval节点在缓冲区中之出现一次。当缓冲区被节点塞满的时候，GC才开始开始对缓冲区中的zval节点进行垃圾判断。 B：当缓冲区满了之后，算法以深度优先对每一个节点所包含的zval进行减1操作，为了确保不会对同一个zval的refcount重复执行减1操作，一旦zval的refcount减1之后会将zval标记成灰色。需要强调的是，这个步骤中，起初节点zval本身不做减1操作，但是如果节点zval中包含的zval又指向了节点zval（环形引用），那么这个时候需要对节点zval进行减1操作。 C：算法再次以深度优先判断每一个节点包含的zval的值，如果zval的refcount等于0，那么将其标记成白色(代表垃圾)，如果zval的refcount大于0，那么将对此zval以及其包含的zval进行refcount加1操作，这个是对非垃圾的还原操作，同时将这些zval的颜色变成黑色（zval的默认颜色属性） D：遍历zval节点，将C中标记成白色的节点zval释放掉。 比如还是前面那个变成垃圾的数组a对应的zval，命名为zval_a, 如果没有执行unset， zval_a的refcount为2，分别由a和a中的索引1指向这个zval。 用算法对这个数组中的所有元素（索引0和索引1）的zval的refcount进行减1操作，由于索引1对应的就是zval_a，所以这个时候zval_a的refcount应该变成了1，这样zva_a就不是一个垃圾。如果执行了unset操作，zval_a的refcount就是1，由zval_a中的索引1指向zval_a，用算法对数组中的所有元素（索引0和索引1）的zval的refcount进行减1操作，这样zval_a的refcount就会变成0，于是就发现zval_a是一个垃圾了。 算法就这样发现了顽固的垃圾数据。 对于一个包含环形引用的数组，对数组中包含的每个元素的zval进行减1操作，之后如果发现数组自身的zval的refcount变成了0，那么可以判断这个数组是一个垃圾。 这个道理其实很简单，假设数组a的refcount等于m，a中有n个元素又指向a，如果m等于n，那么算法的结果是m减n，m-n=0，那么a就是垃圾，如果m&gt;n，那么算法的结果m-n&gt;0，所以a就不是垃圾了。 m=n代表什么？代表a的refcount都来自数组a自身包含的zval元素，代表a之外没有任何变量指向它，代表用户代码空间中无法再访问到a所对应的zval，代表a是泄漏的内存，因此GC将a这个垃圾回收了。 新的GC算法的性能防止泄漏节省内存新的GC算法的目的就是为了防止循环引用的变量引起的内存泄漏问题，在PHP中GC算法，当节点缓冲区满了之后，垃圾分析算法会启动，并且会释放掉发现的垃圾，从而回收内存，在PHP手册上给了一段代码和内存使用状况图： 123456789101112131415161718&lt;?phpclass Foo&#123; public $var = &apos;3.1415962654&apos;;&#125;$baseMemory = memory_get_usage();for ( $i = 0; $i &lt;= 100000; $i++ )&#123; $a = new Foo; $a-&gt;self = $a; if ( $i % 500 === 0 ) &#123; echo sprintf( &apos;%8d: &apos;, $i ), memory_get_usage() - $baseMemory, &quot;/n&quot;; &#125;&#125;?&gt; 这段代码的循环体中，新建了一个对象变量，并且用对象的一个成员指向了自己，这样就形成了一个循环引用，当进入下一次循环的时候，又一次给对象变量重新赋值，这样会导致之前的对象变量内存泄漏，在这个例子里面有两个变量泄漏了，一个是对象本身，另外一个是对象中的成员self，但是这两个变量只有对象会作为垃圾收集器的节点被放入缓冲区(因为重新赋值相当于对它进行了unset操作，满足前面的准则3)。在这里我们进行了100,000次循环，而GC在缓冲区中有10,000节点的时候会启动垃圾分析算法，所以这里一共会进行10次的垃圾分析算法。从图中可以清晰的看到，在5.3版本PHP中，每次GC的垃圾分析算法被触发后，内存会有一个明显的减少。而在5.2版本的PHP中，内存使用量会一直增加。 运行效率影响 启用了新的GC后，垃圾分析算法将是一个比较耗时的操作，手册中给了一段测试代码： 1234567891011121314&lt;?phpclass Foo&#123; public $var = &apos;3.1415962654&apos;;&#125;for ( $i = 0; $i &lt;= 1000000; $i++ )&#123; $a = new Foo; $a-&gt;self = $a;&#125;echo memory_get_peak_usage(), &quot;/n&quot;;?&gt; 然后分别在GC开启和关闭的情况下执行这段代码： 123time php -dzend.enable_gc=0 -dmemory_limit=-1 -n example2.php# andtime php -dzend.enable_gc=1 -dmemory_limit=-1 -n example2.php 最终在该机器上，第一次执行大概使用10.7秒，第二次执行大概使用11.4秒，性能大约降低7%，不过内存的使用量降低了98%，从931M降低到了10M。当然这并不是一个比较科学的测试方法，但是也能说明一定的问题。这种代码测试的是一种极端恶劣条件，实际代码中，特别是在WEB的应用中，很难出现大量循环引用，GC的分析算法的启动不会这么频繁，小规模的代码中甚至很少有机会启动GC分析算法。 总结当GC的垃圾分析算法执行的时候，PHP脚本的效率会受到一定的影响，但是小规模的代码一般不会有这个机会运行这个算法。如果一旦脚本中GC分析算法开始运行了，那么将花费少量的时间节省出来了大量的内存，是一件非常划算的事情。新的GC对一些长期运行的PHP脚本效果更好，比如PHP的DAEMON守护进程，或则PHP-GTK进程等等。 参考文章： PHP新的垃圾回收机制:Zend GC详解 回收周期(Collecting Cycles)]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>垃圾回收</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP内核探索：变量的内部存储（引用和计数）]]></title>
    <url>%2F2017%2F04%2F15%2Fphpvarinternstore2%2F</url>
    <content type="text"><![CDATA[在 PHP内核探索：变量的内部存储（值和类型） 中介绍了PHP变量在引擎中是如何存储以及PHP如何实现其弱类型功能的。本文将从引用和计数的角度继续介绍变量的内部存储。 引用计数基本知识每个php变量存在一个叫”zval”的变量容器中。一个zval变量容器，除了包含变量的类型和值，还包括两个字节的额外信息。第一个是”is_ref”(此字段在5.3.2版本中是is_ref__gc)，是个bool值，用来标识这个变量是否是属于引用集合。通过这个字节，php引擎才能把普通变量和引用变量区分开来，由于php允许用户通过使用&amp;来使用自定义引用，zval变量容器中还有一个内部引用计数机制，来优化内存使用。第二个额外字节是”refcount”(此字段在5.3.2版本中是refcount__gc)，用以表示指向这个zval变量容器的变量(也称符号即symbol)个数。所有的符号存在一个符号表中，其中每个符号都有作用域，那些主脚本(比如：通过浏览器请求的的脚本)和每个函数或者方法也都有作用域。 当一个变量被赋常量值时，就会生成一个zval变量容器，如下例这样： Example #1 生成一个新的zval容器 123&lt;?php$a = &quot;new string&quot;;?&gt; 在上例中，新的变量a，是在当前作用域中生成的。并且生成了类型为 string 和值为new string的变量容器。在额外的两个字节信息中，”is_ref”被默认设置为 FALSE，因为没有任何自定义的引用生成。”refcount” 被设定为 1，因为这里只有一个变量使用这个变量容器， 注意到当”refcount”的值是1时，”is_ref”的值总是FALSE。如果你已经安装了Xdebug，你能通过调用函数 xdebug_debug_zval()显示”refcount”和”is_ref”的值。 Example #2 显示zval信息 123&lt;?phpxdebug_debug_zval(&apos;a&apos;);?&gt; 以上例程会输出： 1a: (refcount=1, is_ref=0)=&apos;new string&apos; 把一个变量赋值给另一变量将增加引用次数(refcount)。 Example #3 增加一个zval的引用计数 12345&lt;?php$a = &quot;new string&quot;;$b = $a;xdebug_debug_zval( &apos;a&apos; );?&gt; 以上例程会输出： 1a: (refcount=2, is_ref=0)=&apos;new string&apos; 这时，引用次数是2，因为同一个变量容器被变量a 和变量 b关联。当没必要时，php不会去复制已生成的变量容器。变量容器在”refcount“变成0时就被销毁. 当任何关联到某个变量容器的变量离开它的作用域(比如：函数执行结束)，或者对变量调用了函数 unset()时，”refcount“就会减1，下面的例子就能说明： Example #4 减少引用计数 1234567&lt;?php$a = &quot;new string&quot;;$c = $b = $a;xdebug_debug_zval( &apos;a&apos; );unset( $b, $c );xdebug_debug_zval( &apos;a&apos; );?&gt; 以上例程会输出： 12a: (refcount=3, is_ref=0)=&apos;new string&apos;a: (refcount=1, is_ref=0)=&apos;new string&apos; 如果我们现在执行 unset($a)，包含类型和值的这个变量容器就会从内存中删除。 复合类型当考虑像 array和object这样的复合类型时，事情就稍微有点复杂。与 标量类型的值不同，array和 object类型的变量把它们的成员或属性存在自己的符号表中。这意味着下面的例子将生成三个zval变量容器。 Example #5 Creating a array zval 1234&lt;?php$a = array( &apos;meaning&apos; =&gt; &apos;life&apos;, &apos;number&apos; =&gt; 42 );xdebug_debug_zval( &apos;a&apos; );?&gt; 以上例程的输出类似于： 1234a: (refcount=1, is_ref=0)=array ( &apos;meaning&apos; =&gt; (refcount=1, is_ref=0)=&apos;life&apos;, &apos;number&apos; =&gt; (refcount=1, is_ref=0)=42) 图示： 这三个zval变量容器是: a，meaning和 number。增加和减少”refcount”的规则和上面提到的一样。下面， 我们在数组中再添加一个元素，并且把它的值设为数组中已存在元素的值： Example #6 添加一个已经存在的元素到数组中 12345&lt;?php$a = array( &apos;meaning&apos; =&gt; &apos;life&apos;, &apos;number&apos; =&gt; 42 );$a[&apos;life&apos;] = $a[&apos;meaning&apos;];xdebug_debug_zval( &apos;a&apos; );?&gt; 以上例程的输出类似于： 12345a: (refcount=1, is_ref=0)=array ( &apos;meaning&apos; =&gt; (refcount=2, is_ref=0)=&apos;life&apos;, &apos;number&apos; =&gt; (refcount=1, is_ref=0)=42, &apos;life&apos; =&gt; (refcount=2, is_ref=0)=&apos;life&apos;) 图示： 从以上的Xdebug输出信息，我们看到原有的数组元素和新添加的数组元素关联到同一个”refcount”为2的zval变量容器. 尽管 Xdebug的输出显示两个值为’life’的 zval 变量容器，其实是同一个。 函数xdebug_debug_zval()不显示这个信息，但是你能通过显示内存指针信息来看到。 删除数组中的一个元素，就是类似于从作用域中删除一个变量. 删除后,数组中的这个元素所在的容器的“refcount”值减少，同样，当“refcount”为0时，这个变量容器就从内存中被删除，下面又一个例子可以说明： Example #7 从数组中删除一个元素 123456&lt;?php$a = array( &apos;meaning&apos; =&gt; &apos;life&apos;, &apos;number&apos; =&gt; 42 );$a[&apos;life&apos;] = $a[&apos;meaning&apos;];unset( $a[&apos;meaning&apos;], $a[&apos;number&apos;] );xdebug_debug_zval( &apos;a&apos; );?&gt; 以上例程的输出类似于： 123a: (refcount=1, is_ref=0)=array ( &apos;life&apos; =&gt; (refcount=1, is_ref=0)=&apos;life&apos;) 现在，当我们添加一个数组本身作为这个数组的元素时，事情就变得有趣，下个例子将说明这个。例中我们加入了引用操作符，否则php将生成一个复制。 Example #8 把数组作为一个元素添加到自己 12345&lt;?php$a = array( &apos;one&apos; );$a[] =&amp; $a;xdebug_debug_zval( &apos;a&apos; );?&gt; 以上例程的输出类似于： 1234a: (refcount=2, is_ref=1)=array ( 0 =&gt; (refcount=1, is_ref=0)=&apos;one&apos;, 1 =&gt; (refcount=2, is_ref=1)=...) 图示： 能看到数组变量 (a) 同时也是这个数组的第二个元素(1) 指向的变量容器中“refcount”为 2。上面的输出结果中的”…”说明发生了递归操作, 显然在这种情况下意味着”…”指向原始数组。 跟刚刚一样，对一个变量调用unset，将删除这个符号，且它指向的变量容器中的引用次数也减1。所以，如果我们在执行完上面的代码后，对变量a调用unset，那么变量 a 和数组元素 “1” 所指向的变量容器的引用次数减1，从”2”变成”1”。下例可以说明： Example #9 Unsetting a 1234(refcount=1, is_ref=1)=array ( 0 =&gt; (refcount=1, is_ref=0)=&apos;one&apos;, 1 =&gt; (refcount=1, is_ref=1)=...) 图示： 清理变量容器的问题尽管不再有某个作用域中的任何符号指向这个结构(就是变量容器)，由于数组元素“1”仍然指向数组本身，所以这个容器不能被清除 。因为没有另外的符号指向它，用户没有办法清除这个结构，结果就会导致内存泄漏。庆幸的是，php将在脚本执行结束时清除这个数据结构，但是在php清除之前，将耗费不少内存。如果你要实现分析算法，或者要做其他像一个子元素指向它的父元素这样的事情，这种情况就会经常发生。当然，同样的情况也会发生在对象上，实际上对象更有可能出现这种情况，因为对象总是隐式的被引用。 如果上面的情况发生仅仅一两次倒没什么，但是如果出现几千次，甚至几十万次的内存泄漏，这显然是个大问题。这样的问题往往发生在长时间运行的脚本中，比如请求基本上不会结束的守护进程或者单元测试中的大的套件中。后者的例子：在给巨大的eZ(一个知名的PHP Library) 组件库的模板组件做单元测试时，就可能会出现问题。有时测试可能需要耗用2GB的内存，而测试服务器很可能没有这么大的内存。 这就涉及到了PHP中的垃圾回收机制，将在下一篇博客中介绍。 写时复制机制（Copy on write）考虑以下示例： 1234567&lt;?php $a = &quot;Hello world&quot;; $b = $a; $b = &quot;new string&quot;; echo $a; echo $b; ?&gt; a和b明明是指向同一个zval，为什么修改了b，a还能保持不变呢，这就是copy on write（写时复制）技术，简单的说，当重新给b赋值的时候，会将b从之前的zval中分离出来。分离之后，a和b分别是指向不同的zval了。 写时复制技术的一个比较有名的应用是在unix类操作系统内核中，当一个进程调用fork函数生成一个子进程的时候，父子进程拥有相同的地址空间内容，在老版本的系统中，子进程是在fork的时候就将父进程的地址空间中的内容都拷贝一份，对于规模较大的程序这个过程可能会有着很大的开销，更崩溃的是，很多进程在fork之后，直接在子进程中调用exec执行另外一个程序，这样原来花了大量时间从父进程复制的地址空间都还没来得及碰一下就被新的进程地址空间代替，这显然是对资源的极大浪费，所以在后来的系统中，就使用了写时复制技术，fork之后，子进程的地址空间还是简单的指向父进程的地址空间，只有当子进程需要写地址空间中的内容的时候，才会单独分离一份（一般以内存页为单位）给子进程，这样就算子进程马上调用exec函数也没关系，因为根本就不需要从父进程的地址空间中拷贝内容，这样节约了内存同时又提高了速度。 当b从a指向的zval分离出来之后，zval的refcount就要减1，这样由之前的2变成了1，表示这个zval还有一个变量指向它，就是a。b变量指向了一个新的zval，新的zval的refcount为1，值为字符串”new string”，大概过程如下： 123$a = &quot;Hello world&quot; //a：(refcount=1, is_ref=0)=&quot;Hello world&quot;$b = $a //a，b： (refcount=2, is_ref=0)=&quot;Hello world&quot;$b = &quot;new string&quot; //a： (refcount=1, is_ref=0)=&quot;Hello world&quot; b: (refcount=1, is_ref=0)=&quot;new string&quot;(发生分离操作) 这个分离逻辑可以表叙为：对一个一般变量a（is_ref=0）进行一般赋值操作，如果a所指向的zval的计数refcount大于1,那么需要为a重新分配一个新的zval，并且把之前的zval的计数refcount减少1。 以上为普通赋值的情况，如果是引用赋值，我们看看这个变化过程： 123$a = &quot;Hello world&quot; //a： (refcount=1, is_ref=0)=&quot;Hello world&quot;$b = &amp;$a //a，b： (refcount=2, is_ref=1)=&quot;Hello world&quot;$b = &quot;new string&quot; //a，b： (refcount=2, is_ref=1)=&quot;new string&quot; 可以看出来，对一个引用类型的zval进行赋值是不会进行分离操作的，实际上我们再产生一个引用变量的时候是可能出现一个分离操作的，只是时机有些不同： 在普通赋值的情况下，分离操作发生在$b=&quot;new string&quot;这一步，也就是在对变量赋新的值的时候，才会进行zval分离操作； 在引用赋值的情况下，分离操作有可能发生在$b = &amp;$a这一步，也就是在生成引用变量的时候。 情况1就不多解释了，情况2中强调是有可能发生分离，以前面的这代码为例子，是否进行分离与a当前指向的zval的refcount有关系，代码中$b = &amp;$a 的时候，a指向的zval的refcount=1，这个时候不需要进行分离操作，但是如果refcount=2，那么就需要分离一个zval出来。比如如下代码： 123456&lt;?php $a = &quot;Hello world&quot;; $c = $a; $b = &amp;$a; $b = &quot;new string&quot;; ?&gt; 在执行引用赋值的时候，a指向的zval的refcount=2，因为a和c同时指向了这个zval，所以在$b=&amp;$a的时候，就需要进行一个分离操作，这个分离操作生成了一个ref=1的zval，并且计数为2，因为a，b两个变量指向分离出来的zval，原来的zval的refcount减少1，所以最终只有c指向一个值为”Hello world”，ref=0的zval1，a和b指向一个值为”Hello world”，ref=1的zval2。 这样我们对c的修改时在操作zval1，对a和b的修改都是在操作zval2，这样就符合引用的特性了。 此过程大致如下： 1234$a = &quot;Hello world&quot;; //a: (refcount=1, is_ref=0)=&quot;Hello world&quot;$c = $a; // a,c: (refcount=2, is_ref=0)=&quot;Hello world&quot;$b = &amp;$a; // c: (refcount=1, is_ref=0)=&quot;Hello world&quot; a,b: (refcount=2, is_ref=1)=&quot;Hello world&quot; (发生分离操作)$b = &quot;new string&quot;; // c: (refcount=1, is_ref=0)=&quot;Hello world&quot; a,b: (refcount=2, is_ref=1)=&quot;new string&quot; 试想一下如果不进行这个分离会有什么后果？如果不进行分离，a，b，c都指向了同一个zval，对b的修改也会影响到c，这显然是不符合PHP语言特性的。 这个分离逻辑可以表述为：将一个一般变量a(is_ref=0)的引用赋给另外一个变量b的时候，如果a的refcount大于1，那么需要对a进行一次分离操作，分离之后的 zval 的is_ref等于1，refcount等于2。 unset的作用unset()并非一个函数，而是一种语言结构，主要的操作时从当前符号表中删除参数中的符号，比如在全局代码中执行unset($a)，那么将会在全局符号表中删除a这个符号。全局符号表是一张哈希表，建立这张表的时候会提供一个表中的项的析构函数，当我们从符号表中删除a的时候，会对符号a指向的项（这里是zval的指针）调用这个析构函数，这个析构函数的主要功能是将a对应的zval的refcount减1，如果refcount变成了0，那么释放这个zval。所以当我们调用unset的时候，不一定能释放变量所占的内存空间，只有当这个变量对应的zval没有别的变量指向它的时候，才会释放掉zval，否则只是对refcount进行减1操作。 继续Example12345678910111213141516如果将代码： &lt;?php $a = &quot;Hello world&quot;; $c = $a; $b = &amp;$a; $b = &quot;new string&quot;; ?&gt; 改为： &lt;?php $a = &quot;Hello world&quot;; $c = &amp;$a; $b = &amp;$a; $b = &quot;new string&quot;; ?&gt; 执行以上示例，会发现，输出的a，b，c都是“new string”： 此过程大致如下： 1234$a = &quot;Hello world&quot;; //a：(refcount=1, is_ref=0)=&quot;Hello world&quot;$c = &amp;$a; //a，c:(refcount=2, is_ref=1)=&quot;hello world&quot;$b = &amp;$a; //a，b，c:(refcount=3, is_ref=1)=&quot;hello world&quot;$b = &quot;new string&quot;; //a，b，c:(refcount=3, is_ref=1)=&quot;new string&quot; 我们发现，如果都是引用赋值的话，PHP是不会进行分离的，这种情况与上述情况2还是有区别的。 参考文章： 引用计数基本知识 变量的内部存储：引用和计数]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>变量的内部存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP内核探索：变量的内部存储（值和类型）]]></title>
    <url>%2F2017%2F04%2F15%2Fphpvarinternstore1%2F</url>
    <content type="text"><![CDATA[在翻看 PHP手册 的时候，看到了 垃圾回收机制 ，介绍了PHP 5.3新的垃圾回收机制（GC）的特点，里面涉及到了PHP变量内部存储的知识，如果你想对PHP变量存储结构有一个了解或想对PHP变量加深理解的话，本文是适合你的，比较深入的去看源代码吧。 PHP是一种弱类型的脚本语言，弱类型不表示PHP的变量没有类型区分，PHP变量有8种原始类型。 四种标量类型： Boolean（布尔型） Integer（整型） Float（浮点型） String（字符串） 两种复合类型： Array（数组） Object（对象） 两种特殊类型： Resource（资源） NULL 我们都知道，在PHP程序运行中，可以将变量从一种类型转换为另一种类型，那么PHP是怎么实现这个过程的呢？ 在PHP引擎（Zend）内部，变量都是用如下的结构体来表示的： 1234567struct _zval_struct &#123; /* Variable information */ zvalue_value value; /* value */ zend_uint refcount__gc; zend_uchar type; /* active type */ zend_uchar is_ref__gc; &#125;; 其中： value：存储变量的值； refcount__gc：引用计数； type：变量的动态类型； is_ref__gc：是否为引用。 type的各种类型都被定义成了宏： 12345678910#define IS_NULL 0 #define IS_LONG 1 #define IS_DOUBLE 2 #define IS_BOOL 3 #define IS_ARRAY 4 #define IS_OBJECT 5 #define IS_STRING 6 #define IS_RESOURCE 7 #define IS_CONSTANT 8 #define IS_CONSTANT_ARRAY 9 zvalue_value 是真正保存数据的关键部分，定义为一个联合： 12345678910typedef union _zvalue_value &#123; long lval; /* long value */ double dval; /* double value */ struct &#123; char *val; int len; &#125; str; HashTable *ht; /* hash table value */ zend_object_value obj; &#125; zvalue_value; PHP根据 zval 中的 type 字段来储存一个变量的真正类型，然后根据 type 来选择如何获取 zvalue_value 的值，比如对于整型和bool值： 12zval.type = IS_LONG; //整型zval.type = IS_BOOL; //布尔型 就去取 zval.value.lval，对于 bool 值来说 lval∈(0|1)；如果是双精度，或者 float 则会去取 zval.value 的 dval。而如果是字符串，那么： 1zval.type = IS_STRING //字符串 这个时候，就会取 zval.value.str 而这个也是个结构，存有C类型的字符串和字符串的长度。 而对于数组和对象，则type分别对应IS_ARRAY、 IS_OBJECT,，相对应的则分别取 zval.value.ht 和 obj。 比较特别的是资源，在PHP中，资源是个很特别的变量，任何不属于PHP内建的变量类型的变量，都会被看作成资源来进行保存，比如，数据库句柄、打开的文件句柄等等。 对于资源： 1type = IS_RESOURCE //资源 这个时候，会去取 zval.value.lval， 此时的 lval 是个整型的指示器， 然后 PHP 会再根据这个指示器在 PHP 内建的一个资源列表中查询相对应的资源，此时的 lval 就好像是对应于资源链表的偏移值。 1ZEND_FETCH_RESOURCE(con, type, zval *, default, resource_name, resource_type); 借用这样的机制，PHP就实现了弱类型，因为对于ZE（Zend）的来说，它所面对的永远都是同一种类型，那就是 zval。 在了解了 PHP 变量的内部存储后，新的问题就来了，ZE是如何把用户自定义变量和内部结构 zval 联系起来的呢？ 1234&lt;?php $var = &quot;laruence&quot;; echo $var;?&gt; PHP内部都是使用 zval 来表示变量的，但是对于上面的脚本，我们的变量是有名字的\$var。而 zval 中并没有相应的字段来体现变量名。在PHP中，所有的变量都会存储在一个数组中（确切的说是hash table）。 当你创建一个变量的时候，PHP会为这个变量分配一个 zval，填入相应的变量值，然后将这个变量的名字、和指向这个zval的指针填入一个数组中。然后，当你获取这个变量的时候，PHP会通过查找这个数组，获得对应的 zval。 查看_zend_executor_globals结构（这个结构在PHP的执行器保存一些执行相关的上下文信息）： 1234567891011struct _zend_executor_globals &#123; .... HashTable *active_symbol_table; /*活动符号表*/ HashTable symbol_table; /*全局符号表*/ HashTable included_files; jmp_buf *bailout; int error_reporting; .....&#125; 以上只是对PHP变量内部存储的简单介绍，如果想深入了解，建议研究PHP的源码。 参考文章： 深入PHP变量存储结构 变量的内部存储：值和类型]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>变量的内部存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法总结]]></title>
    <url>%2F2017%2F04%2F11%2Fphpsortalgorithm%2F</url>
    <content type="text"><![CDATA[各种排序算法稳定性以及时间复杂度总结： 类别 排序方法 平均时间复杂度 最好情况 最坏情况 空间复杂度 稳定性 交换排序 冒泡排序 O(n^2) O(n) O(n^2) O(1) 稳定 交换排序 快排 O(nlogn) O(nlogn) O(n^2) O(nlogn) 不稳定 选择排序 直接选择排序 O(n^2) O(n^2) O(n^2) O(1) 不稳定 选择排序 堆排序 O(nlogn) O(nlogn) O(nlogn) O(1) 不稳定 插入排序 直接插入排序 O(n^2) O(n) O(n^2) O(1) 稳定 插入排序 希尔排序 O(n^1.5) O(n) O(n^2) O(1) 不稳定 未分类 归并排序 O(nlogn) O(nlogn) O(nlogn) O(n) 稳定 未分类 基数排序 O(d(n+r)) O(d(n+rd)) O(d(n+r)) O(rd+n) 稳定 Note：logn代表以2为底n的对数。 当原表有序或基本有序时，直接插入排序和冒泡排序将大大减少比较次数和移动记录的次数，时间复杂度可降至O(n)；而快速排序则相反，当原表基本有序时，将蜕化为冒泡排序，时间复杂度提高为O(n^2)；原表是否有序，对直接选择排序、堆排序、归并排序和基数排序的时间复杂度影响不大。 排序算法的稳定性说明若待排序的序列中，存在多个具有相同关键字的记录，经过排序，这些记录的相对次序保持不变，则称该算法是稳定的；若经排序后，记录的相对次序发生了改变，则称该算法是不稳定的。 Example： 如果 Ai = Aj 且 Ai 原来在位置前，排序后Ai 还是要在 Aj 位置前。 稳定性的好处：排序算法如果是稳定的，那么从一个键上排序，然后再从另一个键上排序，第一个键排序的结果可以为第二个键排序所用。基数排序就是这样，先按低位排序，逐次按高位排序，低位相同的元素，其顺序在高位也相同时是不会改变的。另外，如果排序算法稳定，可以避免多余的比较；Example： 对于不稳定的算法我们可以这样改进： 在每个输入元素加一个 index，表示初始时的数组索引，当不稳定的算法排好序后，对于相同的元素对 index 排序即可。 冒泡排序冒泡排序就是把小的元素往前调或者把大的元素往后调。比较是相邻的两个元素比较，交换也发生在这两个元素之间。所以，如果两个元素相等，是不会再把他们交换的；如果两个相等的元素没有相邻，那么即使通过前面的两两交换把两个相邻起来，这时候也不会交换，所以相同元素的前后顺序并没有改变，所以冒泡排序是一种稳定排序算法。 快排快速排序有两个方向，左边的i下标一直往右走，当 a[i] &lt;= a[center_index]，其中 center_index 是中枢元素的数组下标，一般取为数组第0个元素。而右边的j下标一直往左走，当 a[j] &gt; a[center_index]。如果i和j都走不动了，i &lt;= j，交换 a[i] 和 a[j]，重复上面的过程，直到i &gt; j。 交换a[j]和a[center_index]，完成一趟快速排序。在中枢元素和 a[j] 交换的时候，很有可能把前面的元素的稳定性打乱，比如序列为5 3 3 4 3 8 9 10 11，现在中枢元素5和3（第5个元素，下标从1开始计）交换就会把元素3的稳定性打乱，所以快速排序是一个不稳定的排序算法，不稳定发生在中枢元素和 a[j] 交换的时刻。 直接选择排序选择排序是给每个位置选择当前元素最小的，比如给第一个位置选择最小的，在剩余元素里面给第二个元素选择第二小的，依次类推，直到第n-1个元素，第 n 个元素不用选择了，因为只剩下它一个最大的元素了。那么，在一趟选择，如果当前元素比一个元素小，而该小的元素又出现在一个和当前元素相等的元素后面，那么交换后稳定性就被破坏了。序列5 8 5 2 9，我们知道第一遍选择第1个元素5会和2交换，那么原序列中2个5的相对前后顺序就被破坏了，所以选择排序不是一个稳定的排序算法。 堆排序我们知道堆的结构是节点 i 的孩子为2 i和2 i + 1节点，大顶堆要求父节点大于等于其2个子节点，小顶堆要求父节点小于等于其2个子节点。在一个长为n 的序列，堆排序的过程是从第n / 2开始和其子节点共3个值选择最大（大顶堆）或者最小（小顶堆），这3个元素之间的选择当然不会破坏稳定性。但当为n / 2 - 1， n / 2 - 2， … 1这些个父节点选择元素时，就会破坏稳定性。有可能第n / 2个父节点交换把后面一个元素交换过去了，而第n / 2 - 1个父节点把后面一个相同的元素没 有交换，那么这2个相同的元素之间的稳定性就被破坏了。所以，堆排序不是稳定的排序算法。 直接插入排序插入排序是在一个已经有序的小序列的基础上，一次插入一个元素。当然，刚开始这个有序的小序列只有1个元素，就是第一个元素。比较是从有序序列的末尾开始，也就是想要插入的元素和已经有序的最大者开始比起，如果比它大则直接插入在其后面，否则一直往前找直到找到它该插入的位置。如果碰见一个和插入元素相等的，那么插入元素把想插入的元素放在相等元素的后面。所以，相等元素的前后顺序没有改变，从原无序序列出去的顺序就是排好序后的顺序，所以插入排序是稳定的。 希尔排序希尔排序是按照不同步长对元素进行插入排序，当刚开始元素很无序的时候，步长最大，所以插入排序的元素个数很少，速度很快；当元素基本有序了，步长很小， 插入排序对于有序的序列效率很高。所以，希尔排序的时间复杂度会比O(n^2)好一些。由于多次插入排序，我们知道一次插入排序是稳定的，不会改变相同元素的相对顺序，但在不同的插入排序过程中，相同的元素可能在各自的插入排序中移动，最后其稳定性就会被打乱，所以希尔排序是不稳定的。 归并排序归并排序是把序列递归地分成短序列，递归出口是短序列只有1个元素（认为直接有序）或者2个序列（1次比较和交换），然后把各个有序的段序列合并成一个有序的长序列，不断合并直到原序列全部排好序。可以发现，在1个或2个元素时，1个元素不会交换，2个元素如果大小相等也没有人故意交换，这不会破坏稳定性。那么，在短的有序序列合并的过程中，稳定是否受到破坏？没有，合并过程中我们可以保证如果两个当前元素相等时，我们把处在前面的序列的元素保存在结果序列的前面，这样就保证了稳定性。所以，归并排序也是稳定的排序算法。 基数排序基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序，最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。基数排序基于分别排序，分别收集，所以其是稳定的排序算法。 排序算法冒泡排序基本思想 两个数比较大小，较大的数下沉，较小的数冒起来。 过程 比较相邻的两个数据，如果第二个数小，就交换位置； 从后向前两两比较，一直到比较最前两个数据。最终最小数被交换到起始的位置，这样第一个最小数的位置就排好了；继续重复上述过程，依次将第2、3、……、n-1 个最小数排好位置。 PHP代码实现、 123456789101112public function MaoPao($sort_array)&#123; for ($i = 0; $i &lt; count($sort_array); $i++) &#123; for ($j = 0; $j &lt; count($sort_array) - $i - 1; $j++) &#123; if ($sort_array[$j] &gt; $sort_array[$j + 1])&#123; $tmp = $sort_array[$j]; $sort_array[$j] = $sort_array[$j + 1]; $sort_array[$j + 1] = $tmp; &#125; &#125; &#125; return $sort_array; &#125; 优化 冒泡排序的问题在于，数据的顺序排好之后，冒泡算法仍然会继续进行下一轮的比较，直到 count($sort_array)-1 次，后面的比较是没有意义的。 解决办法：我们可以设置标志位flag，如果发生了交换flag设置为true；如果没有交换就设置为false。这样当一轮比较结束后如果flag仍为false，即：这一轮没有发生交换，说明数据的顺序已经排好，没有必要继续进行下去。 1234567891011121314151617public function MaoPao($sort_array)&#123; for ($i = 0; $i &lt; count($sort_array); $i++) &#123; $flag = 0; for ($j = 0; $j &lt; count($sort_array) - $i - 1; $j++) &#123; if ($sort_array[$j] &gt; $sort_array[$j + 1])&#123; $tmp = $sort_array[$j]; $sort_array[$j] = $sort_array[$j + 1]; $sort_array[$j + 1] = $tmp; $flag = 1; &#125; &#125; if ($flag == 0)&#123; break; &#125; &#125; return $sort_array; &#125; 快排基本思想：分治 先从数列中取出一个数作为key值；将比这个数小的数全部放在它的左边，大于或等于它的数全部放在它的右边；对左右两个小数列重复第二步，直至各区间只有1个数。 PHP代码实现 12345678910111213141516171819202122public function Fast($sort_array)&#123; $left_array = array(); $right_array = array(); $index = 1; if (!is_array($sort_array))&#123; return false; &#125; if (count($sort_array) &lt;= 1)&#123; return $sort_array; &#125; while ($index &lt; count($sort_array))&#123; if ($sort_array[$index] &gt; $sort_array[0])&#123; $right_array[] = $sort_array[$index]; &#125; elseif ($sort_array[$index] &lt;= $sort_array[0])&#123; $left_array[] = $sort_array[$index]; &#125; $index++; &#125; $left_array = $this-&gt;Fast($left_array); $right_array = $this-&gt;Fast($right_array); return array_merge($left_array, array($sort_array[0]), $right_array); &#125; 直接选择排序基本思想 在长度为 n 的无序数组中，第一次遍历 n-1 个数，找到最小的数值与第一个元素交换； 第二次遍历 n-2 个数，找到最小的数值与第二个元素交换； …… 第 n-1 次遍历，找到最小的数值与第 n-1 个元素交换，排序完成。 过程 PHP代码实现 1234567891011121314public function Select($sort_array)&#123; for ($i = 0; $i &lt; count($sort_array); $i++)&#123; $min = $i; for ($j = $i + 1; $j &lt; count($sort_array); $j++)&#123; if ($sort_array[$min] &gt; $sort_array[$j])&#123; $min = $j; &#125; &#125; $tmp = $sort_array[$min]; $sort_array[$min] = $sort_array[$i]; $sort_array[$i] = $tmp; &#125; return $sort_array; &#125; 堆排序基本思想 过程 图示： （88,85,83,73,72,60,57,48,42,6） PHP代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142public function DuiPaiXu($sort_array)&#123; $this-&gt;buildHeap($sort_array); $count = count($sort_array); while ($count &gt; 1) &#123; $this-&gt;swap($sort_array, $count - 1, 0); $count--; $this-&gt;adjustHeap($sort_array, $count, 0); &#125; return $sort_array; &#125; public function buildHeap(&amp;$arr)&#123; $node = floor(count($arr) / 2) - 1; for ($i = $node; $i &gt;= 0; $i--) &#123; $this-&gt;adjustHeap($arr, count($arr), $i); &#125; &#125;public function adjustHeap(&amp;$arr, $maxLen, $node)&#123; $left_child = 2 * $node + 1; $right_child = 2 * $node + 2; $max = $node; while ($left_child &lt; $maxLen || $right_child &lt; $maxLen) &#123; if ($left_child &lt; $maxLen &amp;&amp; $arr[$left_child] &gt; $arr[$max]) &#123; $max = $left_child; &#125; if ($right_child &lt; $maxLen &amp;&amp; $arr[$right_child] &gt; $arr[$max]) &#123; $max = $right_child; &#125; if ($max != $node) &#123; $this-&gt;swap($arr, $max, $node); $node = $max; $left_child = 2 * $node + 1; $right_child = 2 * $node + 2; &#125; else &#123; break; &#125; &#125; &#125;public function swap(&amp;$arr, $m, $n)&#123; $arr[$m] = $arr[$m] ^ $arr[$n]; $arr[$n] = $arr[$n] ^ $arr[$m]; $arr[$m] = $arr[$m] ^ $arr[$n]; &#125; 直接插入排序基本思想 在要排序的一组数中，假定前 n-1 个数已经排好序，现在将第 n 个数插到前面的有序数列中，使得这 n 个数也是排好顺序的。如此反复循环，直到全部排好顺序。 过程 PHP代码实现 123456789101112public function ChaRu($sort_array)&#123; for ($i = 2; $i &lt; count($sort_array); $i++)&#123; $j = $i - 1; $key = $sort_array[$i]; while ($j &gt;= 0 &amp;&amp; $sort_array[$j] &gt; $key)&#123; $sort_array[$j + 1] = $sort_array[$j]; $j--; &#125; $sort_array[$j + 1] = $key; &#125; return $sort_array; &#125; 希尔排序基本思想 在要排序的一组数中，根据某一增量分为若干子序列，并对子序列分别进行插入排序。然后逐渐将增量减小,并重复上述过程。直至增量为1,此时数据序列基本有序,最后进行插入排序。 过程 PHP代码实现 123456789101112131415public function XiErPaiXu($sort_array)&#123; if (!is_array($sort_array)) return false; $n = count($sort_array); for ($gap = floor($n / 2); $gap &gt; 0; $gap = floor($gap /= 2)) &#123; for ($i = $gap; $i &lt; $n; ++$i) &#123; for ($j = $i - $gap; $j &gt;= 0 &amp;&amp; $sort_array[$j + $gap] &lt; $sort_array[$j]; $j -= $gap) &#123; $temp = $sort_array[$j]; $sort_array[$j] = $sort_array[$j + $gap]; $sort_array[$j + $gap] = $temp; &#125; &#125; &#125; return $sort_array; &#125; 归并排序基本思想 归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法的一个非常典型的应用。 首先考虑下如何将2个有序数列合并。这个非常简单，只要从比较2个数列的第一个数，谁小就先取谁，取了后就在对应数列中删除这个数。然后再进行比较，如果有数列为空，那直接将另一个数列的数据依次取出即可。 解决了上面的合并有序数列问题，再来看归并排序，其的基本思路就是将数组分成2组A，B，如果这2组组内的数据都是有序的，那么就可以很方便的将这2组数据进行排序。如何让这2组组内数据有序了？可以将A，B组各自再分成2组。依次类推，当分出来的小组只有1个数据时，可以认为这个小组组内已经达到了有序，然后再合并相邻的2个小组就可以了。这样通过先递归的分解数列，再合并数列就完成了归并排序。 过程 PHP代码实现 1234567891011121314151617181920public function GuiBing($sort_array)&#123; $len = count($sort_array); if($len &lt;= 1) return $sort_array; $mid = intval($len/2); $left_arr = array_slice($sort_array, 0, $mid); $right_arr = array_slice($sort_array, $mid); $left_arr = $this-&gt;GuiBing($left_arr); $right_arr = $this-&gt;GuiBing($right_arr); $sort_array = $this-&gt;GuiBing_child($left_arr, $right_arr); return $sort_array; &#125;public function GuiBing_child($arrA, $arrB)&#123; $arrC = array(); while(count($arrA) &amp;&amp; count($arrB))&#123; $arrC[] = $arrA[&apos;0&apos;] &lt; $arrB[&apos;0&apos;] ? array_shift($arrA) : array_shift($arrB); &#125; return array_merge($arrC, $arrA, $arrB); &#125; 基数排序基本思想 我们知道，任何一个阿拉伯数，它的各个位数上的基数都是以0~9来表示的。假设我们有编号0~9的10个桶，遍历需要进行排序的各个值的个位，然后根据个位的数值放进相应的桶中，分类后，我们在从各个桶中，将这些数按照从编号0到编号9的顺序依次将所有数取出来。这时，得到的序列就是个位数上呈递增趋势的序列。 接下来，可以对十位数、百位数也按照这种方法进行排序，最后就能得到排序完成的序列。 基数排序适用于对位数较少的排序，位数较长会浪费空间。 基数排序的平均时间复杂度为O(d(n+r))，其中 d 表示位数，r为基数（d的取值范围）。 LSD的基数排序适用于位数小的数列，如果位数多的话，使用MSD的效率会比较好。MSD的方式与LSD相反，是由高位数为基底开始进行分配，但在分配之后并不马上合并回一个数组中，而是在每个“桶子”中建立“子桶”，将每个桶子中的数值按照下一数位的值分配到“子桶”中。在进行完最低位数的分配后再合并回单一的数组中。 本文介绍的是LSD模式。 过程 PHP代码实现 123456789101112131415161718192021222324252627public function GetNumInPos($num, $pos)&#123; $temp = 1; for ($i = 0; $i &lt; $pos - 1; $i++) $temp *= 10; return ($num / $temp) % 10; &#125;public function JiShu($sort_array, $pos) &#123; $l = count($sort_array); $bucket = array(); for($i = 0; $i &lt; 10; $i++) $bucket[$i] = array(0); for($p = 1; $p &lt;= $pos; $p++) &#123; for($i = 0; $i &lt; $l; $i++) &#123; $n = $this-&gt;GetNumInPos($sort_array[$i], $p); $index = ++$bucket[$n][0]; $bucket[$n][$index] = $sort_array[$i]; &#125; for($i=0, $j=0; $i&lt;10; $i++) &#123; for($num = 1; $num&lt;=$bucket[$i][0]; $num++) &#123; $sort_array[$j++] = $bucket[$i][$num]; &#125; $bucket[$i][0] = 0; &#125; &#125; return $sort_array; &#125; 参考文章： 排序算法总结 八大排序算法 九大排序算法再总结 稳定排序和不稳定排序 基数排序 排序八 基数排序]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>时间复杂度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新浪2014校园招聘 PHP 开发工程师]]></title>
    <url>%2F2017%2F04%2F09%2Fsina2014php%2F</url>
    <content type="text"><![CDATA[PHP中的%在PHP中，正数%正数为正数，正数%负数为正数，负数%正数为负数，Example： 123echo 24 % 5; // 输出 4echo 24 % (-5); // 输出 4echo -24 % 5; // 输出 -4 析构函数在PHP5中，如果需要类 C 的实例销毁时自动完成默写逻辑，定义析构函数_­­__destruct shell命令top：用于实时显示进程的动态。 ps：用于显示当前进程的状态。 mv：用来为文件或目录改名、或将文件或目录移入其它位置。 find：用来在指定目录下查找文件。任何位于参数之前的字符串都将被视为欲查找的目录名。如果使用该命令时，不设置任何参数，则find命令将在当前目录下查找子目录与文件。并且将查找到的子目录和文件全部进行显示。 df：用于显示目前在Linux系统上的文件系统的磁盘使用情况统计。 cat：用于连接文件并打印到标准输出设备上。 chmod：Linux/Unix 的文件调用权限分为三级 : 文件拥有者、群组、其他。利用 chmod 可以控制文件如何被他人所调用。其实就是修改读写文件权限。 chgrp：用于变更文件或目录的所属群组。 grep：用于查找文件里符合条件的字符串。 wc：用于计算字数。利用 wc 指令我们可以计算文件的Byte数、字数、或是列数，若不指定文件名称、或是所给予的文件名为”-“，则 wc 指令会从标准输入设备读取数据。 文本传输协议FTP：文本传输协议。用于 Internet 上的控制文件的双向传输。TCP/IP 协议中，FTP 标准命令 TCP 端口号为21，Port 方式数据端口为20。FTP 的任务是从一台计算机将文件传送到另一台计算机，不受操作系统的限制。 SSH：安全外壳协议。专为远程登录会话和其他网络服务提供安全性的协议，利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。有基于口令的安全验证和基于秘钥的安全验证两种方式。默认运行端口是22。 HTTP：超文本传输协议。是客户端和服务器端请求和应答的标准。默认端口是80。 Telnet：是Internet远程登陆服务的标准协议和主要方式。为用户提供了在本地计算机上完成远程主机工作的能力，Telnet是常用的远程控制Web服务器的方法，默认端口是23。 HTTPS：以安全为目标的HTTP通道，简单讲是HTTP的安全版。HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。默认端口是443。 http与https的区别超文本传输协议 HTTP 协议被用于在 Web 浏览器和网站服务器之间传递信息。HTTP 协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此 HTTP 协议不适合传输一些敏感信息，比如信用卡号、密码等。 为了解决 HTTP 协议的这一缺陷，需要使用另一种协议：安全套接字层超文本传输协议HTTPS。为了数据传输的安全， HTTPS 在 HTTP 的基础上加入了 SSL 协议，SSL 依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密。 HTTPS和HTTP的区别主要为以下四点： https 协议需要到 ca 申请证书，一般免费证书很少，需要交费。 http 是超文本传输协议，信息是明文传输，https 则是具有安全性的 SSL 加密传输协议。 http 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 http 的连接很简单，是无状态的；HTTPS 协议是由 SSL+HTTP 协议构建的可进行加密传输、身份认证的网络协议，比http 协议安全。 SSH与Telnet的区别 Telnet 是明文传送，SSH是加密的且支持压缩； SSH 服务一般都提供 sftp 支持，支持文件传送。telnet一般只能通过zmodem等协议传送文件; SSH 还可以借助 SSH 连接建立 TCP 通道，映射远端或本地的端口，以及转发X到本地X Server等； 在使用SSH 的时候，一个数字证书将认证客户端(你的工作站)和服务器(你的网络设备)之间的连接，并加密受保护的口令。SSH1 使用 RSA 加密密钥，SSH2 使用数字签名算法(DSA)密钥保护连接和认证。加密算法包括Blowfish，数据加密标准(DES)，以及三重 DES(3DES)。SSH 保护并且有助于防止欺骗，“中间人”攻击，以及数据包监听。 单例设计模式设计模式考察：请用单态设计模式方法设计类满足如下要求：请用PHP5代码编写类实现在每次对数据库连接的访问中都只能获取唯一的一个数据库连接，具体拦截数据库的详细代码忽略，请写出主要逻辑代码。 12345678910111213141516171819class Database&#123; static public $_instance; private function __construct()&#123; // 连接数据库 &#125; static function getInstance()&#123; if (!self::$_instance) self::$_instance = new self(); return self::$_instance; &#125; private function __clone() &#123; // 私有化clone方法 &#125; public function query($sql) &#123; // 查询 &#125;&#125; Session和Cookie的区别 Cookie 数据存放在客户的浏览器上，需要客户端支持Cookie，Session 数据放在服务器上; Cookie 不是很安全，别人可以分析存放在本地的 Cookie 并进行 Cookie 欺骗，考虑到安全应当使用session； Session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能，考虑到减轻服务器性能方面，应当使用 Cookie ； 单个 Cookie 保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个Cookie； Session 中存储的是对象引用，Cookie 中存放的是字符串； Session 不能区分路径，同一个用户在访问一个网站期间，所有的Session 在任何一个地方都可以访问到。而Cookie 中如果设置了路径参数，那么同一个网站中不同路径下的Cookie 互相是访问不到的。 include和require的区别 include() 在执行文件时每次都要进行读取和评估；require() 文件只处理一次(实际上文件内容替换了require()语句)； require() 通常放在 PHP 脚本程序的最前面 include() 的使用和 require() 不一样，一般放在流程控制的处理区段中，PHP脚本文件读到 include() 语句时，才将它包含的文件读进来，这种方式,可以把程序执行时的流程简单化； require() 和 include() 语句是语言结构，不是真正的函数，可以像PHP的其他语言结构一样； require() 包含文件失败，停止执行，给出错误(致命的)； include() 常用于动态包含，通常是自动加载的文件，即使加载出错,整个程序还是继续执行一个页面声明，另一个页面调用包函文件失败，继续向下执行，返回一条警告； include_once() 和 require_once() 语句也是在脚本执行期间包括并运行指定文件，与 include() require() 唯一的区别是如果文件中的代码已经被包括了，则不会再次包括。 PHP处理上传文件的流程流程 首先用户在浏览器端选择上传的文件；提交后，通过post方式上传到Apache服务器；然后由 PHP 引擎处理判断文件是否能够上传到 PHP 配置文件中指定的临时目录；之后获取文件后缀名判断文件是否是允许上传的文件格式；如果没问题，则按照随机数+时间的方式生成文件的名字+后缀；最后将文件从临时目录转移至Apache服务器目录。 限制上传文件大小 在 PHP 配置文件中 有一个 upload_max_filesize 设置其值的大小就可以限制上传文件大小； 在处理上传的 PHP 代码中限制，$_FILES[‘name’][‘size’]。]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP关键字之self,final,static,this,parent]]></title>
    <url>%2F2017%2F04%2F08%2Fphpkeywords1%2F</url>
    <content type="text"><![CDATA[最近在看PHP手册，看到类和对象的时候，有一个“后期静态绑定”的内容，self，static，parent这几个关键字弄的我云里雾里，索性查了一些资料，看看self，final，static，this，parent这些关键字到底是什么意思。 官方文档解释：后期静态绑定 自 PHP 5.3.0 起，PHP 增加了一个叫做后期静态绑定的功能，用于在继承范围内引用静态调用的类。 准确说，后期静态绑定工作原理是存储了在上一个“非转发调用”（non-forwarding call）的类名。当进行静态方法调用时，该类名即为明确指定的那个（通常在 :: 运算符左侧部分）；当进行非静态方法调用时，即为该对象所属的类。所谓的“转发调用”（forwarding call）指的是通过以下几种方式进行的静态调用：self::，parent::，static:: 以及 forward_static_call()。可用 get_called_class() 函数来得到被调用的方法所在的类名，static:: 则指出了其范围。 该功能从语言内部角度考虑被命名为“后期静态绑定”。“后期绑定”的意思是说，static:: 不再被解析为定义当前方法所在的类，而是在实际运行时计算的。也可以称之为“静态绑定”，因为它可以用于（但不限于）静态方法的调用。 因为存在继承和不存在继承的情况下，self，static，parent关键字的作用不同，而this和final是不受继承限制的，我们先来看看this，final。 关键字this1234567891011121314151617181920212223242526272829303132class name //建立了一个名为name的类&#123; private $name; //定义属性，私有 //定义构造函数，用于初始化赋值 function __construct($name) &#123; $this-&gt;name = $name; //这里已经使用了this指针语句(1) &#125; //析构函数 function __destruct() &#123; &#125; //打印用户名成员函数 function print_name() &#123; print($this-&gt;name); //再次使用了this指针语句(2) &#125;&#125;$obj1 = new name(&quot;PHP_Home&quot;); //实例化对象 语句(3)//执行打印$obj1-&gt;print_name(); //输出:PBPHome//第二次实例化对象$obj2 = new name(&quot;PHP&quot;);//执行打印$obj2-&gt;print_name(); //输出：PHP Note：上面的类分别在语句(1)和语句(2)使用了 this 指针，那么当时 this 是指向谁呢？其实 this 是在实例化的时候来确定指向谁，比如第一次实例化对象的时候（语句(3)），那么当时 this 就是指向 \$obj1 对象，那么执行语句(2)的打印时就把 1print($this-&gt;name) 变成了 1print($obj1-&gt;name) 那么当然就输出了”PHP_Home”。 第二个实例的时候， 1print($this-&gt;name ) 变成了 1print( $obj2-&gt;name) 于是就输出了”PHP”。所以说，this 是指向当前对象实例的指针，不指向任何其他对象或类。 关键字final如果父类中的方法被声明为 final，则子类无法覆盖该方法。如果一个类被声明为 final，则不能被继承。 final 方法示例： 1234567891011121314151617class BaseClass &#123; public function test() &#123; echo &quot;BaseClass::test() called\n&quot;; &#125; final public function moreTesting() &#123; echo &quot;BaseClass::moreTesting() called\n&quot;; &#125;&#125;class ChildClass extends BaseClass &#123; public function moreTesting() &#123; echo &quot;ChildClass::moreTesting() called\n&quot;; &#125;&#125;// 产生Fatal error: Cannot override final method BaseClass::moreTesting() final 类示例： 123456789101112131415final class BaseClass &#123; public function test() &#123; echo &quot;BaseClass::test() called\n&quot;; &#125; // 这里无论你是否将方法声明为final，都没有关系 final public function moreTesting() &#123; echo &quot;BaseClass::moreTesting() called\n&quot;; &#125;&#125;class ChildClass extends BaseClass &#123;&#125;// 产生 Fatal error: Class ChildClass may not inherit from final class (BaseClass) Note：属性不能被定义为 final，只有类和方法才能被定义为 final。 下面我们在存在继承和不存在继承两种情况下看看self，static，parent关键字的作用。 不存在继承的时候顾名思义，不存在继承就是书写一个单独的类来使用。self 和 static 在范围解析操作符 （::） 的使用上，并无区别。 在静态函数中，self 和 static 可以调用静态属性和静态函数（没有实例化类，因此不能调用非静态的属性和函数）。在非静态函数中，self 和 static 可以调用静态属性和静态函数以及非静态函数。不存在继承的情况下，self 和 static 的作用是一样的，同时可替换为 类名:: 的方式调用。 1234567891011121314151617181920212223242526272829303132class Demo &#123; public static $static; public $Nostatic; public function __construct() &#123; self::$static = &quot;static&quot;; $this-&gt;Nostatic = &quot;Nostatic&quot;; &#125; public static function get() &#123; return __CLASS__; &#125; public function show() &#123; return &quot;this is function show with &quot; . $this-&gt;Nostatic; &#125; public function test() &#123; echo Demo::$static . &quot;&lt;br/&gt;&quot;; //使用类名调用静态属性 echo Demo::get() . &quot;&lt;br/&gt;&quot;; //使用类名调用静态属性 echo Demo::show() . &quot;&lt;br/&gt;&quot;; //使用类名调用静态属性 echo self::$static . &quot;&lt;br/&gt;&quot;; //self调用静态属性 echo self::show() . &quot;&lt;br/&gt;&quot;; //self调用非静态方法 echo self::get() . &quot;&lt;br/&gt;&quot;; //self调用静态方法 echo static::$static . &quot;&lt;br/&gt;&quot;; //static调用静态属性 echo static::show() . &quot;&lt;br/&gt;&quot;; //static调用非静态方法 echo static::get() . &quot;&lt;br/&gt;&quot;; //static调用静态方法 &#125;&#125;$obj = new Demo();$obj-&gt;test(); 输出结果为： 123456789staticDemothis is function show with Nostaticstaticthis is function show with NostaticDemostaticthis is function show with NostaticDemo 存在继承的时候示例1： 12345678910111213141516171819202122class A &#123; static function getClassName() &#123; return &quot;this is class A&quot;; &#125; static function testSelf() &#123; echo self::getClassName(); &#125; static function testStatic() &#123; echo static::getClassName(); &#125;&#125;class B extends A &#123; static function getClassName() &#123; return &quot;this is class B&quot;; &#125;&#125;B::testSelf();B::testStatic(); 输出结果为： 12this is class Athis is class B Note：self 调用的静态方法或属性始终表示其在使用的时候的当前类（A）的方法或属性，可以替换为其类名，static 调用的静态方法或属性会在继承中被其子类重写覆盖，应该替换为对应的子类名（B）。 对于parent关键字，用于调用父类的方法和属性。在静态方法中，可以调用父类的静态方法和属性；在非静态方法中，可以调用父类的方法和属性。 12345678910111213141516171819202122232425262728293031class A &#123; public static $static; public $Nostatic; public function __construct() &#123; self::$static = &quot;static&quot;; $this-&gt;Nostatic = &quot;Nostatic&quot;; &#125; public static function staticFun() &#123; return self::$static; &#125; public function noStaticFun() &#123; return &quot;this is function show with &quot; . $this-&gt;Nostatic; &#125;&#125;class B extends A &#123; static function testS() &#123; echo parent::staticFun(); &#125; function testNoS() &#123; echo parent::noStaticFun(); &#125;&#125;$obj = new B();$obj-&gt;testS();$obj-&gt;testNoS(); 输出结果为： 12staticthis is function show with Nostatic PHP手册中的例子1234567891011121314151617181920212223242526272829class A &#123; public static function foo() &#123; static::who(); &#125; public static function who() &#123; echo __CLASS__ . &quot;\n&quot;; &#125;&#125;class B extends A &#123; public static function test() &#123; A::foo(); parent::foo(); self::foo(); &#125; public static function who() &#123; echo __CLASS__ . &quot;\n&quot;; &#125;&#125;class C extends B &#123; public static function who() &#123; echo __CLASS__ . &quot;\n&quot;; &#125;&#125;C::test(); 输出结果为： 123ACC 我们分析 test() 方法： 12345public static function test() &#123; A::foo(); parent::foo(); self::foo();&#125; A::foo()；这个语句是可以在任何地方执行的，它表示使用A去调用静态方法 foo() 得到“A”。 parent::foo()；C 的 parent 是 B ， B 的parent是A，回溯找到了A 的 foo 方法；static::who()；语句中的 static:: 调用的方法会被子类覆盖，所以优先调用 C 的 who() 方法，如果 C 的 who 方法不存在会调用 B 的 who 方法，如果 B 的 who 方法不存在会调用 A 的 who 方法。所以，输出结果是”C”。 self::foo()；这个 self:: 是在 B 中使用的，所以 self:: 等价于 B:: ，但是 B 没有实现 foo() 方法，B 又继承自 A，所以我们实际上调用了 A::foo() 这个方法。foo() 方法使用了static::who() 语句，导致我们又调用了 C 的 who 函数。 参考文章： PHP中this,self,parent的区别 Final 关键字 后期静态绑定 PHP中的self、static、parent关键字]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>self</tag>
        <tag>final</tag>
        <tag>this</tag>
        <tag>parent</tag>
        <tag>static</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[约瑟夫环问题]]></title>
    <url>%2F2017%2F04%2F06%2Fjosephring%2F</url>
    <content type="text"><![CDATA[约瑟夫环（约瑟夫问题）是一个数学的应用问题：已知n个人（以编号1，2，3…n分别表示）围坐在一张圆桌周围。从编号为k的人开始报数，数到m的那个人出列；他的下一个人又从1开始报数，数到m的那个人又出列；依此规律重复下去，直到圆桌周围的人全部出列（或者求最后一个出列的人的编号）。 这里提供了三种方法： 逻辑去除法； 递归法； 线性表法。 这个问题我觉得应该也可以用循环队列来解决，这种方法以后再加上。 逻辑去除法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 逻辑去除法 * @param $n * @param $m * @return int */ public function ExJosephRing_Logic($n, $m) &#123; $tmp_array = array(); // $left_over记录数组中不为0的个数 $left_over = $n; // $ret_value记录最后返回的数字 $ret_value = 0; // 记录开始循环查找的起点 $start = 0; // 初始化数组 for ($i = 0; $i &lt;= $n; $i++) &#123; $tmp_array[$i] = $i; &#125; $array_length = count($tmp_array); while ($left_over != 1) &#123; for ($j = 1; $j &lt;= $m; $j++) &#123; $start++; // $tmp_array[$start] == 0检测数组越界 if ($start != $array_length &amp;&amp; $tmp_array[$start] == 0) &#123; for ($k = $start; $k &lt; $array_length; $k++) &#123; if ($tmp_array[$k] == 0) &#123; $start++; &#125; elseif ($tmp_array[$k] &gt; 0) &#123; break; &#125; &#125; &#125; // 循环到头，重新开始 if ($start == $array_length) &#123; $start = 0; &#125; if ($tmp_array[$start] == 0) &#123; for ($k = $start; $k &lt; $array_length; $k++) &#123; if ($tmp_array[$k] == 0) &#123; $start++; &#125; elseif ($tmp_array[$k] &gt; 0) &#123; break; &#125; &#125; &#125; &#125; $tmp_array[$start] = 0; // 循环遍历，记录数组中不为0的元素的个数 $left_over = 0; foreach ($tmp_array as $key =&gt; $value) &#123; if ($value != 0) &#123; $left_over++; $ret_value = $value; &#125; &#125; &#125; return $ret_value; &#125; 递归方法1234567891011121314151617181920212223/** * 递归方法 * @param $tmp_array * @param $m * @param int $current * @return int */ public function ExJosephRing_Recursive($tmp_array, $m, $current = 0) &#123; $array_length = count($tmp_array); $num = 1; if (count($tmp_array) == 1) &#123; return $tmp_array[0]; &#125; else &#123; while ($num++ &lt; $m) &#123; $current++; $current = $current % $array_length; &#125; array_splice($tmp_array, $current, 1); // 如果需要返回数据，这里必须加上return，否则不会返回结果 return $this-&gt;ExJosephRing_Recursive($tmp_array, $m, $current); &#125; &#125; 线性表法1234567891011121314/** * 线性表法 * @param $n * @param $m * @return int */ public function ExJosephRing_Linear($n, $m) &#123; $r = 0; for ($i = 2; $i &lt;= $n; $i++) &#123; $r = ($r + $m) % $i; &#125; return $r + 1; &#125; 对于线性表法的解释：每个人出列后，剩下的人又组成了另一个子问题。只是他们的编号变化了。第一个出列的人肯定是a[1] = m(mod)n（m/n的余数），他除去后剩下的人编号是a[1]+1、a[1]+2、…、n、1、2、…a[1]-2、a[1]-1，对应的新编号是1，2，3…n-1。设此时某个人的新编号是i，他原来的编号就是(i+a[1])%n。于是，这便形成了一个递归问题。假如知道了这个子问题（n-1个人）的解是x，那么原问题（n个人）的解便是：(x+m%n)%n = (x+m)%n。问题的起始条件：如果n=1,那么结果就是1。 参考文章： 用PHP解决“约瑟夫环”的几种方法]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>约瑟夫环</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP常用状态码详解]]></title>
    <url>%2F2017%2F04%2F04%2Fhttpstatecode%2F</url>
    <content type="text"><![CDATA[HTTP状态码（HTTP Status Code）是用以表示网页服务器HTTP响应状态的3位数字代码。它由 RFC 2616 规范定义的，并得到RFC 2518、RFC 2817、RFC 2295、RFC 2774、RFC 4918等规范扩展。 类别 说明 1xx 请求收到，继续处理 2xx 操作成功收到，分析、接受 3xx 完成此请求必须进一步处理 4xx 请求包含一个错误语法或不能完成 5xx 服务器执行一个完全有效请求失败 1xx：请求收到，继续处理 状态码 说明 100 客户端必须继续发送请求 101 服务器已经理解了客户端的请求，并将通过Upgrade 消息头通知客户端采用不同的协议来完成这个请求 102 处理将被继续执行 2xx：操作成功收到，分析、接受 状态码 说明 200 请求已成功，请求所希望的响应头或数据体将随此响应返回 201 请求已经被实现，而且有一个新的资源已经依据请求的需要而建立，且其 URI 已经随Location 头信息返回 202 服务器已接受请求，但尚未处理 203 返回信息不确定或不完整 204 请求收到，但返回信息为空 205 服务器完成了请求，用户代理必须复位当前已经浏览过的文件 206 服务器已经完成了部分用户的GET请求 207 之后的消息体将是一个XML消息，并且可能依照之前子请求数量的不同，包含一系列独立的响应代码 3xx：完成此请求必须进一步处理 状态码 说明 300 请求的资源可在多处得到 301 删除请求数据 302 在其他地址发现了请求数据 303 建议客户访问其他URL或访问方式 304 客户端已经执行了GET，但文件未变化 305 请求的资源必须从服务器指定的地址得到 306 前一版本HTTP中使用的代码，现行版本中不再使用 307 申明请求的资源临时性删除 4xx：请求包含一个错误语法或不能完成 状态码 说明 400 错误请求，如语法错误 401 请求授权失败 402 保留有效ChargeTo头响应 403 请求不允许 404 没有发现文件、查询或URl 405 用户在Request-Line字段定义的方法不允许 406 根据用户发送的Accept拖，请求资源不可访问 407 类似401，用户必须首先在代理服务器上得到授权 408 客户端没有在用户指定的饿时间内完成请求 409 对当前资源状态，请求不能完成 410 服务器上不再有此资源且无进一步的参考地址 411 服务器拒绝用户定义的Content-Length属性请求 412 一个或多个请求头字段在当前请求中错误 413 请求的资源大于服务器允许的大小 414 请求的资源URL长于服务器允许的长度 415 请求资源不支持请求项目格式 416 请求中包含Range请求头字段，在当前请求资源范围内没有range指示值，请求也不包含If-Range请求头字段 417 服务器不满足请求Expect头字段指定的期望值，如果是代理服务器，可能是下一级服务器不能满足请求 422 请求格式正确，但是由于含有语义错误，无法响应 423 当前资源被锁定 424 由于之前的某个请求发生的错误，导致当前请求失败 425 在WebDav Advanced Collections 草案中定义，但是未出现在《WebDAV 顺序集协议》（RFC 3658）中 426 客户端应当切换到TLS/1.0 427 请求应当在执行完适当的操作后进行重试 5xx：服务器执行一个完全有效请求失败 状态 说明 500 服务器产生内部错误 501 服务器不支持请求的函数 502 服务器作为网关或代理，从上游服务器收到无效响应 503 服务器目前无法使用(由于超载或停机维护) 504 关口过载，服务器使用另一个关口或服务来响应用户，等待时间设定值较长 505 服务器不支持或拒绝支请求头中指定的HTTP版本 506 服务器存在内部配置错误 507 服务器无法存储完成请求所必须的内容 509 服务器达到带宽限制 510 获取资源所需要的策略并没有没满足 参考文章： HTTP 返回状态值详解]]></content>
      <categories>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
        <tag>状态码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程与线程的关系]]></title>
    <url>%2F2017%2F04%2F04%2Fprocessthread%2F</url>
    <content type="text"><![CDATA[写这篇文章是想总结一下在操作系统中，进程和线程到底是什么样的，它们之间有什么区别和联系，虽然在实践中使用过多线程和多进程，但也只是了解了皮毛，故在此总结一下，因为操作系统是一个比较复杂的东西，通过进程和线程可以推演出各种复杂的概念和情况，所以文章只是在现有水平下的一点解释。 从操作系统说起操作系统 (Operating System，OS) 是管理和控制计算机硬件与软件资源的计算机程序，是直接运行在“裸机”上的最基本的系统软件，任何其他软件都必须在操作系统的支持下才能运行。 操作系统的功能主要包括管理计算机系统的硬件、软件及数据资源，控制程序运行，改善人机界面，为其它应用软件提供支持，让计算机系统所有资源最大限度地发挥作用，提供各种形式的用户界面，使用户有一个好的工作环境，为其它软件的开发提供必要的服务和相应的接口等。 实际上，用户是不用接触操作系统的，操作系统管理着计算机硬件资源，同时按照应用程序的资源请求，分配资源，如：划分CPU时间，内存空间的开辟，调用打印机等。 我们的计算机由硬件和软件两个部分组成。硬件部分包括控制器、运算器、存储器、输入设备、输出设备；软件部分包括系统软件和应用软件。而在计算机运行过程中最最重要的就是控制器和运算器，他们构成了计算机的核心：CPU。简而言之，CPU+RAM+各种资源 (比如显卡，光驱，键盘，GPS, 等等外设) 构成我们的电脑，但是电脑的运行，实际就是CPU和相关寄存器以及RAM之间的事情。 目前计算机的软硬件发展太快，CPU的运行速度也是直线上升，而寄存器仅仅能够追的上他的脚步，RAM和别的挂在各总线上的设备完全是望其项背。那当多个任务要执行的时候怎么办呢？轮流着来？或者谁优先级高谁来？不管怎么样的策略，一句话就是：在CPU看来就是轮流着来。 搞清楚几个概念进程 是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是操作系统结构的基础，是一个动态的概念； 线程是程序执行流的最小单元。一个标准的线程由线程ID，当前指令指针PC，寄存器集合和堆栈组成。另外，线程是进程中的一个实体，是被系统独立调度和分派的基本单位，线程自己不拥有系统资源，它可与同属一个进程的其它线程共享进程所拥有的全部资源。一个线程可以创建和撤消另一个线程，同一进程中的多个线程之间可以并发执行； 程序只是一组指令的有序集合，是一个静态的概念； 任务是指由软件完成的一个活动。一个任务既可以是一个进程，也可以是一个线程。它指的是一系列共同达到某一目的的操作。例如，读取数据并将数据放入内存中。这个任务可以作为一个进程来实现，也可以作为一个线程来实现。 任务调度目前大部分的操作系统的任务调度是采用时间片轮转的抢占式调度方式，也就是说一个任务执行一小段时间后强制暂停去执行下一个任务，每个任务轮流执行。 任务执行的一小段时间叫做时间片，任务正在执行时的状态叫运行状态，任务执行一段时间后强制暂停去执行下一个任务，被暂停的任务就处于就绪状态，等待下一个属于它的时间片的到来。这样每个任务都能得到执行，由于CPU的执行效率非常高，时间片非常短，在各个任务之间快速地切换，给人的感觉就是多个任务在“同时进行”，这也就是我们所说的并发。多任务运行过程的示意图如下： 当执行一段程序代码，实现一个功能的过程时，得到CPU时间片的时候，相关的资源必须也已经就位，比如显卡等资源，然后CPU开始执行。这时除了CPU以外所有的其他资源就构成了这个程序的执行环境，也就是我们所说的程序上下文。当这个程序执行完了，或者分配给他的CPU时间片用完了，那它就要被切换出去，等待下一次CPU的临幸。在被切换出去的最后一步工作就是保存程序上下文，因为这个是下次他被CPU临幸的运行环境，必须保存。 具体的轮流执行方法就是：先加载程序 A 的上下文，然后开始执行 A ，保存程序 A 的上下文，调入下一个要执行的程序 B 的程序上下文，然后开始执行 B ，保存程序 B 的上下文……循环执行。 进程计算机的核心是CPU，它承担了所有的计算任务；而操作系统是计算机的管理者，它负责任务的调度、资源的分配和管理，统领整个计算机硬件；应用程序侧是具有某种功能的程序，程序是运行于操作系统之上的。 进程一般由程序、数据集合和进程控制块三部分组成。程序用于描述进程要完成的功能，是控制进程执行的指令集；数据集合是程序在执行时所需要的数据和工作区；程序控制块 (PCB) 包含进程的描述信息和控制信息，是进程存在的唯一标志。 程序并不能单独运行，只有将程序装载到内存中，系统为它分配资源才能运行，而这种执行的程序就称之为进程。 进程具有的特征：动态性：进程是程序的一次执行过程，是临时的，有生命期的，是动态产生，动态消亡的；并发性：任何进程都可以同其他进程一起并发执行；独立性：进程是系统进行资源分配和调度的一个独立单位；结构性：进程由程序、数据和进程控制块三部分组成。 线程在早期的操作系统中并没有线程的概念，进程是能拥有资源和独立运行的最小单位，也是程序执行的最小单位。任务调度采用的是时间片轮转的抢占式调度方式，而进程是任务调度的最小单位，每个进程有各自独立的一块内存，使得各个进程之间内存地址相互隔离。 随着计算机的发展，对 CPU 的要求越来越高，进程之间的切换开销较大，已经无法满足越来越复杂的程序的要求了。于是就发明了线程，线程是程序执行中一个单一的顺序控制流程，是程序执行流的最小单元，是处理器调度和分派的基本单位。一个进程可以有一个或多个线程，各个线程之间共享程序的内存空间（也就是所在进程的内存空间）。一个标准的线程由线程ID、当前指令指针(PC)、寄存器和堆栈组成。而进程由内存空间（代码、数据、进程空间、打开的文件）和一个或多个线程组成。 为什么需要线程 进程只能在一个时间干一件事，如果想同时干两件事或多件事，进程就无能为力了。 进程在执行的过程中如果阻塞，例如等待输入，整个进程就会挂起，即使进程中有些工作不依赖于输入的数据，也将无法执行。 线程的优点 进程属于在处理器这一层上提供的抽象；线程则属于在进程这个层次上再提供了一层并发的抽象。如果我们进入计算机体系结构里，就会发现，流水线提供的也是一种并发，不过是指令级的并发。这样，流水线、线程、进程就从低到高在三个层次上提供我们所迫切需要的并发。 除了提高进程的并发度，线程还有个好处，就是可以有效地利用多处理器和多核计算机。现在的处理器有个趋势就是朝着多核方向发展，在没有线程之前，多核并不能让一个进程的执行速度提高，原因还是上面所有的两点限制。但如果讲一个进程分解为若干个线程，则可以让不同的线程运行在不同的核上，从而提高了进程的执行速度。 例如：我们经常使用微软的Word进行文字排版，实际上就打开了多个线程。这些线程一个负责显示，一个接受键盘的输入，一个进行存盘等等。这些线程一起运行，让我们感觉到我们输入和屏幕显示同时发生，而不是输入一些字符，过一段时间才能看到显示出来。在我们不经意间，还进行了自动存盘操作。这就是线程给我们带来的方便之处。 进程间通信无名管道通信管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。 高级管道通信将另一个程序当做一个新的进程在当前程序进程中启动，则它算是当前程序的子进程，这种方式我们成为高级管道方式。 有名管道通信有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。 消息队列通信消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 信号量通信信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 信号信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。 共享内存通信共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量，配合使用，来实现进程间的同步和通信。 套接字通信 套接口也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同机器间的进程通信。 线程间通信线程间的通信目的主要是用于线程同步，所以线程没有像进程通信中的用于数据交换的通信机制。 互斥锁提供了以排他方式阻止数据结构被并发修改的方法。 条件变量可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。 读写锁允许多个线程同时读共享数据，而对写操作时互斥的。 信号量机制包括无名线程信号量和命名线程信号量。 信号类似进程间的信号处理。 进程与线程的区别一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程。 资源分配给进程，同一进程的所有线程共享该进程的所有资源。 处理机分给线程，即真正在处理机上运行的是线程。 线程在执行过程中，需要协作同步。不同进程的线程间要利用消息通信的办法实现同步。 划分尺度：线程更小，所以多线程程序并发性更高； 资源分配：进程是资源分配的基本单位，同一进程内多个线程共享其资源； 地址空间：进程拥有独立的地址空间，同一进程内多个线程共享其资源； 处理器调度：线程是处理器调度的基本单位； 执行：每个线程都有一个程序运行的入口，顺序执行序列和程序的出口，但线程不能单独执行，必须组成进程，一个进程至少有一个主线程。 调度和切换：线程上下文切换比进程上下文切换要快得多； 进程与程序的区别程序只是一组指令的有序集合，它本身没有任何运行的含义，它只是一个静态的实体。而进程则不同，它是程序在某个数据集上的执行。 进程是一个动态的实体，它有自己的生命周期。它因创建而产生，因调度而运行，因等待资源或事件而被处于等待状态，因完成任务而被撤消。反映了一个程序在一定的数据集上运行的全部动态过程。 进程和程序并不是一一对应的，一个程序执行在不同的数据集上就成为不同的进程，可以用进程控制块来唯一地标识每个进程。而这一点正是程序无法做到的，由于程序没有和数据产生直接的联系，既使是执行不同的数据的程序，他们的指令的集合依然是一样的，所以无法唯一地标识出这些运行于不同数据集上的程序。一般来说，一个进程肯定有一个与之对应的程序，而且只有一个。而一个程序有可能没有与之对应的进程（因为它没有执行），也有可能有多个进程与之对应（运行在几个不同的数据集上）。 进程还具有并发性和交往性，这也与程序的封闭性不同。进程和线程都是由操作系统所体会的程序运行的基本单元，系统利用该基本单元实现系统对应用的并发性。 一个程序至少有一个进程，一个进程至少有一个线程。 最后给初学者推荐一篇文章，进程与线程的一个简单解释 参考文章： 操作系统 计算机组成 线程 进程 线程和进程的区别是什么？ 进程间8种通信方式详解 程序、任务、进程和线程的联系与区别 操作系统专题——进程与线程的区别 进程与线程的一个简单解释]]></content>
      <categories>
        <category>进程/线程</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>进程</tag>
        <tag>线程</tag>
        <tag>程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP/IP协议入门篇]]></title>
    <url>%2F2017%2F04%2F03%2Ftcpip%2F</url>
    <content type="text"><![CDATA[写这篇文章的主要动机是，自己在做项目的时候用到了网络编程的内容，然后自己简单学习了一下TCP/IP的东西，属于很基础的那种，在实习面试中，面试官也是问到了这块内容，但是自己理解比较浅，所以回答的也不是很好。想着系统的学习一下网络编程中TCP/IP协议这块，本篇作为入门，简单介绍一下TCP/IP协议。 名词解释TCP/IP：Transmission Control Protocol/Internet Protocol 的简写，中译名为传输控制协议/因特网互联协议，又名网络通讯协议，是Internet最基本的协议、Internet国际互联网络的基础，由网络层的IP协议和传输层的TCP协议组成。是一种面向连接的、可靠的、基于字节流的传输层（Transport layer）通信协议。是专门为了在不可靠的互联网络上提供一个可靠的端到端字节流而设计的。互联网络与单个网络不同，因为互联网络的不同部分可能有着截然不同的拓扑、带宽、延迟、分组大小和其他参数。TCP的设计目标是能够动态的适应互联网络的这些特性，而且当面对多种失败的时候仍然能够健壮。 TCP/IP协议不是TCP和IP这两个协议的合称，而是指因特网整个TCP/IP协议族。 参考模型了解TCP/IP参考模型之前，先来看一下OSI模型。 OSI参考模型OSI模型是ISO的建议，是为了使各层上的协议国际标准化而发展起来的。OSI参考模型全称是开放系统互连参考模型(Open System Interconnection Reference Model)。这一参考模型共分为七层：物理层、数据链路层、网络层、传输层、会话层、表示层和应用层。 层次 说明 物理层 处理机械的、电气的和过程的接口，以及物理层下的物理传输介质等 数据链路层 加强物理层的功能，使其对网络层显示为一条无错的线路 网络层 确定分组从源端到目的端的路由选择。路由可以选用网络中固定的静态路由表，也可以在每一次会话时决定，还可以根据当前的网络负载状况，灵活地为每一个分组分别决定。 传输层 从会话层接收数据，并传输给网络层，同时确保到达目的端的各段信息正确无误，而且使会话层不受硬件变化的影响 会话层 允许不同机器上的用户之间建立会话关系，既可以进行类似传输层的普通数据传输，也可以被用于远程登录到分时系统或在两台机器间传递文件 表示层 用于完成一些特定的功能，这些功能由于经常被请求，因此人们希望有通用的解决办法，而不是由每个用户各自实现 应用层 包含了大量人们普遍需要的协议。不同的文件系统有不同的文件命名原则和不同的文本行表示方法等，不同的系统之间传输文件还有各种不兼容问题，这些都将由应用层来处理。此外，应用层还有虚拟终端、电子邮件和新闻组等各种通用和专用的功能 TCP/IP参考模型TCP/IP参考模型共分为四层：网络访问层、互联网层、传输层和应用层。 层次 说明 应用层 包含所有的高层协议，包括：虚拟终端协议(TELNET)、文件传输协议(FTP)、电子邮件传输协议(SMTP)、域名服务(DNS)、网上新闻传输协议(NNTP)和超文本传送协议(HTTP)等 传输层 源端和目的端机器上的对等实体可以进行会话。在这一层定义了两个端到端的协议：传输控制协议(TCP)和用户数据报协议(UDP)。TCP是面向连接的协议，它提供可靠的报文传输和对上层应用的连接服务。为此，除了基本的数据传输外，它还有可靠性保证、流量控制、多路复用、优先权和安全性控制等功能。UDP是面向无连接的不可靠传输的协议，主要用于不需要TCP的排序和流量控制等功能的应用程序。 互联网层 整个体系结构的关键部分，其功能是使主机可以把分组发往任何网络，并使分组独立地传向目标，TCP/IP参考模型的互联网层和OSI参考模型的网络层在功能上非常相似。 网络访问层 指出主机必须使用某种协议与网络相连 TCP/IP协议层次 链路层链路层是负责接收IP数据包并通过网络发送，或者从网络上接收物理帧，抽出IP数据包，交给IP层。 ARP是正向地址解析协议，通过已知的IP，寻找对应主机的MAC地址。RARP是反向地址解析协议，通过MAC地址确定IP地址。比如无盘工作站还有DHCP服务。 ARP、RARP属于网络层协议，工作在链路层 网络层负责相邻计算机之间的通信。其功能包括三方面。 处理来自传输层的分组发送请求，收到请求后，将分组装入IP数据报，填充报头，选择去往信宿机的路径，然后将数据报发往适当的网络接口。 处理输入数据报：首先检查其合法性，然后进行寻径–假如该数据报已到达信宿机，则去掉报头，将剩下部分交给适当的传输协议；假如该数据报尚未到达信宿，则转发该数据报。 处理路径、流控、拥塞等问题。 传输层提供应用程序间的通信。其功能包括：格式化信息流；提供可靠传输。 为实现可靠传输，传输层协议规定接收端必须发回确认，并且假如分组丢失，必须重新发送，即耳熟能详的“三次握手”过程，从而提供可靠的数据传输。 应用层向用户提供一组常用的应用程序，比如电子邮件、文件传输访问、远程登录等。远程登录TELNET使用TELNET协议提供在网络其它主机上注册的接口。TELNET会话提供了基于字符的虚拟终端。文件传输访问FTP使用FTP协议来提供网络内机器间的文件拷贝功能。 FTP 是文件传输协议，一般上传下载用FTP服务，数据端口是20H，控制端口是21H。 Telnet 服务是用户远程登录服务，使用23H端口，使用明码传送，保密性差、简单方便。 DNS 是域名解析服务，提供域名到IP地址之间的转换，使用端口53。 SMTP 是简单邮件传输协议，用来控制信件的发送、中转，使用端口25。 NFS 是网络文件系统，用于网络中不同主机间的文件共享。 HTTP 是超文本传输协议，用于实现互联网中的WWW服务，使用端口80。 TCP协议TCP头格式 端口号[16bit] 网络实现的是不同主机的进程间通信。在一个操作系统中，有很多进程，当数据到来时要提交给哪个进程进行处理呢？这就需要用到端口号。在TCP头中，有源端口号(Source Port)和目标端口号(Destination Port)。源端口号标识了发送主机的进程，目标端口号标识接受方主机的进程。 序号[32bit] 序号分为发送序号SN (Sequence Number)和确认序号ACK (Acknowledgment Number)。 发送序号：用来标识从 TCP源端向 TCP目的端发送的数据字节流，它表示在这个报文段中的第一个数据字节的顺序号。 如果将字节流看作在两个应用程序间的单向流动，则 TCP用顺序号对每个字节进行计数。序号是 32bit的无符号数，序号到达 2^32-1后又从 0开始。当建立一个新的连接时， SYN标志变 1，顺序号字段包含由这个主机选择的该连接的初始顺序号 ISN (Initial Sequence Number)。 确认序号：包含发送确认的一端所期望收到的下一个顺序号。因此，确认序号应当是上次已成功收到数据字节顺序号加 1。只有 ACK标志为 1时确认序号字段才有效。 TCP为应用层提供全双工服务，这意味数据能在两个方向上独立地进行传输。因此，连接的每一端必须保持每个方向上的传输数据顺序号。 偏移[4bit] 这里的偏移实际指的是TCP首部的长度，它用来表明TCP首部中32 bit字的数目，通过它可以知道一个TCP包它的用户数据是从哪里开始的。这个字段占4bit，如4bit的值是0101，则说明TCP首部长度是5 * 4 = 20字节。 所以TCP的首部长度最大为15 * 4 = 60字节。然而没有可选字段，正常长度为20字节。 Reserved [6bit] 目前没有使用，它的值都为0 标志[6bit] 在TCP首部中有6个标志比特。他们中的多个可同时被置为1 。 URG 紧急指针(urgent pointer)有效 ACK 确认序号有效 PSH 指示接收方应该尽快将这个报文段交给应用层而不用等待缓冲区装满 RST 一般表示断开一个连接 SYN 同步序号用来发起一个连接 FIN 发送端完成发送任务(即断开连接) 窗口大小(window)[16bit] 窗口的大小，表示源方法最多能接受的字节数。 校验和[16bit] 校验和覆盖了整个的TCP报文段:TCP首部和TCP数据。这是一个强制性的字段，一定是由发端计算和存储，并由收端进行验证。 紧急指针[16bit] 只有当URG标志置为1时紧急指针才有效。紧急指针是一个正的偏移量，和序号字段中的值相加表示紧急数据最后一个字节的序号。TCP的紧急方式是发送端向另一端发送紧急数据的一种方式。 TCP选项 是可选的。 三次握手建立连接 第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。 第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。 第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。 三次握手连接成功的标准客户端发完第三次ACK，就立马成为established状态，即使第三次ACK丢失，客户端并不关心，可以发送数据，格式为ACK+Data，当到达服务器时，服务器状态虽然为SYN_RCVD，它依然可以将Data缓存下来，客户端捎带过来的ACK，这个ACK就是对服务器SYN+ACK又一次确认，所以服务器端立马切换为established状态，然后将缓存下来的客户端Data提交给应用程序。 所以客户端只要发送了三次握手的ACK即认为自己为established状态，对应的就是connect成功返回，同时可以接受用户数据。 而服务器端没有收到客户端的ACK，accept依然为阻塞状态，直到由于客户端发送数据+ACK，或自己超时重传+ACK并接收到客户端ACK，accept才会返回。 TCP连接为什么需要三次握手在谢希仁著《计算机网络》第四版中讲“三次握手”的目的是“为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误”。在另一部经典的《计算机网络》一书中讲“三次握手”的目的是为了解决“网络中存在延迟的重复分组”的问题。这两种不用的表述其实阐明的是同一个问题。 谢希仁版《计算机网络》中的例子是这样的，“已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。” 在Google Groups的TopLanguage中看到一帖讨论TCP“三次握手”觉得很有意思。贴主提出“TCP建立连接为什么是三次握手？”的问题，在众多回复中，有一条回复写道：“这个问题的本质是, 信道不可靠, 但是通信双发需要就某个问题达成一致. 而要解决这个问题, 无论你在消息中包含什么信息, 三次通信是理论上的最小值. 所以三次握手不是TCP本身的要求, 而是为了满足”在不可靠信道上可靠地传输信息”这一需求所导致的. 请注意这里的本质需求,信道不可靠, 数据传输要可靠. 三次达到了, 那后面你想接着握手也好, 发数据也好, 跟进行可靠信息传输的需求就没关系了. 因此,如果信道是可靠的, 即无论什么时候发出消息, 对方一定能收到, 或者你不关心是否要保证对方收到你的消息, 那就能像UDP那样直接发送消息就可以了.”。这可视为对“三次握手”目的的另一种解答思路。 原文地址：TCP连接建立过程中为什么需要三次握手 四次挥手释放连接由于TCP连接时全双工的，因此，每个方向都必须要单独进行关闭，这一原则是当一方完成数据发送任务后，发送一个FIN来终止这一方向的连接，收到一个FIN只是意味着这一方向上没有数据流动了，即不会再收到数据了，但是在这个TCP连接上仍然能够发送数据，直到这一方向也发送了FIN。首先进行关闭的一方将执行主动关闭，而另一方则执行被动关闭。 第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。 第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。 第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。 第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。 TCP断开为什么需要四次挥手这是因为服务端在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方也未必全部数据都发送给对方了，所以己方可以立即close，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送。 TCP可靠性的保证TCP采用一种名为“带重传功能的肯定确认”的技术作为提供可靠数据传输服务的基础。这项技术要求接收方收到数据之后向源站回送确认信息ACK。发送方对发出的每个分组都保存一份记录，在发送下一个分组之前等待确认信息。发送方还在送出分组的同时启动一个定时器，并在定时器的定时期满而确认信息还没有到达的情况下，重发刚才发出的分组。 图3-5表示带重传功能的肯定确认协议传输数据的情况，图 3-6表示分组丢失引起超时和重传。 为了避免由于网络延迟引起迟到的确认和重复的确认，协议规定在确认信息中稍带一个分组的序号，使接收方能正确将分组与确认关联起来。 从图 3-5可以看出，虽然网络具有同时进行双向通信的能力，但由于在接到前一个分组的确认信息之前必须推迟下一个分组的发送，简单的肯定确认协议浪费了大量宝贵的网络带宽。为此， TCP使用滑动窗口的机制来提高网络吞吐量，同时解决端到端的流量控制。 滑动窗口技术滑动窗口技术是简单的带重传的肯定确认机制的一个更复杂的变形，它允许发送方在等待一个确认信息之前可以发送多个分组。如图 3-7所示，发送方要发送一个分组序列，滑动窗口协议在分组序列中放置一个固定长度的窗口，然后将窗口内的所有分组都发送出去；当发送方收到对窗口内第一个分组的确认信息时，它可以向后滑动并发送下一个分组；随着确认的不断到达，窗口也在不断的向后滑动。 TCP与UDP的区别基本区别 基于连接与无连接（TCP有连接，UDP无连接）； 对系统资源的要求（TCP较多，UDP少）； UDP程序结构较简单； 流模式与数据报模式（TCP属于流模式，UDP属于数据报模式）； TCP保证数据正确性，UDP可能丢包，TCP保证数据顺序，UDP不保证。 UDP应用场景 面向数据报方式； 网络数据大多为短消息 ； 拥有大量Client； 对数据安全性无特殊要求； 网络负担非常重，但对响应速度要求高。 TCP编程的服务器端一般步骤 创建一个socket，用函数socket()； 设置socket属性，用函数setsockopt()； * 可选 绑定IP地址、端口等信息到socket上，用函数bind()； 开启监听，用函数listen()； 接收客户端上来的连接，用函数accept()； 收发数据，用函数send()和recv()，或者read()和write(); 关闭网络连接； 关闭监听； TCP编程的客户端一般步骤 创建一个socket，用函数socket()； 设置socket属性，用函数setsockopt()；* 可选 绑定IP地址、端口等信息到socket上，用函数bind()；* 可选 设置要连接的对方的IP地址和端口等属性； 连接服务器，用函数connect()； 收发数据，用函数send()和recv()，或者read()和write()； 关闭网络连接； UDP编程的服务器端一般步骤 创建一个socket，用函数socket()； 设置socket属性，用函数setsockopt()；* 可选 绑定IP地址、端口等信息到socket上，用函数bind()； 循环接收数据，用函数recvfrom()； 关闭网络连接； UDP编程的客户端一般步骤 创建一个socket，用函数socket()； 设置socket属性，用函数setsockopt()；* 可选 绑定IP地址、端口等信息到socket上，用函数bind()；* 可选 设置对方的IP地址和端口等属性； 发送数据，用函数sendto()； 关闭网络连接； UDP补充 UDP不提供复杂的控制机制，利用IP提供面向无连接的通信服务。 它是将应用程序发来的数据在收到的那一刻，立刻按照原样发送到网络上的一种机制。即使是出现网络拥堵的情况下， UDP也无法进行流量控制等避免网络拥塞的行为。 传输途中如果出现了丢包，UDO也不负责重发。甚至当出现包的到达顺序乱掉时也没有纠正的功能。如果需要这些细节控制，那么不得不交给由采用UDO的应用程序去处理。换句话说，UDP将部分控制转移到应用程序去处理，自己却只提供作为传输层协议的最基本功能。 TCP补充 TCP充分实现了数据传输时各种控制功能，可以进行丢包的重发控制，还可以对次序乱掉的分包进行顺序控制。而这些在UDP中都没有。此外，TCP作为一种面向有连接的协议，只有在确认通信对端存在时才会发送数据，从而可以控制通信流量的浪费。TCP通过检验和、序列号、确认应答、重发控制、连接管理以及窗口控制等机制实现可靠性传输。 参考： TCP和UDP的最完整的区别 计算机网络入门基础篇 TCP/IP协议 tcp 编程中，connect 连接成功的标准是什么？ 如何通俗地解释一下 TCP/UDP 协议和 HTTP、FTP、SMTP 等协议之间的区别？ TCP、UDP、IP 协议分析]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-crontab]]></title>
    <url>%2F2017%2F04%2F02%2Flinuxcrontab%2F</url>
    <content type="text"><![CDATA[crond是linux下用来周期性的执行某种任务或等待处理某些事件的一个守护进程，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。 Linux下的任务调度Linux下的任务调度分为两类，系统任务调度和用户任务调度。 系统任务调度系统周期性所要执行的工作，比如写缓存数据到硬盘、日志清理等。在/etc目录下有一个crontab文件，这个就是系统任务调度的配置文件。 /etc/crontab文件包括下面几行： 12345678910[root@localhost ~]# cat /etc/crontab SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=&quot;&quot;HOME=/# run-parts51 * * * * root run-parts /etc/cron.hourly24 7 * * * root run-parts /etc/cron.daily22 4 * * 0 root run-parts /etc/cron.weekly42 4 1 * * root run-parts /etc/cron.monthly[root@localhost ~]# 前四行是用来配置crond任务运行的环境变量，第一行SHELL变量指定了系统要使用哪个shell，这里是bash，第二行PATH变量指定了系统执行命令的路径，第三行MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户，第四行的HOME变量指定了在执行命令或者脚本时使用的主目录。 用户任务调度用户定期要执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用 crontab 工具来定制自己的计划任务。所有用户定义的crontab 文件都被保存在 /var/spool/cron 目录中。其文件名与用户名一致。 使用者权限文件 文件 说明 /etc/cron.deny 该文件中所列用户不允许使用crontab命令 /etc/cron.allow 该文件中所列用户允许使用crontab命令 /var/spool/cron/ 所有用户crontab文件存放的目录，以用户名命名 crontab文件的含义用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下： 1minute hour day month week command 参数 说明 minute 表示分钟，可以是从0到59之间的任何整数 hour 表示小时，可以是从0到23之间的任何整数 day 表示日期，可以是从1到31之间的任何整数 month 表示月份，可以是从1到12之间的任何整数 week 表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日 command 要执行的命令，可以是系统命令，也可以是自己编写的脚本文件 在以上各个字段中，还可以使用以下特殊字符： 星号(*)：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作； 逗号(,)：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”； 中杠(-)：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”； 正斜线(/)：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。 crond服务安装crontab： 1yum install crontabs 服务操作说明： 1234/sbin/service crond start //启动服务/sbin/service crond stop //关闭服务/sbin/service crond restart //重启服务/sbin/service crond reload //重新载入配置 查看crontab服务状态： 1service crond status 手动启动crontab服务： 1service crond start 查看crontab服务是否已设置为开机启动，执行命令： 1ntsysv 加入开机自动启动： 1chkconfig –level 35 crond on crontab命令详解命令格式12crontab [-u user] filecrontab [-u user] [ -e | -l | -r ] 命令功能通过crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell script脚本。时间间隔的单位可以是分钟、小时、日、月、周及以上的任意组合。这个命令非常设合周期性的日志分析或数据备份等工作。 命令参数 参数 说明 -u user 用来设定某个用户的crontab服务，例如，“-u ixdba”表示设定ixdba用户的crontab服务，此参数一般有root用户来运行 file file是命令文件的名字,表示将file做为crontab的任务列表文件并载入crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab -e 编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件 -l 显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容 -r 从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件 -i 在删除用户的crontab文件时给确认提示 使用示例 示例 命令 每1分钟执行一次command * * * * * command 每小时的第3和第15分钟执行 3,15 * * * * command 在上午8点到11点的第3和第15分钟执行 3,15 8-11 * * * command 每隔两天的上午8点到11点的第3和第15分钟执行 3,15 8-11 */2 * * command 每个星期一的上午8点到11点的第3和第15分钟执行 3,15 8-11 * * 1 command 每晚的21:30重启smb 30 21 * * * /etc/init.d/smb restart 每月1、10、22日的4 : 45重启smb 45 4 1,10,22 * * /etc/init.d/smb restart 一月一号的4点重启smb 0 4 1 jan * /etc/init.d/smb restart 每小时执行/etc/cron.hourly目录内的脚本 01 * * * * root run-parts /etc/cron.hourly 使用注意事项注意环境变量问题有时我们创建了一个crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在crontab文件中没有配置环境变量引起的。 在crontab文件中定义多个调度任务时，需要特别注意的一个问题就是环境变量的设置，因为我们手动执行某个任务时，是在当前shell环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在crontab文件中指定任务运行所需的所有环境变量，这样，系统执行任务调度时就没有问题了。 不要假定cron知道所需要的特殊环境，它其实并不知道。所以你要保证在shelll脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。所以注意如下3点： 脚本中涉及文件路径时写全局路径； 脚本执行要用到java或其他环境变量时，通过source命令引入环境变量，如： ​ 12345cat start_cbp.sh#!/bin/shsource /etc/profileexport RUN_CONF=/home/d139/conf/platform/cbp/cbp_jboss.conf/usr/local/jboss-4.0.5/bin/run.sh -c mev &amp; 当手动执行脚本OK，但是crontab死活不执行时。这时必须大胆怀疑是环境变量惹的祸，并可以尝试在crontab中直接引入环境变量解决问题，如： 10 * * * * . /etc/profile;/bin/sh /var/www/java/audit_no_count/bin/restart_audit.sh 注意清理系统用户的邮件日志每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要。 例如，可以在crontab文件中设置如下形式，忽略日志输出： 10 */3 * * * /usr/local/apache2/apachectl restart &gt;/dev/null 2&gt;&amp;1 “/dev/null 2&gt;&amp;1” 表示先将标准输出重定向到/dev/null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了/dev/null，因此标准错误也会重定向到/dev/null，这样日志输出问题就解决了。 系统级任务调度与用户级任务调度系统级任务调度主要完成系统的一些维护操作，用户级任务调度主要完成用户自定义的一些任务，可以将用户级任务调度放到系统级任务调度来完成（不建议这么做），但是反过来却不行。 root用户的任务调度操作可以通过 1crontab –uroot –e 来设置，也可以将调度任务直接写入/etc/crontab文件，需要注意的是，如果要定义一个定时重启系统的任务，就必须将任务放到/etc/crontab文件，即使在root用户下创建一个定时重启系统的任务也是无效的。 其他注意事项新创建的cron 任务，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。当 crontab 突然失效时，可以尝试/etc/init.d/crond restart解决问题。或者查看日志看某个job有没有执行/报错 1tail -f /var/log/cron 千万别乱运行 1crontab -r 它从Crontab目录（/var/spool/cron）中删除用户的Crontab文件。删除了该用户的所有crontab都没了。 在crontab中%是有特殊含义的，表示换行的意思。如果要用的话必须进行转义\%，如经常用的 1date &apos;+%Y%m%d&apos; 在crontab里是不会执行的，应该换成 1date &apos;+\%Y\%m\%d&apos; Read More: 每天一个linux命令（50）：crontab命令]]></content>
      <categories>
        <category>Linux/Unix</category>
      </categories>
      <tags>
        <tag>命令</tag>
        <tag>Linux</tag>
        <tag>crontab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统中的进程调度算法]]></title>
    <url>%2F2017%2F04%2F02%2Fprocessscheduling%2F</url>
    <content type="text"><![CDATA[无论是在批处理系统还是分时系统中，用户进程数一般都多于处理机数、这将导致它们互相争夺处理机。另外，系统进程也同样需要使用处理机。这就要求进程调度程序按一定的策略，动态地把处理机分配给处于就绪队列中的某一个进程，以使之执行。 处理机高级、中级和低级调度作业从提交开始直到完成，往往要经历下述三级调度：高级调度：又称为作业调度，它决定把后备作业调入内存运行；低级调度：又称为进程调度，它决定把就绪队列的某进程获得CPU；中级调度：又称为在虚拟存储器中引入，在内、外存对换区进行进程对换。 占用CPU的方式非剥夺方式分派程序一旦把处理机分配给某进程后便让它一直运行下去，直到进程完成或某事件而阻塞时，才把处理机分配给另一个进程。 即使在就绪队列存在有优先级高于当前执行进程时，当前进程仍将占用处理机直到该进程自己因调用原语操作或等待I/O而进入阻塞、睡眠状态，或时间片用完时才重新发生调度让出处理机。 剥夺方式当一个进程正在运行时，系统可以基于某种原则，剥夺已分配给它的处理机，将之分配给其它进程。 剥夺原则有：优先权原则、短进程优先原则、时间片原则。 引起进程调度的原因 正在执行的进程执行完毕。这时，如果不选择新的就绪进程执行，将浪费处理机资源； 执行中进程自己调用阻塞原语将自己阻塞起来进入睡眠等状态； 执行中进程调用了P原语操作，从而因资源不足而被阻塞；或调用了v原语操作激活了等待资源的进程队列； 执行中进程提出I/O请求后被阻塞； 在分时系统中时间片已经用完； 在执行完系统调用等系统程序后返回用户进程时，这时可看作系统进程执行完毕，从而可调度选择一新的用户进程执行； 就绪队列中的某进程的优先级变得高于当前执行进程的优先级，从而也将引发进程调度。 进程调度算法先来先服务调度算法(FCFS)根据进程到达的先后顺序执行进程，不考虑等待时间和执行时间，会产生饥饿现象。属于非抢占式调度，优点是公平，实现简单；缺点是不利于短作业。 短进程优先调度算法(SPF)就是优先调度并处理短作业，所谓短是指作业的运行时间短。而在作业未投入运行时，并不能知道它实际的运行时间的长短，因此需要用户在提交作业时同时提交作业运行时间的估计值。 时间片轮转调度算法(RR)前两种算法主要用于批处理系统中，不能作为分时系统中的主调度算法，在分时系统中，都采用时间片轮转法。简单轮转法：系统将所有就绪进程按FIFO规则排队，按一定的时间间隔把处理机分配给队列中的进程。这样，就绪队列中所有进程均可获得一个时间片的处理机而运行。多级队列方法：将系统中所有进程分成若干类，每类为一级。 具体调度过程是：内核从Ready队列中选取第一个进程，将CPU资源分配给它，并且设置一个定时器在一个时间片后中断该进程，调度Ready队列中的下一进程。 很明显，RR调度算法是抢占式的，并且在该算法的调度下，没有一个进程能够连续占用CPU超过一个时间片，从而达到了分时的目的。 给每个进程固定的执行时间，根据进程到达的先后顺序让进程在单位时间片内执行，执行完成后便调度下一个进程执行，时间片轮转调度不考虑进程等待时间和执行时间，属于抢占式调度。优点是兼顾长短作业；缺点是平均等待时间较长，上下文切换较费时。适用于分时系统。 优先级调度算法(HPF)在优先级调度算法中，每个进程都关联一个优先级，内核将CPU分配给最高优先级的进程。具有相同优先级的进程，按照先来先服务的原则进行调度。 高响应比优先调度算法(HRN)根据 响应比 = （进程执行时间 + 进程等待时间） / 进程执行时间 这个公式得到的响应比来进行调度。高响应比优先算法在等待时间相同的情况下，作业执行的时间越短，响应比越高，满足段任务优先，同时响应比会随着等待时间增加而变大，优先级会提高，能够避免饥饿现象。优点是兼顾长短作业，缺点是计算响应比开销大，适用于批处理系统。 多级反馈队列多级反馈队列方式是在系统中设置多个就绪队列，并赋予各队列以不同的优先权。 进程(数据库也适用)死锁产生死锁的原因 系统资源不足； 资源分配不当； 进程运行推进顺序不合适。 产生进程死锁的必要条件 互斥条件：线程在某一时间内独占资源； 不剥夺条件：线程已获得资源，在末使用完之前，不能强行剥夺； 请求与保持条件：一个线程因请求资源而阻塞时，对已获得的资源保持不放； 循环等待条件：若干线程之间形成一种头尾相接的循环等待资源关系。 避免死锁加锁顺序：确保所有的进程都是按照相同的顺序获得锁； 加锁限时：在尝试获取锁的时候加一个超时时间，若超过了这个时限该进程则放弃对该锁请求； 银行家算法，每次在分配资源之前，先判断资源分配后，系统是否会进入不安全的状态，会则不分配，不会则分配； 死锁检测：检测如果发生死锁，通过外边破坏产生死锁的四个必要条件，打破死锁。 死锁预防 破坏互斥条件； 破坏不剥夺条件； 破坏请求和保持条件； 破坏循环等待条件。 饥饿进程当进程的等待时间过长，给进程推进和响应带来明显影响时，称发生了进程“饥饿”。当“饥饿”到一定程度的进程被赋予的任务即使完成也不再具有实际意义时称该进程被“饿死”。 饥饿产生的原因进程所请求的资源（可以是CPU资源）长时间得不到满足，从而长时间处于阻塞或就绪状态。 饥饿与死锁的区别 进入饥饿的进程可以只有一个，而死锁的进程至少有两个； 饥饿状态的进程可以是就绪状态和阻塞状态，而处于死锁的进程一定是阻塞状态。]]></content>
      <categories>
        <category>进程/线程</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
        <tag>进程调度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo原理及其常用命令]]></title>
    <url>%2F2017%2F04%2F02%2FaboutHexo%2F</url>
    <content type="text"><![CDATA[Hexo原理2017年4月1日使用Hexo+GitHub创建了自己的博客，主要用于生活和学习总结，经过两天的折腾，基本的博客雏形已经出来了，不得不说GitHub Pages真是个好东西。给赞。 对于Hexo，官网文档 解释为： Hexo is a fast, simple and powerful blog framework. You write posts in Markdown (or other languages) and Hexo generates static files with a beautiful theme in seconds. generates static files 这就是Hexo的作用所在，我们都知道GitHub给了广大程序设计人员一个存放开源项目的平台，每个人都可以迭代更新自己的项目或参与开发其他人的项目，而GitHub Pages就是给你提供了一个平台来显示你博客的静态页面，注意是静态页面，因为GitHub Pages不支持动态语言，只能使用 html 拼合成博客。 Hexo就是这个生成静态页面的框架，流程如下 1markdown**.md -&gt; hexo -&gt; **.html -&gt; github -&gt; update website 你在本地编写并用Hexo生成了静态页面，存储在/public 目录下，然后push到GitHub上，GitHub展示了你的页面而已。 根据Hexo的原理是什么 问题中孔晨皓 的回答： 如果要做博客 wordpress 的思路是 php + MySql 而 gitpages 不支持动态语言，因此只能使用 html 拼合成博客 首先自己本地文件夹的 source 就是数据库，以 .md(markdown) 格式存储文章， theme 文件夹是主题文件，以 .yml 等类型，决定了页面如何“组装” 每次运行 hexo g 命令，hexo(node.js程序)会遍历你的 source 目录，建立索引，根据你 theme 文件夹的主题生成页面到 public 文件夹。这时 public 文件夹就是一个纯由 html javascript css 等内容制作的博客，而这些恰好能在 git pages 识别 而我们直接打开/public 目录中的**.html 是不能直接显示的，需要在命令行中执行以下命令： 1hexo s 这条命令相当于hexo通过nide.js启动了一个本地服务器。 Hexo常用命令详见hexo常用命令笔记]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>原理</tag>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Heox主题的其他配置]]></title>
    <url>%2F2017%2F04%2F01%2FhexoOthers%2F</url>
    <content type="text"><![CDATA[Hexo页面底部次数显示很多网站中都有访问人数和总访问量，也就是下图所示的功能： 我们可以通过在主题配置文件中添加以下代码来实现此功能： 123456789101112131415busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt; 访问人数 site_uv_footer: # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; 总访问量 site_pv_footer: 次 # custom pv span for one page only page_pv: true page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt; 浏览 page_pv_footer: 次 添加社交链接如果希望试下下图显示的功能： 可以在主题配置文件中添加如下代码： 12345678910111213141516171819social: #LinkLabel: Link GitHub: 链接地址 weibo: 链接地址 zhihu: 链接地址# Social Links Icons# Icon Mapping:# Map a menu item to a specific FontAwesome icon name.# Key is the name of the item and value is the name of FontAwesome icon. Key is case-senstive.# When an globe mask icon presenting up means that the item has no mapping icon.social_icons: enable: true # Icon Mappings. # KeyMapsToSocialItemKey: NameOfTheIconFromFontAwesome GitHub: github weibo: weibo zhihu: hand-o-right Hexo主题使用的图标都在 FontAwesome 上，你可以根据自己需要选择，我发现这是一个很不错的网站，各种图标很漂亮，PPT中也可以用。 分类、归档、关于页面的创建安装完成Hexo之后，我们点击“分类”、“关于”、“标签”页面都会显示GitH的404页面，这是因为我们还没有创建每个功能对应的文件，使用如下方法创建： 分类： 1hexo new page &quot;categories&quot; 关于： 1hexo new page &quot;about&quot; 标签： 1hexo new page &quot;tags&quot; 使用以上hexo命令会在Hexo根目录的source文件夹下创建各个对应的文件夹，每个文件夹中都有index.md文件，这就成功了，文件中的内容不用修改。 创建完成之后，我们在编辑文章时： 12345678title: 标题date: 2017-04-01 21:34:40tags: - Hexo - 主题 - 博客categories: - Hexo “tags”、“categories”是对文章打的标签和分类，默认文章是可以评论的，如果不希望评论，可以加上： 1comments： false 首页文章显示摘要第一种方式：在主题配置文件中，找到auto_excerpt，修改如下： 12345# Automatically Excerpt. Not recommend.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: true length: 150 第二种方式：根据文章的内容，自己在合适的位置添加&lt;!--more--&gt;标签，使用灵活，也是Hexo推荐的方法 第三种方式：在文章中的front-matter中添加description，并提供文章摘录。这种方式只会在首页列表中显示文章的摘要内容，进入文章详情后不会再显示。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>博客</tag>
        <tag>主题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo应用NexT主题]]></title>
    <url>%2F2017%2F04%2F01%2FhexoNexT%2F</url>
    <content type="text"><![CDATA[Hexo主题Hexo 为我们提供了很多主题，本博客的主题使用的是NexT主题，是目前Github上Star最高的Hexo主题，支持几种不同的风格。所以本文以NexT为例，介绍Hexo主题的配置及使用。 安装NexT主题在 Hexo 中有两份主要的配置文件，其名称都是 _config.yml。 其中，一份位于站点根目录下，主要包含 Hexo 本身的配置；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项。 为了描述方便，在以下说明中，将前者称为 站点配置文件， 后者称为 主题配置文件。比如我的电脑下的 D:\Hexo 目录下的称为站点配置文件，D:\Hexo\themes\next 目录下的成为主题配置文件。 在终端窗口下，定位到 Hexo 站点目录下。运行下面的代码： 1git clone https://github.com/iissnan/hexo-theme-next themes/next 启用NexT主题与所有 Hexo 主题启用的模式一样。 当 克隆/下载 完成后，打开 站点配置文件， 找到 theme 字段，并将其值更改为 next。 1theme: next 到此，NexT 主题安装完成。下一步我们将验证主题是否正确启用。在切换主题之后、验证之前， 执行以下代码清除 Hexo 的缓存 1hexo clean 验证主题首先启动 Hexo 本地站点，并开启调试模式（即加上 –debug），整个命令是 hexo s –debug。 在服务启动的过程，注意观察命令行输出是否有任何异常信息，如果你碰到问题，这些信息将帮助他人更好的定位错误。 当命令行输出中提示出： INFO Hexo is running at http://0.0.0.0:4000/. Press Ctrl+C to stop. 若运行成功，NexT 默认的 Scheme —— Muse 主题设定Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。目前 NexT 支持三种 Scheme 123Muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白Mist - Muse 的紧凑版本，整洁有序的单栏外观Pisces - 双栏 Scheme，小家碧玉似的清新 将不需要启用的主题注释 # 掉即可。 123# scheme: Muse# scheme: Mistscheme: Pisces 设置语言编辑 站点配置文件， 将 language 设置成你所需要的语言。建议明确设置你所需要的语言，例如选用简体中文，配置如下： 1language: zh-Hans 其他主题设置访问 NexT官方设置文档 进行设置 如果想要在“站点概览”页面显示“友情链接”，需要在站点配置文件中加入以下代码： 12345# title, chinese availablelinks_title: 友情链接 # # linkslinks: 显示名称: 链接地址 集成三方工具添加文章阅读量功能参考 为NexT主题添加文章阅读量统计功能 进行设置，该博客只是针对NexT主题，其他主题请寻找其他解决办法。 添加站内搜索能NexT主题支持集成 Swiftype、 微搜索、Local Search 和 Algolia,Swiftype和Algolia都只有一段时间的试用期，可以采用Hexo提供的Local Search,原理是通过hexo-generator-search插件在本地生成一个search.xml文件，搜索的时候从这个文件中根据关键字检索出相应的链接。 安装 hexo-generator-search 在站点的根目录下执行以下命令： 1npm install hexo-generator-search --save 安装 hexo-generator-searchdb 在站点的根目录下执行以下命令： 1npm install hexo-generator-searchdb --save 启用搜索 编辑 站点配置文件，新增以下内容到任意位置： 12345search: path: search.xml field: post format: html limit: 10000 最后不要忘了将站点配置文件的local_search改成true。 123# Local searchlocal_search: enable: true 给博客添加feed安装hexo-generator-feed 1npm install hexo-generator-feed --save 配置到站点配置文件_config.yml 12345678910# Extensions## Plugins: http://hexo.io/plugins/#RSS订阅plugin:- hexo-generator-feed#Feed Atomfeed:type: atompath: atom.xmllimit: 20 最后，在你next主题下的_config.yml下，添加RSS订阅链接即可： 1rss: /atom.xml 给博客生成一个站点地图安装hexo-generator-seo-friendly-sitemap 1npm install hexo-generator-seo-friendly-sitemap --save 在站点配置文件_config.yml 中添加 12sitemap: path: sitemap.xml 头像圆形旋转把完整的 sidebar-author.styl 文件内容复制替换到 1..hexo/themes/next/source/css/_common/components/sidebar/sidebar-author.styl 参考博客：Hexo博客添加站内搜索记录Hexo+Github免费搭建个人博客]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>NexT</tag>
        <tag>主题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github搭建个人博客]]></title>
    <url>%2F2017%2F04%2F01%2Fhexogithub%2F</url>
    <content type="text"><![CDATA[动机促使我建立自己博客的动机很简单，其他的博客平台太丑了！简直不能看啊，然而我并没有在说CSDN、CNBLOG等等。还有一个原因就是，我有一颗向往技术大神的心，所以需要靠自己一点点的积累。在过滤各种博客平台的时候，看到某乎大神建议使用GitHub搭建自己的博客，So，心动不如行动，于是搜集各种资料，搭建了自己博客，本文主要是记录搭建的过程以及可能遇到的问题及解决办法。 本教程只适用于windows操作系统 准备工具 GitHub账户配置 安装Node.js环境 安装Git环境 GitHub账户配置GitHub账户申请： 如果已经拥有GitHub账户，请进行第二步注册结束后，一定前往自己的注册邮箱，点开GitHub发送给你的注册确认信，确认注册，结束注册流程。否则无法使用gh-pages。 创建代码库： 登陆之后，点击页面右上角的加号，选择New repository新建代码库；进入代码库创建页面，在Repository name下填写yourname.github.io，Description (可选)下填写一些简单的描述（不写也可以）； 注意：比如我的github名称是runnerliu,这里就填 runnerliu.github.io； 代码库设置: 开启gh-pages功能，正确创建代码库后，点击界面右侧的Settings，你将会打开这个库的setting页面，向下拖动，直到看见GitHub Pages； 点击Automatic page generator，Github将会自动替你创建出一个gh-pages的页面。 如果配置没有问题，那么大约15分钟之后，yourname.github.io这个网址就可以正常访问了；注意：我自己在操作过程中并没有这一步，可能是GitHub升级了？ 安装Node.js环境根据自己操作系统的版本前往Node.js下载相应的Node.js，可以根据所需选择安装路径，默认安装的话一路next就可以。 验证安装： 打开控制台（Win+R输入cmd，回车）； 输入node -v查看是否输出版本号； 输入npm -v查看是否输出版本号； 如果全部输出成功，证明安装成功，否则检查系统环境变量是否配置。 安装Git环境前往Git官网下载相应版本的Git； 和Node.js一样，根据需要选择安装路径，可以一路默认安装； 安装完成后，在控制台中输入git --version查看是否添加了系统环境变量，这样就可以直接在dos中使用git命令了； 如果输出有误，检查git环境变量的设置。 完成以上步骤，准备阶段的工作就完成了。 Hexo 安装Hexo 配置Hexo 将Hexo与github page绑定 Hexo的使用设置 安装Hexo在合适的地方创建一个文件夹，这里我以D：/hexo 为例讲解，首先在D盘目录下创建Hexo文件夹，并在命令行的窗口进入到该目录； 在命令行中输入npm install hexo-cli -g，如果看到WARN也别担心，不影响使用； 继续输入npm install hexo --save； 稍等安装完成后，输入hexo -v查看Hexo是否安装成功。如果安装成功会显示出hexo的版本号以及其他信息； 配置Hexo初始化Hexo： 在DOS中输入hexo init； 然后输入npm install，npm将会自动安装你需要的组件，只需要等待npm操作即可。 体验Hexo： 在DOS中输入hexo g，等待完成； 然后输入hexo s，如果提示INFO Hexo is running at http://0.0.0.0:4000/. Press Ctrl+C to stop.，证明Hexo启动成功； 在浏览器中打开http://localhost:4000/，会看到Hexo的效果。 将Hexo与github page绑定配置Git个人信息 设置Git的user name和email：(如果是第一次的话) 运行git config --global user.name &quot;zhangsan&quot;，”zhangsan”是你GitHub账户名； 运行git config --global user.email &quot;zhangsan@163.com&quot;，”zhangsan@163.com”是你注册GitHub的邮箱。 使用SSH远程连接GitHub库（本部分命令都在Git Bash中执行） 注意：配置SSH主要是用于本地远程更新GitHub代码库，在搜集资料的时候看到有人说可以用https url的方式，但是我实践过程中是不行的，而且https url的方式在fetch和push代码都需要输入账号和密码，这是比较麻烦的。建议使用SSH-Key方式。 打开Git Bash，输入ssh-keygen -t rsa -C &quot;zhangsan@163.com&quot;生成密钥，”zhangsan@163.com”是你注册GitHub的邮箱，然后连续三个回车； 最后得到了两个文件：id_rsa和id_rsa.pub，默认存数在C:\Users\电脑名称\.ssh； 添加密钥到ssh-agent：输入 eval &quot;$(ssh-agent -s)&quot; ，输入 ssh-add ~/.ssh/id_rsa ； 登录GitHub，点击自己头像选择”Settings”，选择”SSH and GPG keys”，选择”New SSH key”，将id_rsa.pub中的内容copy过去； 在Git Bash终端中输入 ssh -T git@github.com 验证SSH-key是否配置成功，如果出现 Hi zhagnsan! You’ve successfully authenticated, but GitHub does not provide shell access. 表明配置成功； 配置Deployment 在Hexo根目录下的_config.yml文件中，找到Deployment，然后按照如下修改： 1234deploy: type: git repo: git@github.com:yourname/yourname.github.io.git branch: master Hexo的使用设置首先安装扩展： npm install hexo-deployer-git --save ； 新建一篇博客，执行下面的命令： hexo new post &quot;article title&quot; ，这时候在电脑的目录下 D:\Hexo\source_posts 将会看到 article title.md 文件； 用MarDown编辑器打开就可以编辑文章了。文章编辑好之后，运行生成、部署命令： 1234hexo g // 生成hexo d // 部署 或者 hexo d -g //在部署前先生成 然后访问 https://yourName.github.io/ 查看生成的博客。 站点配置信息在Hexo根目录下的_config.yml文件中，根据需要配置各项信息: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556博客名称title: 我的博客副标题subtitle: 一天进步一点简介description: 记录生活点滴博客作者author: John Doe博客语言language: zh-CN时区timezone:博客地址,与申请的GitHub一致url: http://zhangsan.github.ioroot: /博客链接格式permalink: :year/:month/:day/:title/permalink_defaults:source_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:new_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: true tab_replace:default_category: uncategorizedcategory_map:tag_map:日期格式date_format: YYYY-MM-DDtime_format: HH:mm:ss分页，每页文章数量per_page: 10pagination_dir: page博客主题theme: 发布设置deploy: type: git repository: https://github.com/zhangsan/zhangsan.github.io.git branch: master 参考博客：手把手教你用Hexo+Github 搭建属于自己的博客Git ssh 配置及使用]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[愚人节]]></title>
    <url>%2F2017%2F04%2F01%2Fheyliuxiaoyang%2F</url>
    <content type="text"><![CDATA[今天是愚人节，我的个人博客上线了。]]></content>
      <categories>
        <category>生活日志</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>Life</tag>
      </tags>
  </entry>
</search>
